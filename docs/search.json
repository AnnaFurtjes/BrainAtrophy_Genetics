[
  {
    "objectID": "age_corrs.html",
    "href": "age_corrs.html",
    "title": "Age-associated brain shrinkage",
    "section": "",
    "text": "Results produced by the code below are described in the manuscript under section:\nMeasures of LBA indicated age-associated brain shrinkage.\nThe traits used for correlation analysis were derived using code displayed in ‘Data preparation’: UKB: neuroimaging & phenotypic data.."
  },
  {
    "objectID": "age_corrs.html#read-in-all-samples",
    "href": "age_corrs.html#read-in-all-samples",
    "title": "Age-associated brain shrinkage",
    "section": "Read in all samples",
    "text": "Read in all samples\n\n\nCode\n# UKB - use cross-sectional data we're also using for GWAS\nUKB = fread(paste0(wd, \"/UKB_CrossNeuroIDP_noOutliers.txt\"))\nage = fread(paste0(wd, \"/UKB_covarGWAS.txt\"))\nUKB = merge(UKB, age[,c(\"FID\", \"age\",\"sex\")], by = \"FID\")\nUKB$Sample = \"UKB\"\nnames(UKB)[which(names(UKB) == \"IID\")] = \"ID\"\nnames(UKB)[which(names(UKB) == \"age\")] = \"Age\"\n\n# restrict to fam file\nfam = fread(paste0(wd, \"/ukb_neuroimaging_brainAtrophy_GWASinput.fam\"))\nUKB = UKB[UKB$FID %in% fam$V1,]\n\n# for more intuitive interpretation, we will flip the associations for resid and ratio sscore\n# larger score = more atrophy\nUKB$resid = UKB$resid*(-1)\nUKB$resid_stand = UKB$resid_stand*(-1)\nUKB$ratio = UKB$ratio*(-1)\nUKB$ratio_stand = UKB$ratio_stand*(-1)\n\n#####################\n## Human Connectome Project\n#####################\n# read in HCP data\nHCP = fread(paste0(wd,\"/unrestricted_hcp_freesurfer.csv\"))\nHCP = HCP[,c(\"Subject\", \"Gender\", \"FS_InterCranial_Vol\", \"FS_BrainSeg_Vol_No_Vent\")]\nnames(HCP) = c(\"ID\", \"Sex\", \"ICV\", \"TBV\")\n\n# add age information\nHCPage = fread(paste0(wd, \"/RESTRICTED_annafurtjes_12_14_2023_4_18_2.csv\"))\nnames(HCPage)[which(names(HCPage) == \"Subject\")] = \"ID\"\nnames(HCPage)[which(names(HCPage) == \"Age_in_Yrs\")] = \"Age\"\nHCP = merge(HCP, HCPage[,c(\"ID\",\"Age\")], by = \"ID\")\n\n# as outlined elsewhere, empirical investigations warrant to use an age cut-off of 31 years in this sample\nHCP = HCP[which(HCP$Age <= 31),]\n\n# convert mm3 estimates to more intuitive cm3 estimates\nHCP$ICV = HCP$ICV/1000\nHCP$TBV = HCP$TBV/1000\n\n# estimate brain atrophy from single MRI scan\nHCP$diff = HCP$ICV - HCP$TBV\nHCP$ratio = HCP$TBV / HCP$ICV\n\n# Quality control: \n#print(paste0(\"Some participants have negative difference scores and ratio scores > 1, which means that their ICV estimate is smaller than their TBV estimate. This must be an error as the skull always surrounds the brain. Those \", sum((HCP$diff < 0)),\" HCP participants were excluded from the data set.\"))\n\ndeletedHCP = sum(HCP$diff < 0)\n# delete those from data \nif(sum(HCP$diff < 0) != 0){HCP=HCP[-which(HCP$diff < 0),]}\n\n# estimate residual model\nmodel <- lm(TBV ~ ICV, data = HCP)\nHCP$resid = as.vector(resid(model, na.rm=T))\n\n# for more intuitive interpretation, we will flip the associations for resid and ratio sscore\n# larger score = more atrophy\nHCP$resid = HCP$resid*(-1)\nHCP$ratio = HCP$ratio*(-1)\n\n# standardise variables\nHCP$diff_stand = as.vector(scale(HCP$diff))\nHCP$ratio_stand = as.vector(scale(HCP$ratio))\nHCP$resid_stand = as.vector(scale(HCP$resid))\n\n\n#####################\n## MRi-Share\n#####################\n# read in MRi-Share\nShare = fread(paste0(wd, \"/MRiShare_global_IDPs_BSAF2021.csv\"))\nShare$TBV = Share$SPM_GM_Volume + Share$SPM_WM_Volume\nShare = Share[,c(\"ID\", \"Age\", \"Sex\", \"eTIV\", \"TBV\")]\nnames(Share) = c(\"ID\", \"Age\", \"Sex\", \"ICV\", \"TBV\")\n\n# convert mm3 estimates to more intuitive cm3 estimates\nShare$ICV = Share$ICV/1000\nShare$TBV = Share$TBV/1000\n\n# estimate brain atrophy from single MRI scan\nShare$diff = Share$ICV - Share$TBV\nShare$ratio = Share$TBV / Share$ICV\n\nmodel <- lm(TBV ~ ICV, data = Share)\nShare$resid = resid(model)\n\n# save intercept value from the regression\nShareintercept = summary(model)$coefficients[1,1]\n\n# for more intuitive interpretation, we will flip the associations for resid and ratio sscore\n# larger score = more atrophy\nShare$resid = Share$resid*(-1)\nShare$ratio = Share$ratio*(-1)\n\n# standardise variables\nShare$diff_stand = as.vector(scale(Share$diff))\nShare$ratio_stand = as.vector(scale(Share$ratio))\nShare$resid_stand = as.vector(scale(Share$resid))"
  },
  {
    "objectID": "age_corrs.html#calculate-age-correlations-and-plot",
    "href": "age_corrs.html#calculate-age-correlations-and-plot",
    "title": "Age-associated brain shrinkage",
    "section": "Calculate age correlations and plot",
    "text": "Calculate age correlations and plot\n\n\nCode\n# determine age cut-offs to iterate through\nageCut = seq(from = 22, to = max(Share$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotShare = successivelyReduceAge(data = Share, ageCut = ageCut)\n\npShare = ggplot(data = agePlotShare)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(21, 35), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.1, 0.2), breaks = seq(from = -0.5, to = 0.35, by = 0.1))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"MRi-Share (N = \", nrow(Share),\")\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 23, to = max(HCP$Age), by = 1)\n \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nsuccessivelyYoungertHCP = successivelyReduceAge(data = HCP, ageCut = ageCut)\n\npHCP = ggplot(data = successivelyYoungertHCP)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 29, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(22, 31.5), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.2, 0.3), breaks = seq(from = -0.5, to = 0.35, by = 0.1))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"HCP (N = \", nrow(HCP),\")\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n\n#### UKB\nif(mean(UKB$Age > 100)){UKB$Age = UKB$Age /12}\n# determine age cut-offs to iterate through\nageCut = seq(from = 47, to = max(UKB$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotUKB= successivelyReduceAge(data = UKB, ageCut = ageCut)\n\npUKB = ggplot(data = agePlotUKB)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(46, 84.5), breaks = seq(from = 44, to = 84, by = 2))+\n  scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(from = -0.5, to = 0.5, by = 0.2))+\n  ggtitle(paste0(\"UKB (N = \", nrow(UKB),\")\"))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 35, to = max(STRADL$Age), by = 1)\n  \n\n########\n# change direction STRADL\nSTRADL$resid <- STRADL$resid*(-1)\nSTRADL$ratio <- STRADL$ratio*(-1)\n\n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotSTRADL = successivelyReduceAge(data = STRADL, ageCut = ageCut)\n\npSTRADL = ggplot(data = agePlotSTRADL)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(34.5, 84.5), breaks = seq(from = 34, to = 84, by = 4))+\n  scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(from = -0.5, to = 0.5, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"STRADL (N = \", nrow(STRADL),\")\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\nggsave(\"Fig3_indiv.jpg\", bg = \"white\",plot = newFig2bottoM, width = 30, height = 12, units = \"cm\", dpi = 150)"
  },
  {
    "objectID": "age_corrs.html#repeat-for-males-and-females-separately",
    "href": "age_corrs.html#repeat-for-males-and-females-separately",
    "title": "Age-associated brain shrinkage",
    "section": "Repeat for males and females separately",
    "text": "Repeat for males and females separately\nPlots generated below are in Supplementary Plot 13.\n\nFemales\n\n\nCode\n###### FEMALES\n# determine age cut-offs to iterate through\nageCut = seq(from = 22, to = max(Share$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotShare = successivelyReduceAge(data = Share[which(Share$Sex == \"F\"),], ageCut = ageCut)\n\npShareF = ggplot(data = agePlotShare)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(21, 35), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.1, 0.2), breaks = seq(from = -0.5, to = 0.35, by = 0.1))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"MRi-Share\\n(N = \", nrow(Share[which(Share$Sex == \"F\"),]),\" females)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 23, to = max(HCP$Age), by = 1)\n \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nsuccessivelyYoungertHCP = successivelyReduceAge(data = HCP[which(HCP$Sex == \"F\")], ageCut = ageCut)\n\npHCPF = ggplot(data = successivelyYoungertHCP)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 29, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(22, 31.5), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.3, 0.6), breaks = seq(from = -0.3, to = 0.6, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"HCP\\n(N = \", nrow(HCP[which(HCP$Sex == \"F\")]),\" females)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n\n#### UKB\nif(mean(UKB$Age > 100)){UKB$Age = UKB$Age /12}\n# determine age cut-offs to iterate through\nageCut = seq(from = 47, to = max(UKB$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotUKB= successivelyReduceAge(data = UKB[UKB$sex == 1,], ageCut = ageCut)\n\npUKBF = ggplot(data = agePlotUKB)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(46, 84.5), breaks = seq(from = 44, to = 84, by = 2))+\n  scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(from = -0.5, to = 0.5, by = 0.2))+\n  ggtitle(paste0(\"UKB\\n(N = \", nrow(UKB[UKB$sex == 1,]),\" females)\"))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 35, to = max(STRADL$Age), by = 1)\n  \n\n########\n# change direction STRADL\n#STRADL$resid <- STRADL$resid*(-1)\n#STRADL$ratio <- STRADL$ratio*(-1)\n\n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotSTRADL = successivelyReduceAge(data = STRADL[STRADL$Sex == 0,], ageCut = ageCut)\n\npSTRADLF = ggplot(data = agePlotSTRADL)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(34.5, 84.5), breaks = seq(from = 34, to = 84, by = 4))+\n  scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(from = -0.5, to = 0.5, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"STRADL\\n(N = \", nrow(STRADL[STRADL$Sex == 0,]),\" females)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n\nAgecorrFEMALES = ggarrange(pShareF, pHCPF, pUKBF, pSTRADLF, labels = c(\"A\",\"B\",\"C\",\"D\"), common.legend = T, legend = \"bottom\", nrow=1)\nggsave(\"AgeCorr_FEMALES.jpg\", bg = \"white\",plot = AgecorrFEMALES, width = 30, height = 12, units = \"cm\", dpi = 150)\n\n\n\n\n\n\n\n\n\nMales\n\n\nCode\n###### MALES\n# determine age cut-offs to iterate through\nageCut = seq(from = 22, to = max(Share$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotShare = successivelyReduceAge(data = Share[which(Share$Sex == \"M\"),], ageCut = ageCut)\n\npShareM = ggplot(data = agePlotShare)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(21, 35), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.2, 0.2), breaks = seq(from = -0.5, to = 0.35, by = 0.1))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"MRi-Share\\n(N = \", nrow(Share[which(Share$Sex == \"M\"),]),\" males)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 23, to = max(HCP$Age), by = 1)\n \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nsuccessivelyYoungertHCP = successivelyReduceAge(data = HCP[which(HCP$Sex == \"M\")], ageCut = ageCut)\n\npHCPM = ggplot(data = successivelyYoungertHCP)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 29, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(22, 31.5), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.3, 0.6), breaks = seq(from = -0.3, to = 0.6, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"HCP\\n(N = \", nrow(HCP[which(HCP$Sex == \"M\")]),\" males)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n\n#### UKB\nif(mean(UKB$Age > 100)){UKB$Age = UKB$Age /12}\n# determine age cut-offs to iterate through\nageCut = seq(from = 48, to = max(UKB$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotUKB= successivelyReduceAge(data = UKB[UKB$sex == 0,], ageCut = ageCut)\n\npUKBM = ggplot(data = agePlotUKB)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(48, 84.5), breaks = seq(from = 44, to = 84, by = 2))+\n  scale_x_continuous(limits = c(-0.55, 0.6), breaks = seq(from = -1, to = 0.9, by = 0.2))+\n  ggtitle(paste0(\"UKB\\n(N = \", nrow(UKB[UKB$sex == 0,]),\" males)\"))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 35, to = max(STRADL$Age), by = 1)\n  \n\n########\n# change direction STRADL\n#STRADL$resid <- STRADL$resid*(-1)\n#STRADL$ratio <- STRADL$ratio*(-1)\n\n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotSTRADL = successivelyReduceAge(data = STRADL[STRADL$Sex == 1,], ageCut = ageCut)\n\npSTRADLM = ggplot(data = agePlotSTRADL)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(34.5, 84.5), breaks = seq(from = 34, to = 84, by = 4))+\n  scale_x_continuous(limits = c(-0.7, 0.55), breaks = seq(from = -1, to = 0.55, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"STRADL\\n(N = \", nrow(STRADL[STRADL$Sex == 1,]),\" males)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\nAgecorrMALES = ggarrange(pShareM, pHCPM, pUKBM, pSTRADLM, labels = c(\"A\",\"B\",\"C\",\"D\"), common.legend = T, legend = \"bottom\", nrow=1)\nggsave(\"AgeCorr_MALES.jpg\", bg = \"white\",plot = AgecorrMALES, width = 30, height = 12, units = \"cm\", dpi = 150)"
  },
  {
    "objectID": "age_corrsLBC.html",
    "href": "age_corrsLBC.html",
    "title": "Lifetime brain atrophy increases with age within individuals in LBC1936",
    "section": "",
    "text": "This analysis was moved to the Supplement during revisions of the paper. It shows that measures of lifetime brain atrophy worsen over a 9-year period in LBC1936, even when we cross-sectionally process our neuroimaging data (as opposed to using FS longitudinal processing).\nNeuroimaging data used here was prepared using code displayed in ‘Data preparation’: LBC: neuroimaging data."
  },
  {
    "objectID": "age_corrsLBC.html#read-in-data",
    "href": "age_corrsLBC.html#read-in-data",
    "title": "Lifetime brain atrophy increases with age within individuals in LBC1936",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\n# get cross-sectionally processed data from \n# wave 1\nwave1 = fread(paste0(wd, \"/LBC1936_crossNeuroWave1.txt\"))\nwave1$wave = \"wave 2\"\nwave1$age = 73\n# wave 2\nwave2 = fread(paste0(wd, \"/LBC1936_crossNeuroWave2.txt\"))\nwave2$wave = \"wave 3\"\nwave2$age = 76\n# wave 3\nwave3 = fread(paste0(wd, \"/LBC1936_crossNeuroWave3.txt\"))\nwave3$wave = \"wave 4\"\nwave3$age = 79\n# wave 4\nwave4 = fread(paste0(wd, \"/LBC1936_crossNeuroWave4.txt\"))\nwave4$wave = \"wave 5\"\nwave4$age = 82\n\n# rbind wave data\nall = rbind(wave1, wave2, wave3, wave4)\n\n# only keep participants who have all measurement points\nsave = table(all$lbc36no) == 4\nIDs = dimnames(save)[[1]][as.vector(save)]\nall = all[all$lbc36no %in% IDs,]\n\n#### later edit: so far this data has a residual score for each of the visits meaning that there can never be an increase with age in the residual score\n# Hence, here we standardise across all waves to be able to compare different time points\n# estimate residual model\nmodel <- lm(TBV ~ ICV, data = all)\nall$residALL = as.vector(resid(model, na.rm=T))"
  },
  {
    "objectID": "age_corrsLBC.html#plot-trajectories",
    "href": "age_corrsLBC.html#plot-trajectories",
    "title": "Lifetime brain atrophy increases with age within individuals in LBC1936",
    "section": "Plot trajectories",
    "text": "Plot trajectories\nThis is now Supplementary Figure 12.\n\n\nCode\np_diff = plotTraject(dat = all, y = \"diff\", col = \"#D81B60\") + \n  ggtitle(\"Raw difference score\")+\n  ylab(\"<- less atrophy         more atrophy ->\")\n\np_ratio = plotTraject(dat = all, y = \"ratio\", col = \"#FFC107\") + \n  ggtitle(\"Raw ratio score\") + \n  scale_y_reverse() + \n  ylab(\"<- less atrophy         more atrophy ->\")\n\np_resid = plotTraject(dat = all, y = \"residALL\", col = \"#004D40\") + \n  ggtitle(\"Raw residual score\")+ \n  scale_y_reverse()+ \n  ylab(\"<- less atrophy         more atrophy ->\")\n\n#cowplot::plot_grid(p_diff, p_ratio, p_resid, nrow = 1, labels = c(\"A\", \"B\", \"C\"), label_size = 6, rel_widths = c(1,1,1))\n## overall title: \"Estimated brain atrophy in LBC1936 (cross-sectional processing)\"\nplot = (p_diff | p_ratio | p_resid) + \n    plot_annotation(title = \"Lifetime brain atrophy\\n(estimated from cross-sectionally processed measures of TBV and ICV)\", \n                    tag_levels = \"A\",\n                            theme = theme(plot.tag = element_text(face = \"bold\"),\n                                  plot.title = element_text(face = \"bold\", size = 20, hjust = 0.5)))\n\nggsave(\"EstimatedAtrophy_LBC1936_wave2to5.jpg\", bg = \"white\",plot = plot, width = 35, height = 20, units = \"cm\", dpi = 150)"
  },
  {
    "objectID": "GWAS.html",
    "href": "GWAS.html",
    "title": "Genome-wide association study",
    "section": "",
    "text": "GWAS analysis was performed with the REGENIE software.\nInput data was prepared using scripts for phenotypic and genotype data. Imputed data was imputed using the Michigan imputation server.\nEnsure that the resid_stand variable was calculated in the subsample with non-missing data that is analysed in the GWAS analysis."
  },
  {
    "objectID": "GWAS.html#regenie-step-1",
    "href": "GWAS.html#regenie-step-1",
    "title": "Genome-wide association study",
    "section": "Regenie: Step 1",
    "text": "Regenie: Step 1\n\n\nCode\n#!/bin/bash\n\nregenie_v3.4 \\\n--step 1 \\\n--bed $genotype/ukb_neuroimaging_brainAtrophy_GWASinput \\\n--phenoFile $phenotype/UKB_CrossNeuroIDP_noOutliers.txt \\\n--phenoColList ${trait}  \\\n--covarFile $phenotype/UKB_covarGWAS.txt \\\n--catCovarList sex,assessmentMonth,site,array,batch \\\n--bsize 100 \\\n--threads 30 \\\n--maxCatLevels 106 \\\n--lowmem \\\n--lowmem-prefix ${wd}/tmp_noOutliers \\\n--iid-only \\\n--out ${wd}/Step1_out_all_noOutliers"
  },
  {
    "objectID": "GWAS.html#regenie-step-2",
    "href": "GWAS.html#regenie-step-2",
    "title": "Genome-wide association study",
    "section": "Regenie: Step 2",
    "text": "Regenie: Step 2\n\n\nCode\n#!/bin/bash\n\nregenie_v3.4 \\\n--step 2 \\\n--bgen $imputed/ukb_chr${CHR}_clean_v3.bgen \\\n--sample $sampleFile/ukb10279_imp_chr${CHR}_v3_s487395.sample \\\n--keep $genotype/ukb_neuroimaging_brainAtrophy_GWASinput.fam \\\n--covarFile $phenotype/UKB_covarGWAS.txt \\\n--catCovarList sex,assessmentMonth,site,array,batch \\\n--maxCatLevels 106 \\\n--phenoFile $phenotype/UKB_CrossNeuroIDP_noOutliers.txt \\\n--phenoColList ${trait} \\\n--bsize 400 \\\n--minINFO 0.4 \\\n--minMAC 5 \\\n--threads 5 \\\n--pred ${step1}/Step1_out_all_pred_noOutliers.list \\\n--out $wd/GWAS_${trait}_chr${CHR}"
  },
  {
    "objectID": "GWAS.html#regenie-interaction-models-step-2",
    "href": "GWAS.html#regenie-interaction-models-step-2",
    "title": "Genome-wide association study",
    "section": "Regenie interaction models: step 2",
    "text": "Regenie interaction models: step 2\nAs unintuitive as it may seem, you must standardise the age variable before running this script. Regenie calculates a product-wise variable for the interaction between phenotype and age and if age is indicated in months or even days, the variable will be so large that regenie cannot handle it, and results will be massively inflated (results are complete nonsense with every SNP being extremely significant).\n\n\nCode\n#!/bin/bash\n\n# note that this script only runs when linked to the bgen sample file \n\ntrait=$1\n\nfor CHR in {1..22}\ndo\nregenie_v3.4 \\\n--step 2 \\\n--bgen $imputed/ukb_chr${CHR}_clean_v3.bgen \\\n--ref-first \\\n--sample $sampleFile/ukb10279_imp_chr${CHR}_v3_s487395.sample \\\n--keep $genotype/ukb_neuroimaging_brainAtrophy_GWASinput.fam \\\n--covarFile $phenotype/UKB_covarGWAS.txt \\\n--catCovarList sex,assessmentMonth,site,array,batch \\\n--maxCatLevels 106 \\\n--phenoFile $phenotype/UKB_CrossNeuroIDP_noOutliers.txt \\\n--phenoColList ${trait} \\\n--bsize 400 \\\n--minINFO 0.4 \\\n--minMAC 5 \\\n--threads 5 \\\n--gz \\\n--interaction age \\\n--pred ${step1}/Step1_out_all_noOutliers_pred.list \\\n--out $wd/GWAS_${trait}_chr${CHR}_interaction\ndone"
  },
  {
    "objectID": "GWAS.html#format-gwas-data",
    "href": "GWAS.html#format-gwas-data",
    "title": "Genome-wide association study",
    "section": "Format GWAS data",
    "text": "Format GWAS data\n\n\nCode\nlibrary(data.table)\nlibrary(stringr)\n\n# Read in the files for each area and merge them \nall_traits<-c(\"resid_stand\",\"ratio_stand\",\"diff_stand\",\"TBVstand\",\"ICVstand\",\"CSFstand\")\n\nfor(i in all_traits){\n\n    # list all files that belong to this trait\n    fileNames = list.files(pattern = i) \n    fileNames = fileNames[grepl(\".regenie.gz\", fileNames)]\n    \n\n    # exclude the 'interaction' GWAS\n    fileNames = fileNames[!grepl(\"_interaction_\", fileNames)]\n    # include the 'interaction' GWAS\n    #fileNames = fileNames[grepl(\"_interaction_\", fileNames)]\n\n    # sex split analyses\n    #fileNames<-fileNames[grepl(\"_males\", fileNames)]\n    #i = paste0(i,\"_males\")\n\n    # count number of chromosomes\n    if(length(fileNames) != 22){warning(paste0(i, \" GWAS has not got exactly 22 chromosome files!\"))}\n\n    # set object to count number of rows to 0\n    count_rows<-0\n    \n    # object to hold GWAS\n    dat <- data.frame()    \n\n     for(j in fileNames){\n            \n       print(j)\n\n        # read in file\n       file<-fread(j,header=T,data.table=F)\n\n        # if this is an interaction test, only keep interaction row for each SNP\n        if(grepl(\"interaction\", j)){\n            \n            # pull out the interaction effect from the full model\n            file <- file[grepl(\"SNPxage\", file$TEST),] \n\n            # if we're interested to look at attenuation of SNP effects, we can pull out ADD_INT_SNP which should represent the main effect of the tested SNP in the interaction model\n            #file <- file[grepl(\"INT_SNP\", file$TEST),] \n            # remove SNPxage\n            #file <- file[!grepl(\"SNPxage\", file$TEST),]\n        }\n\n        # merge\n        dat <- rbind(dat, file) \n       \n        # count number of rows \n        count_rows<-count_rows+nrow(file)\n     }\n    \n    print(\"Done merging chromosome files\")\n    \n    ## Sanity checks: Does the resulting file have the expected dimensions?\n    if(nrow(dat)!= count_rows){print(\"Resulting merged file does not match the expected numeber of rows\");break}\n    \n    if(ncol(dat) != 14){print(\"Resulting merged files does not match the expected number of columns\"); break}\n    \n    print(\"File has dimensions as expected\")\n    print(dim(dat))\n    \n    # transform pvalues from log transformation to regular\n    # https://www.biostars.org/p/358046/\n    dat$P<-10^(-dat$LOG10P)\n    #if(min(dat$P,na.rm=T)<=0 | max(dat$P,na.rm=T) >= 1){\"Transformed p-value is out of bounds\"; break}\n    summary(dat$P)\n\n    print(\"Done transform p-value column, and p-values are between 0 and 1\")\n\n    # create MAF column\n    dat$MAF<-ifelse(dat$A1FREQ < 0.5, dat$A1FREQ, 1-dat$A1FREQ)\n\n    # remove EXTRA column because it's empty\n    dat$EXTRA <- NULL\n\n    print(paste0(\"This is the file head for \",i))\n    print(head(dat))\n    \n    # define file name to save the merged file\n    fileName <- paste0(\"/CCACE_Shared/Anna_F/BrainAtrophy/data/regenie_out/finalGWAS/GWAS_brainAtrophy_\", i,\"_N43110.gz\")\n    \n    # if this is an interaction run, name accordingly}               \n    if(grepl(\"interaction\", j)){\n        #fileName <- paste0(wd,\"/GWAS_brainAtrophy_\", i,\"_attenuatedMainEffects_N43110.gz\")\n        fileName <- paste0(wd,\"/GWAS_brainAtrophy_\", i,\"_SNPxage_N43110.gz\")\n    }\n\n    # save merged file\n    fwrite(dat, fileName, na = \"NA\", quote = F, sep = \"\\t\", row.names = FALSE, col.names = TRUE)\n    \n    print(paste0(\"Done writing file for \", i))\n}\n\nprint(\"Done all traits\")"
  },
  {
    "objectID": "GWAS.html#manhattan-plot",
    "href": "GWAS.html#manhattan-plot",
    "title": "Genome-wide association study",
    "section": "Manhattan plot",
    "text": "Manhattan plot\nThe scripts below were used to make Figure 4 in the main paper, and Supplementary Plots 15-19.\n\nAlter Manhattan function for better looking annotation\nThis function was saved in a file called manhattan_big.R to be read in with source(manhattan_big.R) in the next step.\n\n\nCode\nmanhattan_big <- function(x, chr=\"CHR\", bp=\"BP\", p=\"P\", snp=\"SNP\", \n                      col=c(\"gray10\", \"gray60\"), chrlabs=NULL,\n                      suggestiveline=-log10(1e-5), genomewideline=-log10(5e-8), \n                      highlight=NULL, logp=TRUE, annotatePval = NULL, annotateTop = TRUE, ...) {\n\n    # Not sure why, but package check will warn without this.\n    CHR=BP=P=index=NULL\n    \n    # Check for sensible dataset\n    ## Make sure you have chr, bp and p columns.\n    if (!(chr %in% names(x))) stop(paste(\"Column\", chr, \"not found!\"))\n    if (!(bp %in% names(x))) stop(paste(\"Column\", bp, \"not found!\"))\n    if (!(p %in% names(x))) stop(paste(\"Column\", p, \"not found!\"))\n    ## warn if you don't have a snp column\n    if (!(snp %in% names(x))) warning(paste(\"No SNP column found. OK unless you're trying to highlight.\"))\n    ## make sure chr, bp, and p columns are numeric.\n    if (!is.numeric(x[[chr]])) stop(paste(chr, \"column should be numeric. Do you have 'X', 'Y', 'MT', etc? If so change to numbers and try again.\"))\n    if (!is.numeric(x[[bp]])) stop(paste(bp, \"column should be numeric.\"))\n    if (!is.numeric(x[[p]])) stop(paste(p, \"column should be numeric.\"))\n    \n    # Create a new data.frame with columns called CHR, BP, and P.\n    # d=data.frame(CHR=x[[chr]], BP=x[[bp]], P=x[[p]], pos = NA, index = NA) # with millions of SNPs, create dataframe at once \n                                                             #  rather than dynamically allocated(see line 72-73, and remove line 87 and line 91 )\n    \n    # If the input data frame has a SNP column, add it to the new data frame you're creating.\n    if (!is.null(x[[snp]])) d = data.frame(CHR=x[[chr]], BP=x[[bp]], P=x[[p]], pos = NA, index = NA ,SNP=x[[snp]], stringsAsFactors = FALSE) else \n        d = data.frame(CHR=x[[chr]], BP=x[[bp]], P=x[[p]], pos = NA, index = NA)\n        \n    \n    # Set positions, ticks, and labels for plotting\n    ## Sort and keep only values where is numeric.\n    #d <- subset(d[order(d$CHR, d$BP), ], (P>0 & P<=1 & is.numeric(P)))\n    #  d <- subset(d, (is.numeric(CHR) & is.numeric(BP) & is.numeric(P)))       ## unused, all three variables are numeric, line:63-65 \n    d <- d[order(d$CHR, d$BP), ]\n    #d$logp <- ifelse(logp, yes=-log10(d$P), no=d$P)\n    if (logp) {\n        d$logp <- -log10(d$P)\n    } else {\n        d$logp <- d$P\n    }\n   # d$pos=NA\n    \n    \n    # Fixes the bug where one chromosome is missing by adding a sequential index column.\n   # d$index=NA\n   # ind = 0\n   # for (i in unique(d$CHR)){\n   #     ind = ind + 1\n   #     d[d$CHR==i,]$index = ind\n   # }\n   d$index = rep.int(seq_along(unique(d$CHR)), times = tapply(d$SNP,d$CHR,length))  # replcace the for loop of line 92-96 to improve efficiency\n    \n    # This section sets up positions and ticks. Ticks should be placed in the\n    # middle of a chromosome. The a new pos column is added that keeps a running\n    # sum of the positions of each successive chromsome. For example:\n    # chr bp pos\n    # 1   1  1\n    # 1   2  2\n    # 2   1  3\n    # 2   2  4\n    # 3   1  5\n    nchr = length(unique(d$CHR))\n    if (nchr==1) { ## For a single chromosome\n        ## Uncomment the next two linex to plot single chr results in Mb\n        #options(scipen=999)\n        #d$pos=d$BP/1e6\n        d$pos=d$BP\n      #  ticks=floor(length(d$pos))/2+1          ## unused, from code line: 169\n        xlabel = paste('Chromosome',unique(d$CHR),'position')\n      #  labs = ticks          ## unused, from code line: 169\n    } else { ## For multiple chromosomes\n        lastbase=0\n        ticks=NULL\n        for (i in unique(d$index)) {\n            if (i==1) {\n                d[d$index==i, ]$pos=d[d$index==i, ]$BP\n            } else {\n        ## chromosome position maybe not start at 1, eg. 9999. So gaps may be produced. \n        lastbase = lastbase +max(d[d$index==(i-1),\"BP\"])   # replace line 128\n        d[d$index == i,\"BP\"] = d[d$index == i,\"BP\"]-min(d[d$index==i,\"BP\"]) +1\n        d[d$index == i, \"pos\"] = d[d$index == i,\"BP\"] + lastbase    # replace line 129\n                # lastbase=lastbase+tail(subset(d,index==i-1)$BP, 1)\n                # d[d$index==i, ]$pos=d[d$index==i, ]$BP+lastbase\n           \n            }\n            # Old way: assumes SNPs evenly distributed\n            # ticks=c(ticks, d[d$index==i, ]$pos[floor(length(d[d$index==i, ]$pos)/2)+1])\n            # New way: doesn't make that assumption\n           # ticks = c(ticks, (min(d[d$index == i,]$pos) + max(d[d$index == i,]$pos))/2 + 1)  # see line 136, to reduce the burden of for loop \n        }\n    ticks <-tapply(d$pos,d$index,quantile,probs=0.5)   # replace line 135\n        xlabel = 'Chromosome'\n        #labs = append(unique(d$CHR),'') ## I forgot what this was here for... if seems to work, remove.\n        labs <- unique(d$CHR)\n    }\n    \n    # Initialize plot\n    xmax = ceiling(max(d$pos) * 1.03)\n    xmin = floor(max(d$pos) * -0.03)\n    \n    # The old way to initialize the plot\n    # plot(NULL, xaxt='n', bty='n', xaxs='i', yaxs='i', xlim=c(xmin,xmax), ylim=c(ymin,ymax),\n    #      xlab=xlabel, ylab=expression(-log[10](italic(p))), las=1, pch=20, ...)\n\n    \n    # The new way to initialize the plot.\n    ## See http://stackoverflow.com/q/23922130/654296\n    ## First, define your default arguments\n    def_args <- list(xaxt='n', bty='n', xaxs='i', yaxs='i', las=1, pch=20,\n                     xlim=c(xmin,xmax), ylim=c(0,ceiling(max(d$logp))),\n                     xlab=xlabel, ylab=expression(-log[10](italic(p))))\n    ## Next, get a list of ... arguments\n    #dotargs <- as.list(match.call())[-1L]\n    dotargs <- list(...)\n    ## And call the plot function passing NA, your ... arguments, and the default\n    ## arguments that were not defined in the ... arguments.\n    do.call(\"plot\", c(NA, dotargs, def_args[!names(def_args) %in% names(dotargs)]))\n    \n    # If manually specifying chromosome labels, ensure a character vector and number of labels matches number chrs.\n    if (!is.null(chrlabs)) {\n        if (is.character(chrlabs)) {\n            if (length(chrlabs)==length(labs)) {\n                labs <- chrlabs\n            } else {\n                warning(\"You're trying to specify chromosome labels but the number of labels != number of chromosomes.\")\n            }\n        } else {\n            warning(\"If you're trying to specify chromosome labels, chrlabs must be a character vector\")\n        }\n    }\n    \n    # Add an axis. \n    if (nchr==1) { #If single chromosome, ticks and labels automatic.\n        axis(1, ...)\n    } else { # if multiple chrs, use the ticks and labels you created above.\n        axis(1, at=ticks, labels=labs, ...)\n    }\n    \n    # Create a vector of alternatiting colors\n    #col=rep(col, max(d$CHR))  # replaced by line 187\n    col = rep_len(col, max(d$index))  ## mean this one?  the results are same\n\n    # Add points to the plot\n    if (nchr==1) {\n        with(d, points(pos, logp, pch=20, col=col[1], ...))\n    } else {\n        # if multiple chromosomes, need to alternate colors and increase the color index (icol) each chr.\n        icol=1\n        for (i in unique(d$index)) {\n            #with(d[d$index==unique(d$index)[i], ], points(pos, logp, col=col[icol], pch=20, ...))\n        points(d[d$index==i,\"pos\"], d[d$index==i,\"logp\"], col=col[icol], pch=20, ...)\n            icol=icol+1\n        }\n    }\n    \n    # Add suggestive and genomewide lines\n    if (suggestiveline) abline(h=suggestiveline, col=\"blue\")\n    if (genomewideline) abline(h=genomewideline, col=\"red\")\n    \n    # Highlight snps from a character vector\n    if (!is.null(highlight)) {\n        if (any(!(highlight %in% d$SNP))) warning(\"You're trying to highlight SNPs that don't exist in your results.\")\n        d.highlight=d[which(d$SNP %in% highlight), ]\n        with(d.highlight, points(pos, logp, col=\"green3\", pch=20, ...)) \n    }\n    \n    # Highlight top SNPs\n    if (!is.null(annotatePval)) {\n        # extract top SNPs at given p-val\n        if (logp) {\n            topHits = subset(d, P <= annotatePval)\n        } else\n            topHits = subset(d, P >= annotatePval)\n        par(xpd = TRUE)\n        # annotate these SNPs\n        if (annotateTop == FALSE) {\n          if (logp) {\n              with(subset(d, P <= annotatePval), \n                   textxy(pos, -log10(P), offset = 0.625, labs = topHits$SNP, cex = 1), ...)\n          } else\n              with(subset(d, P >= annotatePval), \n                   textxy(pos, P, offset = 0.625, labs = topHits$SNP, cex = 1), ...)\n        }\n        else {\n            # could try alternative, annotate top SNP of each sig chr\n            topHits <- topHits[order(topHits$P),]\n            topSNPs <- NULL\n            \n            for (i in unique(topHits$CHR)) {\n                \n                chrSNPs <- topHits[topHits$CHR == i,]\n                topSNPs <- rbind(topSNPs, chrSNPs[1,])\n                \n            }\n            if (logp ){\n                textxy(topSNPs$pos, -log10(topSNPs$P), offset = 0.625, labs = topSNPs$SNP, cex = 1, ...)\n            } else\n              textxy(topSNPs$pos, topSNPs$P, offset = 0.625, labs = topSNPs$SNP, cex = 1, ...)\n        }\n    }  \n    par(xpd = FALSE)\n}\n\n\n\n\nPlot GWAS associations\n\n\nCode\n#######\n# install.packages(\"R.utils\") # to read .gz files\n# install.packages(\"qqman\")\n# install.packages(\"calibrate\")\nlibrary(data.table)\nlibrary(qqman)\nlibrary(calibrate)\n\n# Manhattan plot\n\n# loop through all traits\nall_traits <- c(\"resid_stand\", \"ratio_stand\", \"diff_stand\", \"TBVstand\", \"ICVstand\", \"CSFstand\")\n\nfor(trait_name in all_traits){\n\n    dat = fread(list.files(pattern = paste0(trait_name, \"_N\")), data.table = F)\n\n    ## remove MAF below 0.01\n    dat = dat[dat$MAF > 0.01,]\n    ## remove INFO below 0.8\n    dat = dat[dat$INFO > 0.8,]\n\n    ## for some reason the function plots too many SNP names below indicated threshold - reomve names \n    dat[which(dat$P > 5e-8), \"ID\"] = NA\n\n    source(paste0(wd,\"manhattan_big.R\"))\n    png(filename = paste0(wd, \"/Manhattan\",trait_name,\"_clean.png\"), width = 1575, height=700, units=\"px\")\n\n        layout_matrix <- matrix(1:2, nrow = 1, ncol=2)\n        layout(layout_matrix,  widths = 2:1) #heights = 1.5:1,\n        par(mar=c(5, 4, 4, 2))\n        par(oma = c(3,3,3,3))\n\n    # determine color for each of the comp methods\n    if(trait_name == \"resid_stand\"){col = c(\"#004D40\", \"#00e7c0\");pretty_name = \"Lifetime atrophy (Residual score)\"}\n    if(trait_name == \"ratio_stand\"){col = c(\"#FFC107\", \"#FFE7a1\"); pretty_name = \"Lifetime atrophy (Ratio score)\"}\n    if(trait_name == \"diff_stand\"){col = c(\"#D81B60\", \"#F29ABA\"); pretty_name = \"Lifetime atrophy (Difference score)\"}\n\n\n    # Make MANHATTAN PLOT\n    main = paste0(\"Manhattan plot: \", pretty_name)\n    manhattan_big(dat, \n                main = main, \n                col = col, \n                chr=\"CHROM\",\n                bp=\"GENPOS\",\n                snp=\"ID\",\n                p=\"P\", \n                annotatePval=-log10(5e-12),\n                cex.axis = 1.5)\n\n    # Assess systmeatic bias using Genomic Inflation factor\n    # the genomic inflation factor is defined as the ratio of the median of the empirically observed distribution of test statistics to the expected mean, thus quantifying the extent of the bulk inflation and the excess false positive rate\n    chisq <- qchisq(1-dat$P,1)\n    lambda <- median(chisq)/qchisq(0.5,1)\n\n    # make qq plot\n    main <- paste0(\"QQ plot: \", pretty_name, \" (Lambda = \", round(lambda, digits = 3), \")\")\n    qq(dat$P, main = main, cex = 1, cex.axis = 1.5)\n\n    dev.off()\n}\n\n\n\n\n\n\n\n\n\nMiami plots\nThis script was used to generate Supplementary Plots 20-23 and 28-32.\n\n\nCode\n# Aim is to contrast main effects with the main effects that controlled for interaction effects for each of the three lifetime atrophy phenotypes\n\n# 1. Read in main effects\n# 2. Read in attenuated main effects\n# 3. Create dummy columns to indicate which is which (will be used for split_by column)\n# 4. Merge the two GWAS\n# 5. Call function to contrast the two\n\nlibrary(data.table)\nlibrary(miamiplot)\nlibrary(ggplot2)\n\nall_traits <- c(\"resid_stand\", \"ratio_stand\", \"diff_stand\",\"TBVstand\")\n\n# set INFO filter\nINFOfilter = 0.9\n\n# keep only diallelic SNPs\ndiallelic = TRUE\n\n# MAF filter\nMAFfilter = 0.01\n\n#i = all_traits[1]\n\nfor(i in all_traits){\n\n    # 1. Read in main effects\n    main = fread(list.files(pattern = paste0(i, \"_N\")), data.table = F)\n\n    # dummy var\n    main$split = 1\n\n    # 2. Read in attenuated main effects\n    #att = fread(list.files(pattern = paste0(i, \"_attenuatedMainEffects\")), data.table = F)\n    att = fread(list.files(pattern = paste0(i, \"_SNPxage\")), data.table = F)\n    # dummy var\n    att$split = -1\n\n    # some associations have a P value of 0 which will give an infinite value when transforming it into log\n    # this is the case for 18,794 SNPs in resid GWAS for example, will exclude them here as plot otherwise does not work\n    #att <- att[att$P != 0,]\n\n    # 4. Merge the two GWAS\n    merged = rbind(main, att)\n\n    # if there is an info filter, filter for more reliable SNPs\n    if(any(ls() == \"INFOfilter\")){merged <- merged[merged$INFO > INFOfilter,]}\n\n    # if there is an MAF filter, filter for MAF\n    if(any(ls() == \"MAFfilter\")){merged <- merged[merged$MAF > MAFfilter,]}\n\n    # if diallelic has been set to TRUE, filter for diallelic SNPs only\n    if(any(ls() == \"diallelic\")){   \n        if(diallelic == T){\n            merged <- merged[which(nchar(merged$ALLELE0) == 1),]\n            merged <- merged[which(nchar(merged$ALLELE1) == 1),]\n    }}\n\n    #merged = merged[merged$CHROM == 1,] # for testing\n\n    # summary(merged$P[which(merged$split == -1)])\n    # summary(merged$P[which(merged$split == 1)])\n\n    # clean up big files\n    rm(list = c(\"att\", \"main\"))\n\n    # plot\n    if(grepl(\"diff\", i)){prettyName = \"Difference score\"; col = c(\"#D81B60\", \"#F29ABA\")}\n    if(grepl(\"ratio\", i)){prettyName = \"Ratio score\"; col = c(\"#FFC107\", \"#FFE7a1\")}\n    if(grepl(\"resid\", i)){prettyName = \"Residual score\"; col = c(\"#004D40\", \"#00e7c0\")}\n    if(grepl(\"TBV\", i)){prettyName = \"TBV\"; col = c(\"#82A0D8\",\"#8DDFCB\")}\n\n    ulabel = paste0(\"Main SNP effects\\n(Original GWAS)\")\n    #llabel = paste0( \"Main SNP effects\\n(GWAS including SNP-by-age interaction term)\")\n    llabel = paste0( \"SNP-by-age effects (Interaction GWAS)\")\n\n    p = ggmiami(data = merged, \n            chr = \"CHROM\",\n            pos = \"GENPOS\",\n            p = \"P\",\n            split_by = \"split\", split_at = 0,\n            upper_ylab = ulabel,\n            lower_ylab = llabel,\n            chr_colors = col)\n\n    # save plot\n    fileName = paste0(wd,\"/miami_SNPxage_\",i)\n\n    # if INFO filter was used, write into file name\n    if(any(ls() == \"INFOfilter\")){fileName = paste0(fileName,\"_INFO\", INFOfilter)}\n\n    # if INFO filter was used, write into file name\n    if(any(ls() == \"MAFfilter\")){fileName = paste0(fileName,\"_MAF\", MAFfilter)}\n\n    # if only diallelic SNPs used, say in the name\n    if(any(ls() == \"diallelic\")){\n        if(diallelic == T){\n            fileName = paste0(fileName, \"_diallelic\")\n    }}\n\n    fileName = paste0(fileName, \"_2.png\")\n\n    ggsave(fileName, plot = p, width = 20, height = 10, dpi = 200)\n}\n\n\n\n\n\n\n\n\n\nExamine QQ plots\nThis was used to generate Supplementary Plot 33.\n\n\nCode\n### examine why the qqplot seems so inflated\n# plot qq all, MAF filter & INFO filter, hm3 SNPs (MAF > 0.01; INFO > 0.9), 1000G SNPs (MAF > 0.01; INFO > 0.9)\n\nlibrary(data.table)\nlibrary(qqman)\nlibrary(calibrate)\nlibrary(cowplot)\nlibrary(ggplot2)\n\n# loop through all three traits\nall_traits <- c(\"resid_stand\", \"ratio_stand\", \"diff_stand\")\n\nfor(trait_name in all_traits){\n    dat = fread(list.files(pattern = paste0(trait_name, \"_N\")), data.table = F)#, nrows = 200000\n    #dat = dat[1:20000,]\n    #dat = dat[dat$CHROM == 22,]\n\n    # determine color for each of the comp methods\n    if(trait_name == \"resid_stand\"){pretty_name = \"Residual score\"}\n    if(trait_name == \"ratio_stand\"){pretty_name = \"Ratio score\"}\n    if(trait_name == \"diff_stand\"){pretty_name = \"Difference score\"}\n\n    # set up to save plot\n    fileName <- paste0(\"/CCACE_Shared/Anna_F/BrainAtrophy/data/QQeval_\",trait_name,\".png\")\n    png(filename = fileName, width = 1000, height=1000, units=\"px\")\n\n        layout_matrix <- matrix(c(1,3,5,2,4,6), ncol=2, nrow=3)\n        layout(layout_matrix,  widths = 1:1:1:1:1:1) #heights = 1.5:1,\n        par(mar=c(5, 4, 4, 2))\n        par(oma = c(3,3,3,3))\n\n\n    ## PLOT 1: all SNPs\n    ### Calculate Genomic Inflation factor\n    chisq <- qchisq(1-dat$P,1)\n    lambda <- median(chisq)/qchisq(0.5,1)\n\n    # make qq plot\n    main <- paste0(\"All SNPs: \", pretty_name, \" (Lambda = \", round(lambda, digits = 3), \")\\n\", nrow(dat), \" SNPs\")\n    p1 <- qq(dat$P, main = main, cex = 1, cex.axis = 1.5)\n\n    ###############################\n    ## PLOT 2: Apply INFO and MAF filter\n    dat2 = dat[dat$MAF > 0.01,]\n    dat2 = dat2[dat2$INFO > 0.8,]\n\n    ### Calculate Genomic Inflation factor\n    chisq <- qchisq(1-dat2$P,1)\n    lambda <- median(chisq)/qchisq(0.5,1)\n\n    # make qq plot\n    main <- paste0(\"SNPs passing INFO > 0.9 & MAF > 0.01 filters:\\n\", pretty_name, \" (Lambda = \", round(lambda, digits = 3), \")\\n\", nrow(dat2), \" SNPs\")\n    p2 <- qq(dat2$P, main = main, cex = 1, cex.axis = 1.5)\n\n    ###############################\n    ## PLOT 3: HapMap3 SNPs only\n    hm3 <- \"eur_w_ld_chr/w_hm3.snplist\"\n    hm3SNPs <- fread(hm3)\n    names(hm3SNPs) <- c(\"ID\", \"ALLELE1\", \"ALLELE0\")\n\n    # merge the two data sets and only keep overlapping SNPS that agree in A1 and A2\n    dat2 <- merge(dat, hm3SNPs, by = \"ID\")\n\n    ### Calculate Genomic Inflation factor\n    chisq <- qchisq(1-dat2$P,1)\n    lambda <- median(chisq)/qchisq(0.5,1)\n\n    # make qq plot\n    main <- paste0(\"HapMap3 SNPs: \", pretty_name, \" (Lambda = \", round(lambda, digits = 3), \")\\n\", nrow(dat2), \" SNPs\")\n    p3 <- qq(dat2$P, main = main, cex = 1, cex.axis = 1.5)\n    \n    ###############################\n    ## PLOT 4: 1000 Genomes SNPs only\n    # different reference files: this one is from FUMA website\n    ref = \"1KGphase3EURvariants.txt.gz\"\n    ref1000 <- fread(ref, select = c(\"SNP\", \"A1\", \"A2\"))\n    names(ref1000) <- c(\"ID\", \"ALLELE1\", \"ALLELE0\")\n\n    # merge the two data sets and only keep overlapping SNPS that agree in A1 and A2\n    dat2 <- merge(dat, ref1000, by = \"ID\")\n\n    ### Calculate Genomic Inflation factor\n    chisq <- qchisq(1-dat2$P,1)\n    lambda <- median(chisq)/qchisq(0.5,1)\n\n    # make qq plot\n    main <- paste0(\"1000 Genomes SNPs: \", pretty_name, \" (Lambda = \", round(lambda, digits = 3), \")\\n\", nrow(dat2), \" SNPs\")\n    p4 <- qq(dat2$P, main = main, cex = 1, cex.axis = 1.5)\n    \n    ###############################\n    ## PLOT 5: HapMap3 SNPs removed\n    # isolate SNPs not shared between data and hm3\n    notHm3 = setdiff(dat$ID, hm3SNPs$ID)\n\n    # only retain those \n    dat2 <- dat[dat$ID %in% notHm3,]\n    \n    ### Calculate Genomic Inflation factor\n    chisq <- qchisq(1-dat2$P,1)\n    lambda <- median(chisq)/qchisq(0.5,1)\n\n    # make qq plot\n    main <- paste0(\"All SNPs that are not HapMap3: \", pretty_name, \" (Lambda = \", round(lambda, digits = 3), \")\\n\", nrow(dat2), \" SNPs\")\n    p5 <- qq(dat2$P, main = main, cex = 1, cex.axis = 1.5)\n\n    # save\n    dev.off()\n}"
  },
  {
    "objectID": "h2.html",
    "href": "h2.html",
    "title": "SNP-heritability",
    "section": "",
    "text": "SNP-heritability for all phenotypes of interest was calculated in the GCTA software.\nInput data was prepared using scripts for phenotypic and genotype data.\nEnsure that the resid_stand variable was calculated in the subsample with non-missing data that is analysed in the GCTA analysis."
  },
  {
    "objectID": "h2.html#make-grm",
    "href": "h2.html#make-grm",
    "title": "SNP-heritability",
    "section": "Make GRM",
    "text": "Make GRM\n\n\nCode\n#!/bin/bash\ncd $wd\n# using PLINK files with my own qc (N = 43392)\n# I'd been wondering whether to restrict SNPs for MAF > 0.4 (Andrew and Isy do that in the context of calculating Nhat because outlier estimates are more likely due to mismatches with reference data set)\n# However, I don't see how this would apply to this case\n# Also found this paper: https://www.nature.com/articles/ng.3941\n# Fig.4 shows that heritability estimates in a simulation are about the same in each of the presented MAF bins, even MAF bin 0.4-0.5 produces very similar estimates - decided not to exclude, especially since I already have more stringent cut-off and fewer SNPs than Gails processing\n\ngcta-1.94.1 \\\n--bfile $genotype/ukb_neuroimaging_brainAtrophy_GWASinput \\\n--autosome \\\n--make-grm \\\n--maf 0.01 \\\n--out $wd/UKB_neuroimagingN43392_MAF0.01"
  },
  {
    "objectID": "h2.html#remove-related-individuals",
    "href": "h2.html#remove-related-individuals",
    "title": "SNP-heritability",
    "section": "Remove related individuals",
    "text": "Remove related individuals\nInput data here has already been screened for related individuals with greedyRelated in the genetic QC step, but GTCA will remove even more because it’s stricter.\n\n\nCode\n## identify genetically unrelated individuals\n# cut-off of 0.125 up to third-degree relatives\n# testing 0.025, 0.05, 0.1, 0.125\ngcta-1.94.1 \\\n--grm $wd/UKB_neuroimagingN43392_MAF0.01 \\\n--grm-singleton 0.125 \\\n--out unrelated_singleton0.125 \n\n## prune GRM for relatedness by using cut-off\n#cut=0.125\n\nfor cut in 0.025 0.05 0.1 0.125\ndo\ngcta-1.94.1 \\\n--grm $wd/UKB_neuroimagingN43392_MAF0.01 \\\n--grm-cutoff $cut \\\n--make-grm \\\n--out $wd/UKB_neuroimagingN43392_MAF0.01_unrelated${cut}\ndone"
  },
  {
    "objectID": "h2.html#reml",
    "href": "h2.html#reml",
    "title": "SNP-heritability",
    "section": "REML",
    "text": "REML\n\nAdjust phenotypic variables\nThis step was introduced while testing the reml script because it runs much more efficiently when the phenotypes of interest have been adjusted for covariates prior to running reml. This was deliberately not done for the GWAS associations because we want to be able to interpret SNP effects in units of outcome phenotype (which does not apply to reml where we model a variance component for all SNPs simultaneously).\n\n\nCode\nlibrary(data.table)\n# prep input covar files in R\n# which need to be split into categorical and continuous traits\n covar = fread(\"UKB_covarGWAS.txt\")\n# Cols = c(\"FID\", \"IID\", \"xCoord\", \"yCoord\", \"zCoord\", paste0(\"PC\", 1:40))\n# qcovar = covar[, ..Cols]\n# Cols = c(\"FID\", \"IID\", \"assessmentMonth\",\"site\", \"array\", \"batch\")\n# bcovar = covar[, ..Cols]\n# write.table(qcovar, file = \"qcovar.gcta\", col.names = F, row.names = F, sep = \"\\t\", na = \"NA\")\n# write.table(bcovar, file = \"bcovar.gcta\", col.names = F, row.names = F, sep = \"\\t\", na = \"NA\")\n\n# require one pheno files for each trait: no header, columns are IID, FID, phenotypes\n# could use --mpheno option but it feels like I have more control when I do it separately\n dat = fread(\"UKB_CrossNeuroIDP_noOutliers.txt\")\n# it is recommended to adjust the phenotyped for age and sex effects and standardise them tp z-scores prior to REML analysis because for some traits, the variance in females is larger than in males which cannot be corrected by including the sex effect as a covariate in the REML analysis\n dat = merge(dat, covar, by = c(\"FID\", \"IID\")) \n\n# remove missing values for regression to run\ndat = dat[complete.cases(dat),]\n\n# restrict only to participants with clean genetic data\nkeep = fread(\"/CCACE_Shared/Anna_F/BrainAtrophy/data/GCTA_out/UKB_neuroimagingN43392_MAF0.01_unrelated0.025.grm.id\")\n\ndat = dat[dat$IID %in% keep$V1,]\n\n# build model \n\nfor(trait in c(\"TBVstand\", \"ICVstand\", \"CSFstand\", \"diff_stand\", \"ratio_stand\", \"resid_stand\")){\n  cov1 = paste0(\"age + sex + assessmentMonth + site + xCoord + yCoord + zCoord + array + batch +\")\n  cov2 = paste0(\"PC\", 1:40, collapse = \" + \")\n  \n  formula = paste0(trait , \" ~ \", cov1, cov2)\n  \n  model = lm(as.formula(formula), data = dat)\n  \n  pred = scale(resid(model))\n  \n  summary(pred)\n  \n  # merge data\n  colNames = names(dat)\n  both = cbind(dat,pred)\n  names(both) = c(colNames, paste0(trait, \"_adjusted\"))\n  \n  keep = c(\"FID\",\"IID\",paste0(trait, \"_adjusted\"))\n  \n  write.table(both[, ..keep], file = paste0(trait,\"_adjusted.pheno\"), col.names = F, row.names = F, sep = \"\\t\", na = \"NA\")\n}\n\n\n\n\nRun GCTA-REML\n\n\nCode\n## the covar file must be split up into categorical and quantitative covariates\n## --covar indicates the categorical file, and --qcovar the quantitative files \n\ncut=0.025\n\nfor trait in TBVstand ICVstand CSFstand diff_stand ratio_stand resid_stand\ndo\n\ngcta-1.94.1 \\\n--reml \\\n--grm-bin $genotype/UKB_neuroimagingN43392_MAF0.01_unrelated${cut} \\\n--pheno $phenotype/${trait}_age_sex.pheno \\\n--reml-alg 0 \\\n--covar $phenotype/bcovar.gcta \\\n--qcovar $phenotype/qcovar.gcta \\\n--reml-lrt 1 \\\n--thread-num 10 \\\n--reml-maxit 200 \\\n--out $out/gcta_${trait}_unrelated${cut}\n\ndone"
  },
  {
    "objectID": "h2.html#snp-by-age-interactions",
    "href": "h2.html#snp-by-age-interactions",
    "title": "SNP-heritability",
    "section": "SNP-by-age interactions",
    "text": "SNP-by-age interactions\n\n\nCode\n#!/bin/bash\n\n# R\n# library(data.table)\n# setwd(\"/CCACE_Shared/Anna_F/BrainAtrophy/data\")\n# prep input covar files in R\n# covar = fread(\"UKB_covarGWAS.txt\")\n\n# save age variable as plain text file\n# write.table(covar[,c(\"FID\", \"IID\", \"age\")], file = \"UKB_age.gxe\", col.names = F, row.names = F, sep = \"\\t\", na = \"NA\")\n\n\n#### Here I am using the same script as in GCTA_reml_adjusted.sh only that I am including a gxe interaction term\n#### Chose to keep the phenotype residualised for age because we want to control for a main effect of age (so we're comparing people of similar ages), but then we're asking whether the there is a significant GxAge variance contribution to the toal phenotypic variance \n#### also changing --reml-lrt to 2 because I am assuming there are two genetic variance components included in the analysis (main genetic effect and then the gene environment effect)\n\ncut=0.025\n\ngcta-1.94.1 \\\n--reml \\\n--grm-bin $genotype/UKB_neuroimagingN43392_MAF0.01_unrelated${cut} \\\n--pheno $phenotype/${trait}_adjusted.pheno \\\n--reml-alg 0 \\\n--reml-lrt 2 \\\n--gxe $phenotype/UKB_age.gxe \\\n--thread-num 30 \\\n--reml-maxit 200 \\\n--out $out/gcta_${trait}_unrelated${cut}_GxAge"
  },
  {
    "objectID": "HCP.html#determine-most-appropriate-age-cut-off-for-hcp",
    "href": "HCP.html#determine-most-appropriate-age-cut-off-for-hcp",
    "title": "Human Connectome Project",
    "section": "Determine most appropriate age cut-off for HCP",
    "text": "Determine most appropriate age cut-off for HCP\n\nthe plot shows that the ICV-age correlation disappears at age 29 and below\nalso this sample includes only 3 out of the 10 participants that had to get excluded due to ICV < TBV\n\n\n\nCode\n### determne which age is a good one to use for cut-off\n# determine age cut-offs to iterate through\nageCut = seq(from = min(HCP$Age)+1, to = max(HCP$Age), by = 1)\n\n# object to store results\nstoreNames = c(\"Age cut-off value\", \"Cor\", \"p\", \"ci_l\", \"ci_u\", \"Measure\")\nstore = as.data.frame(matrix(nrow = length(ageCut), ncol = length(storeNames)))\nnames(store) = storeNames\n\n# iterate over each age cut-off and calculate scores\nfor(i in ageCut){\n  # store which age cut off iteration this is\n  loc = which(is.na(store$`Age cut-off value`))[1]\n  store[loc,\"Age cut-off value\"] = i\n  \n  # cut sample\n  Youngdata = HCP[which(HCP$Age <= i),]\n  \n  # calculate correlations\n  ## ICVerence\n  store[loc,\"Cor\"] =\n    with(Youngdata, cor.test(Age, ICV))$estimate\n  \n  store[loc,\"p\"] =\n    with(Youngdata, cor.test(Age, ICV))$p.value\n  \n  store[loc,\"ci_l\"] =\n    with(Youngdata, cor.test(Age, ICV))$conf.int[1]\n  \n  store[loc,\"ci_u\"] =\n    with(Youngdata, cor.test(Age, ICV))$conf.int[2]\n  \n  store[loc,\"Measure\"] = \"ICV\"\n}\n\nggplot(data = store)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u), alpha = 0.3)+\n  geom_vline(xintercept = -0.1, color = \"grey\")+\n  geom_hline(yintercept = 31, color = \"grey\")+\n  xlab(\"Correlation with age\")+\n  ylab(\"Age cut-off used to subset\\nsample into a younger subsample\")+\n  scale_y_continuous(limits = c(21, 37.5), breaks = seq(from = 20, to = 36, by = 2))+\n  ggtitle(\"Age-ICV correlation in HCP\")+\n  theme_bw()+\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "HCP.html#explore-age-correlations-in-sample-of-or-below-31-years",
    "href": "HCP.html#explore-age-correlations-in-sample-of-or-below-31-years",
    "title": "Human Connectome Project",
    "section": "Explore age correlations in sample of or below 31 years",
    "text": "Explore age correlations in sample of or below 31 years\n\nWhen we restrict our sample to <= 31 years, the age correlations disappear\n\n\n\nCode\nHCPyoung = HCP[which(HCP$Age <= 31),]\n\n# calculate atrophy scores\nHCPyoung$diff = HCPyoung$ICV - HCPyoung$TBV\nHCPyoung$ratio = HCPyoung$TBV / HCPyoung$ICV\n\n# delete those from data \n#HCPyoung=HCPyoung[-which(HCPyoung$diff < 0),]\n\n# estimate residual model\nmodel <- lm(TBV ~ ICV, data = HCPyoung)\nHCPyoung$resid = as.vector(resid(model, na.rm=T))\n\n\ncorICV = ggplot(data = HCPyoung, aes(x = ICV, y = Age))+\n  geom_point(alpha = 0.2)+\n  theme_bw()+\n  geom_smooth(method='lm')+\n  stat_cor(method = \"pearson\",color =\"blue\")+\n  ylab(\"Age in years\")+\n  theme(panel.border = element_blank())\n\ncorTBV = ggplot(data = HCPyoung, aes(x = TBV, y = Age))+\n  geom_point(alpha = 0.2)+\n  theme_bw()+\n  geom_smooth(method='lm')+\n  stat_cor(method = \"pearson\",color =\"blue\")+\n  ylab(\"Age in years\")+\n  theme(panel.border = element_blank())\n\n\ncordiff = ggplot(data = HCPyoung, aes(x = diff, y = Age))+\n  geom_point(alpha = 0.2)+\n  theme_bw()+\n  geom_smooth(method='lm')+\n  stat_cor(method = \"pearson\",color =\"blue\")+\n  ylab(\"Age in years\")+\n  xlab(\"Difference score\")+\n  theme(panel.border = element_blank())\n\ncorratio = ggplot(data = HCPyoung, aes(x = ratio, y = Age))+\n  geom_point(alpha = 0.2)+\n  theme_bw()+\n  geom_smooth(method='lm')+\n  stat_cor(method = \"pearson\",color =\"blue\")+\n  ylab(\"Age in years\")+\n  xlab(\"Ratio score\")+\n  theme(panel.border = element_blank())\n\ncorresid = ggplot(data = HCPyoung, aes(x = resid, y = Age))+\n  geom_point(alpha = 0.2)+\n  theme_bw()+\n  geom_smooth(method='lm')+\n  stat_cor(method = \"pearson\",color =\"blue\")+\n  ylab(\"Age in years\")+\n  xlab(\"Residual score\")+\n  theme(panel.border = element_blank())\n\nggarrange(corICV, corTBV, cordiff, corratio, corresid, common.legend=T, legend = \"bottom\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lifetime brain atrophy GWAS",
    "section": "",
    "text": "Here I display the analysis code to accompany our lifetime brain atrophy (LBA) genetics project. For consistency, the analysis steps outlined below match the order of presentation in the manuscript but does not necessarily reflect the order in which analyses were executed.\nPre-registration: https://osf.io/gydmw/\n\nAnalysis steps\nData preparation\n\nUK Biobank\n1.1. Phenotypic data\n1.2. Neuroimaging data\n1.3. Genetic data\nLBC1936\n2.1. Phenotypic data\n2.2. Neuroimaging data\nHuman Connectome Project (HCP)\nGeneration Scotland Subsample (STRADL)\nMRi-Share\n\nDescription and characterisation of the LBA phenotype\n\nMeasures of LBA predict brain atrophy rated by neuroradiological experts, as well as other ageing-related health traits such as frailty and cognitive ability\nMeasures of LBA indicate age-associated brain shrinkage\n2.1. Single time-point MRI measures age correlation\n2.2. Repeated MRI measures age correlations in LBC1936\nLBA moderately captures within-person atrophic changes that were longitudinally observed between two MRI scans a few years apart\n\nGenome-wide association study of LBA\n\nSNP-heritability\nGWAS analysis\nGenetic correlations\n\n\n\n\n\nGWAS Manhattan plot for LBA (residual score)"
  },
  {
    "objectID": "LBC_neuro.html",
    "href": "LBC_neuro.html",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "",
    "text": "Code displayed here was used to obtain neuroimaging measures: TBV, ICV, LBA (difference, ratio, residual scores). These measures were obtained for all waves from cross-sectionally processed data, and from longitudinal data considering waves 2 and 5.\nThe LBC neuroimaging data was processed with FS v5.1, which does not produce BrainSegNotVent estimates that we pre-registered to use across all samples. Instead, we derive TBV as the sum of GMV (cortical and subcortical should also include cerebellum) + cerebellum WMV + cerebral WMV, as was done in a previous paper. One participant was excluded because TBV estimate was larger than ICV estimate - total of 269 participants with two assessments."
  },
  {
    "objectID": "LBC_neuro.html#load-packages",
    "href": "LBC_neuro.html#load-packages",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "Load packages",
    "text": "Load packages\n\n\nCode\nlibrary(cowplot)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(data.table)\nlibrary(stringr)\nlibrary(dplyr)"
  },
  {
    "objectID": "LBC_neuro.html#define-functions",
    "href": "LBC_neuro.html#define-functions",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "Define functions",
    "text": "Define functions\nFunctions plot_hist and descriptives expect input data set to contain variables called diff, ratio, resid. plot_hist can also handle diff_stand, ratio_stand, resid_stand and will add an extra x-axis if input are standardised variables.\ndescriptives gives a table of descriptive statistics for TBV, ICV and LBA phenotypes.\n\n\nCode\nplot_hist <- function(dat = dat, var = \"diff_stand\", split_sample_by = NULL){\n  # install packages if they don't already exits\n  packages = c(\"ggplot2\",\"stringr\", \"tidyr\", \"dplyr\")\n  install.packages(setdiff(packages, rownames(installed.packages())))\n  # load packages\n  library(ggplot2)\n  library(stringr)\n  library(tidyr)\n  library(dplyr)\n\n  # make sure input data is data.frame\n  dat = as.data.frame(dat)\n  # rename for simplicity\n  dat$var = dat[,var]\n\n  # calculate summary stats\n    df_stats <-\n        dat %>%\n        summarize(\n          mean = mean(var, na.rm=T),\n          median = median(var, na.rm=T)\n        ) %>%\n        gather(key = Statistic, value = value, mean:median)\n\n    # calculate SD cutoffs\n    insert = c(\"+2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) + 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    insert = c(\"-2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) - 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    # format\n    df_stats$value <- as.numeric(df_stats$value)\n\n    # consider one-sided nature of cut-off\n    # if difference score, we use the upper 2 SD limit\n    # if ratio or residual score, we use the lower 2 SD limit\n    if(var == \"diff\" | var == \"diff_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"-2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }else if(var == \"ratio\" | var == \"resid\" | var == \"ratio_stand\" | var == \"resid_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"+2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }\n\n\n  # PLOT\n  # different output when there is a \"sample\" column\n  if(is.null(split_sample_by)){\n      plot = ggplot(dat, aes(x = var))+\n          geom_histogram(bins = 100, alpha = 0.5, fill = \"#56B4E9\")+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n\n\n  }else if(!is.null(split_sample_by)){\n\n    if(length(which(names(dat) == split_sample_by)) == 0){\n      message(paste0(\"You have indicated that you wanted to group plotted values by \", split_sample_by,\", but the data contains no such column.\")); break\n    }\n\n    # incorporate grouping variable\n    names(dat)[which(names(dat) == split_sample_by)] = \"split_sample_by\"\n    # make sure its a factor\n    dat$split_sample_by = as.factor(dat$split_sample_by)\n\n    colors = c(\"#56B4E9\",\"#009E73\", \"#E69F00\") # \"#79AC78\" #grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]\n    colors = colors[1:length(unique(dat$split_sample_by))]\n\n      plot = ggplot(dat)+\n          geom_histogram(aes(x = var, fill = split_sample_by), bins = 100, alpha = 0.5)+\n          scale_fill_manual(values = colors, name = split_sample_by)+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n  }\n\n    # make second x-axis if we're working with standardised variables\n    if(length(grep(\"_stand\", var)) != 0){\n\n      # calculate mean from original variable\n      varOr = str_remove(var, \"_stand\")\n      mean = mean(dat[,varOr], na.rm=T)\n      sd = sd(dat[,varOr], na.rm=T)\n\n      # add secondary x axis\n      plot = plot+\n         scale_x_continuous(sec.axis = sec_axis(name = \"Raw values\", trans=~.*sd+mean))\n\n    }\n\n  plot = plot+theme(panel.border = element_blank())\n\n  return(plot)\n}\n\n# this onyl works for the correct naming of the variable names to diff, ratio and resid\ndescriptives = function(samples = c(\"HCP\", \"Share\", \"both\")){\n  # define statistics to include\n  stats = c(\"N\", \"TBV: Mean (SD)\", \"ICV: Mean (SD)\", \"cor(ICV,TBV)\",\n            \"*Difference score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\",\n            \"*Ratio score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\",\n            \"*Residual score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\")\n\n  # object to hold results\n  res = as.data.frame(matrix(ncol = length(samples)+1, nrow = length(stats)))\n  names(res) = c(\"Statistic\", samples)\n  res$Statistic = stats\n\n  for(i in samples){\n    # pull sample\n    dat = as.data.frame(get(i))\n\n    # N\n    N = sum(!is.na(dat$diff))\n    res[which(res$Statistic == \"N\"), which(names(res) == i)] = N\n\n    # TBV: Mean (SD)\n    mean = round(mean(dat$TBV, na.rm = T), digits = 2)\n    SD = signif(sd(dat$TBV, na.rm = T), digits = 2)\n    res[which(res$Statistic == \"TBV: Mean (SD)\"), which(names(res) == i)] = paste0(mean, \" (\", SD,\")\")\n\n    # ICV: Mean (SD)\n    mean = round(mean(dat$ICV, na.rm = T), digits = 2)\n    SD = signif(sd(dat$ICV, na.rm = T), digits = 2)\n    res[which(res$Statistic == \"ICV: Mean (SD)\"), which(names(res) == i)] = paste0(mean, \" (\", SD,\")\")\n\n    # ICV TBV correlation\n    cor = round(cor.test(dat$ICV, dat$TBV)$estimate, digits = 2)\n    res[which(res$Statistic == \"cor(ICV,TBV)\"), which(names(res) == i)] = cor\n\n    # Cycle through different scores\n    for(j in c(\"Difference\", \"Ratio\", \"Resid\")){\n        # determine variable that matches the right score\n        if(j == \"Difference\"){\n          VarName = \"diff\"\n        }else if(j == \"Ratio\"){\n          VarName = \"ratio\"\n        }else if(j == \"Resid\"){\n          VarName = \"resid\"\n        }\n\n        dat$var = dat[,VarName]\n\n        ### Calculate mean and SD\n        mean = round(mean(dat$var, na.rm=T), digits = 2)\n        sd = round(sd(dat$var, na.rm=T), digits = 2)\n        # find correct position in res to store result\n        index = grep(j, res$Statistic)\n        Cand = grep(\"Mean\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        # store mean result\n        res[pos, which(names(res) == i)] = paste0(mean, \" (\", sd, \")\")\n\n        ### Calculate median\n        median = round(median(dat$var, na.rm=T), digits = 2)\n        #store median result\n        Cand = grep(\"Median\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = median\n\n        ### Calculate range\n        min = round(min(dat$var, na.rm = T), digits = 2)\n        max = round(max(dat$var, na.rm = T), digits = 2)\n        # store results\n        Cand = grep(\"Range\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = paste0(min, \" to \", max)\n\n        ## Calculate variance\n        variance = signif(var(dat$var, na.rm = T), digit = 2)\n        # store variance result\n        Cand = grep(\"Variance\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = variance\n\n        ### calculate cut-off\n        if(j == \"Difference\"){\n          cutOff = mean(dat$var, na.rm = T)+(2*sd(dat$var, na.rm = T))\n        }else{\n            cutOff = mean(dat$var, na.rm = T)-(2*sd(dat$var, na.rm = T))\n        }\n        # store results\n        Cand = grep(\"Cut\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = round(cutOff, digit = 1)\n    }\n  }\n\n  return(res)\n}\n\n\n# define function to make ggplots prettier\nmake_pretty <- function(){\n  theme(text = element_text(size=6),\n        axis.text.x = element_text(size=4, colour='#696969'),\n        axis.text.y = element_blank(),\n        plot.title = element_text(face=\"bold\", colour='#1A1A1A', size=6, hjust = 0.5),\n        axis.title.x = element_text(face=\"bold\", colour='#1A1A1A', size=6),\n        axis.title.y = element_text(face=\"bold\", colour='#1A1A1A', size=6),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x.top = element_text(color = \"grey\", size=6, hjust=0))\n}\n\n# this onyl works for the correct naming of the variable names to diff, ratio and resid\ndescriptives = function(samples = c(\"HCP\", \"Share\", \"both\")){\n  # define statistics to include\n  stats = c(\"N\", \"TBV: Mean (SD)\", \"ICV: Mean (SD)\", \"cor(ICV,TBV)\",\n            \"*Difference score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\",\n            \"*Ratio score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\",\n            \"*Residual score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\")\n\n  # object to hold results\n  res = as.data.frame(matrix(ncol = length(samples)+1, nrow = length(stats)))\n  names(res) = c(\"Statistic\", samples)\n  res$Statistic = stats\n\n  for(i in samples){\n    # pull sample\n    dat = as.data.frame(get(i))\n\n    # N\n    N = sum(!is.na(dat$diff))\n    res[which(res$Statistic == \"N\"), which(names(res) == i)] = N\n\n    # TBV: Mean (SD)\n    mean = round(mean(dat$TBV, na.rm = T), digits = 2)\n    SD = signif(sd(dat$TBV, na.rm = T), digits = 2)\n    res[which(res$Statistic == \"TBV: Mean (SD)\"), which(names(res) == i)] = paste0(mean, \" (\", SD,\")\")\n\n    # ICV: Mean (SD)\n    mean = round(mean(dat$ICV, na.rm = T), digits = 2)\n    SD = signif(sd(dat$ICV, na.rm = T), digits = 2)\n    res[which(res$Statistic == \"ICV: Mean (SD)\"), which(names(res) == i)] = paste0(mean, \" (\", SD,\")\")\n\n    # ICV TBV correlation\n    cor = round(cor.test(dat$ICV, dat$TBV)$estimate, digits = 2)\n    res[which(res$Statistic == \"cor(ICV,TBV)\"), which(names(res) == i)] = cor\n\n    # Cycle through different scores\n    for(j in c(\"Difference\", \"Ratio\", \"Resid\")){\n        # determine variable that matches the right score\n        if(j == \"Difference\"){\n          VarName = \"diff\"\n        }else if(j == \"Ratio\"){\n          VarName = \"ratio\"\n        }else if(j == \"Resid\"){\n          VarName = \"resid\"\n        }\n\n        dat$var = dat[,VarName]\n\n        ### Calculate mean and SD\n        mean = round(mean(dat$var, na.rm=T), digits = 2)\n        sd = round(sd(dat$var, na.rm=T), digits = 2)\n        # find correct position in res to store result\n        index = grep(j, res$Statistic)\n        Cand = grep(\"Mean\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        # store mean result\n        res[pos, which(names(res) == i)] = paste0(mean, \" (\", sd, \")\")\n\n        ### Calculate median\n        median = round(median(dat$var, na.rm=T), digits = 2)\n        #store median result\n        Cand = grep(\"Median\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = median\n\n        ### Calculate range\n        min = round(min(dat$var, na.rm = T), digits = 2)\n        max = round(max(dat$var, na.rm = T), digits = 2)\n        # store results\n        Cand = grep(\"Range\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = paste0(min, \" to \", max)\n\n        ## Calculate variance\n        variance = signif(var(dat$var, na.rm = T), digit = 2)\n        # store variance result\n        Cand = grep(\"Variance\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = variance\n\n        ### calculate cut-off\n        if(j == \"Difference\"){\n          cutOff = mean(dat$var, na.rm = T)+(2*sd(dat$var, na.rm = T))\n        }else{\n            cutOff = mean(dat$var, na.rm = T)-(2*sd(dat$var, na.rm = T))\n        }\n        # store results\n        Cand = grep(\"Cut\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = round(cutOff, digit = 1)\n    }\n  }\n\n  return(res)\n}"
  },
  {
    "objectID": "LBC_neuro.html#extract-from-cross-sectional-fs-processing-stream-output",
    "href": "LBC_neuro.html#extract-from-cross-sectional-fs-processing-stream-output",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "Extract from cross-sectional FS processing stream output",
    "text": "Extract from cross-sectional FS processing stream output\nData from the cross-sectional processing stream is used in all cases where samples are compared, as well as in analyses where cross-sectional and longitudinal measures are directly compared.\nThe naming is kind of confusing and here I am using the naming as the folder are called. There are wave 1, wave 2, wave 3 and wave 4 (which technically are waves 2,3,4,5 and scans 1,2,3,4), but that’s what they were called during processing.\n\nTabulate FS data\n\n\nCode\n#!/bin/bash\n# Extract FreeSurfer variables (volume, area, thickness) for our LBC1936 W2 (scan 1) cross-sectional subjects\n# Colin Buchanan, 2022\n# Colin's script was adapted to extract cross-sectional estimates from LBC at wave 5\n\n\n# first, create subjects list of participants with wave 5 data\n#R\n\n#setwd(\"/Brain_Imaging/LBC1936_FS_long/LBC_long_W4\")\n# note that all subjects in this directory without a wave are the templates and the ones with a wave but without 'long' should be the longitudinally processed waves\n# list dirs\n#dirs = dir()\n# keep only W4 because it's visit 5\n#dirs = dirs[grepl(\"W4\", dirs)]\n# remove long scans\n#dirs = dirs[!grepl(\".long.\", dirs)]\n\n#write.table(dirs, \"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave3.csv\", col.names=F, row.names=F, quote=F, sep=\"\\t\")\n## use this ID list to extract measurements from FS dirs\n\n\nFREESURFER_HOME=/Cluster_Filespace/mharris4/LBC_long_W4/freesurfer510\n$FREESURFER_HOME/SetUpFreeSurfer.sh\nout=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\nref1=\"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave1.csv\"\nref2=\"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave2.csv\"\nref3=\"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave3.csv\"\nref4=\"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave4.csv\"\n\n\nSUBJECTS_DIR=/Brain_Imaging/LBC1936_FS_long/LBC_long_W4\n\n# Wave1 \nasegstats2table --subjectsfile $ref1 --meas volume --common-segs --delimiter comma --tablefile \"${out}/LBC1936_global_w1_cross.csv\"\n\n# Wave2 \nasegstats2table --subjectsfile $ref2 --meas volume --common-segs --delimiter comma --tablefile \"${out}/LBC1936_global_w2_cross.csv\"\n\n# Wave3\nasegstats2table --subjectsfile $ref3 --meas volume --common-segs --delimiter comma --tablefile \"${out}/LBC1936_global_w3_cross.csv\"\n\n# Wave4\nasegstats2table --subjectsfile $ref4 --meas volume --common-segs --delimiter comma --tablefile \"${out}/LBC1936_global_w4_cross.csv\"\n\n\n\n\nFormat Wave 1\n\n\nCode\n# read in cross-stats\n## wave 1\ncrossDir=\"/Brain_Imaging/LBC1936_FS_long/freesurfer_crosssect_stats\"\ncross = fread(paste0(crossDir, \"/global_w2.csv\"), data.table=F)\n# the naming here is super confusing because it switches from wave 1 to scan 1 ...\n# but the actual participant names definitively say which wave this scan is from and it's (wave 1 naming, which corresponds to the second wave but first scan - so confusing)\n# I will name it below to reflect the folder names (i.e., wave 1 in this case)\nnames(cross) = gsub(\"-\", \".\", names(cross), fixed = T)\n\n# format each of those variables to long format\n## TotalGrayVol\n## Right.Cerebellum.White.Matter\n## Left.Cerebellum.White.Matter\ncrossLBC = cross[ , c(1, grep(\"TotalGrayVol|Right.Cerebellum.White.Matter|Left.Cerebellum.White.Matter|CorticalWhiteMatterVol|CSF|IntraCranialVol\", names(cross))) ]\n# remove lh & rhCorticalWhiteMatterVol (because whole measure is also included)\ncrossLBC = crossLBC[, !grepl(\"rh|lh\", names(crossLBC))]\n\n# calculate sum of the regions\ncrossLBC$TBV = rowSums(crossLBC[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n\n# rename for easier names\nnames(crossLBC)[grep(\"Measure:volume\", names(crossLBC))] = \"lbc36no\"\nnames(crossLBC)[grep(\"IntraCranialVol\", names(crossLBC))] = \"ICV\"\n\ncrossLBC = crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\")]\n\n# two participants have a smaller ICV than TBV sum(crossLBC$ICV - crossLBC$TBV < 0)\n# must be an error (LBC360213 & LBC361303)\ncrossLBC = crossLBC[-which(crossLBC$ICV - crossLBC$TBV <0),]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\ncrossLBC$ICV = crossLBC$ICV/1000\ncrossLBC$TBV = crossLBC$TBV/1000\n\n# estimate brain atrophy from single MRI scan\ncrossLBC$diff = crossLBC$ICV - crossLBC$TBV\ncrossLBC$ratio = crossLBC$TBV / crossLBC$ICV\n\n##### derive the residuals for each time point separately \nmodel <- lm(TBV ~ ICV, data = crossLBC)\ncrossLBC$resid = resid(model)\n\n# standardise variables within one time-point\ncrossLBC$resid_stand = as.vector(scale(crossLBC$resid))\ncrossLBC$diff_stand = as.vector(scale(crossLBC$diff))\ncrossLBC$ratio_stand = as.vector(scale(crossLBC$ratio))\ncrossLBC$TBVstand = as.vector(scale(crossLBC$TBV))\ncrossLBC$ICVstand = as.vector(scale(crossLBC$ICV))\ncrossLBC$CSFstand = as.vector(scale(crossLBC$CSF))\n\n# rename participant labels to match global naming\ncrossLBC$lbc36no = stringr::str_remove(crossLBC$lbc36no, pattern = \"_W1\")\n\n# store as txt file\nfwrite(crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\")], paste0(wd, \"/LBC1936_crossNeuroWave1.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nFormat Wave 2\n\n\nCode\n# read in cross-stats\n## wave 2\ncrossDir=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\ncross = fread(paste0(crossDir, \"/LBC1936_global_w2_cross.csv\"), data.table=F)\nnames(cross) = gsub(\"-\", \".\", names(cross), fixed = T)\n\n# format each of those variables to long format\n## TotalGrayVol\n## Right.Cerebellum.White.Matter\n## Left.Cerebellum.White.Matter\ncrossLBC = cross[ , c(1, grep(\"TotalGrayVol|Right.Cerebellum.White.Matter|Left.Cerebellum.White.Matter|CorticalWhiteMatterVol|CSF|IntraCranialVol\", names(cross))) ]\n# remove lh & rhCorticalWhiteMatterVol (because whole measure is also included)\ncrossLBC = crossLBC[, !grepl(\"rh|lh\", names(crossLBC))]\n\n# calculate sum of the regions\ncrossLBC$TBV = rowSums(crossLBC[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n\n# rename for easier names\nnames(crossLBC)[grep(\"Measure:volume\", names(crossLBC))] = \"lbc36no\"\nnames(crossLBC)[grep(\"IntraCranialVol\", names(crossLBC))] = \"ICV\"\n\ncrossLBC = crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\")]\n\n# two participants have a smaller ICV than TBV sum(crossLBC$ICV - crossLBC$TBV < 0)\n# must be an error (LBC360213 & LBC361303)\ncrossLBC = crossLBC[-which(crossLBC$ICV - crossLBC$TBV <0),]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\ncrossLBC$ICV = crossLBC$ICV/1000\ncrossLBC$TBV = crossLBC$TBV/1000\n\n# estimate brain atrophy from single MRI scan\ncrossLBC$diff = crossLBC$ICV - crossLBC$TBV\ncrossLBC$ratio = crossLBC$TBV / crossLBC$ICV\n\n##### derive the residuals for each time point separately \nmodel <- lm(TBV ~ ICV, data = crossLBC)\ncrossLBC$resid = resid(model)\n\n# standardise variables within one time-point\ncrossLBC$resid_stand = as.vector(scale(crossLBC$resid))\ncrossLBC$diff_stand = as.vector(scale(crossLBC$diff))\ncrossLBC$ratio_stand = as.vector(scale(crossLBC$ratio))\ncrossLBC$TBVstand = as.vector(scale(crossLBC$TBV))\ncrossLBC$ICVstand = as.vector(scale(crossLBC$ICV))\ncrossLBC$CSFstand = as.vector(scale(crossLBC$CSF))\n\n# rename participant labels to match global naming\ncrossLBC$lbc36no = stringr::str_remove(crossLBC$lbc36no, pattern = \"_W2\")\n\n# store as txt file\nfwrite(crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\")], paste0(wd, \"/LBC1936_crossNeuroWave2.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nFormat Wave 3\n\n\nCode\n# read in cross-stats\n## wave 3\ncrossDir=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\ncross = fread(paste0(crossDir, \"/LBC1936_global_w3_cross.csv\"), data.table=F)\nnames(cross) = gsub(\"-\", \".\", names(cross), fixed = T)\n\n# format each of those variables to long format\n## TotalGrayVol\n## Right.Cerebellum.White.Matter\n## Left.Cerebellum.White.Matter\ncrossLBC = cross[ , c(1, grep(\"TotalGrayVol|Right.Cerebellum.White.Matter|Left.Cerebellum.White.Matter|CorticalWhiteMatterVol|CSF|IntraCranialVol\", names(cross))) ]\n# remove lh & rhCorticalWhiteMatterVol (because whole measure is also included)\ncrossLBC = crossLBC[, !grepl(\"rh|lh\", names(crossLBC))]\n\n# calculate sum of the regions\ncrossLBC$TBV = rowSums(crossLBC[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n\n# rename for easier names\nnames(crossLBC)[grep(\"Measure:volume\", names(crossLBC))] = \"lbc36no\"\nnames(crossLBC)[grep(\"IntraCranialVol\", names(crossLBC))] = \"ICV\"\n\ncrossLBC = crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\")]\n\n# no participants have smaller ICV than TBV\n#crossLBC = crossLBC[-which(crossLBC$ICV - crossLBC$TBV <0),]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\ncrossLBC$ICV = crossLBC$ICV/1000\ncrossLBC$TBV = crossLBC$TBV/1000\n\n# estimate brain atrophy from single MRI scan\ncrossLBC$diff = crossLBC$ICV - crossLBC$TBV\ncrossLBC$ratio = crossLBC$TBV / crossLBC$ICV\n\n##### derive the residuals for each time point separately \nmodel <- lm(TBV ~ ICV, data = crossLBC)\ncrossLBC$resid = resid(model)\n\n# standardise variables within one time-point\ncrossLBC$resid_stand = as.vector(scale(crossLBC$resid))\ncrossLBC$diff_stand = as.vector(scale(crossLBC$diff))\ncrossLBC$ratio_stand = as.vector(scale(crossLBC$ratio))\ncrossLBC$TBVstand = as.vector(scale(crossLBC$TBV))\ncrossLBC$ICVstand = as.vector(scale(crossLBC$ICV))\ncrossLBC$CSFstand = as.vector(scale(crossLBC$CSF))\n\n# rename participant labels to match global naming\ncrossLBC$lbc36no = stringr::str_remove(crossLBC$lbc36no, pattern = \"_W3\")\n\n# store as txt file\nfwrite(crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\")], paste0(wd, \"/LBC1936_crossNeuroWave3.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nFormat Wave 4\n\n\nCode\n# read in cross-stats\n## wave 4\ncrossDir=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\ncross = fread(paste0(crossDir, \"/LBC1936_global_w4_cross.csv\"), data.table=F)\nnames(cross) = gsub(\"-\", \".\", names(cross), fixed = T)\n\n# format each of those variables to long format\n## TotalGrayVol\n## Right.Cerebellum.White.Matter\n## Left.Cerebellum.White.Matter\ncrossLBC = cross[ , c(1, grep(\"TotalGrayVol|Right.Cerebellum.White.Matter|Left.Cerebellum.White.Matter|CorticalWhiteMatterVol|CSF|IntraCranialVol\", names(cross))) ]\n# remove lh & rhCorticalWhiteMatterVol (because whole measure is also included)\ncrossLBC = crossLBC[, !grepl(\"rh|lh\", names(crossLBC))]\n\n# calculate sum of the regions\ncrossLBC$TBV = rowSums(crossLBC[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n\n# rename for easier names\nnames(crossLBC)[grep(\"Measure:volume\", names(crossLBC))] = \"lbc36no\"\nnames(crossLBC)[grep(\"IntraCranialVol\", names(crossLBC))] = \"ICV\"\n\ncrossLBC = crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\")]\n\n# no participants have smaller ICV than TBV\n#crossLBC = crossLBC[-which(crossLBC$ICV - crossLBC$TBV <0),]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\ncrossLBC$ICV = crossLBC$ICV/1000\ncrossLBC$TBV = crossLBC$TBV/1000\n\n# estimate brain atrophy from single MRI scan\ncrossLBC$diff = crossLBC$ICV - crossLBC$TBV\ncrossLBC$ratio = crossLBC$TBV / crossLBC$ICV\n\n##### derive the residuals for each time point separately \nmodel <- lm(TBV ~ ICV, data = crossLBC)\ncrossLBC$resid = resid(model)\n\n# standardise variables within one time-point\ncrossLBC$resid_stand = as.vector(scale(crossLBC$resid))\ncrossLBC$diff_stand = as.vector(scale(crossLBC$diff))\ncrossLBC$ratio_stand = as.vector(scale(crossLBC$ratio))\ncrossLBC$TBVstand = as.vector(scale(crossLBC$TBV))\ncrossLBC$ICVstand = as.vector(scale(crossLBC$ICV))\ncrossLBC$CSFstand = as.vector(scale(crossLBC$CSF))\n\n# rename participant labels to match global naming\ncrossLBC$lbc36no = stringr::str_remove(crossLBC$lbc36no, pattern = \"_W4\")\n\n# store as txt file\nfwrite(crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\")], paste0(wd, \"/LBC1936_crossNeuroWave4.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nVisualise cross-secional scores (waves 2 and 5)\n\n\nCode\n# read in LBC neuroimaging data \nLBC2 = fread(paste0(out, \"/LBC1936_crossNeuroWave1.txt\"))\nLBC2$wave = 2\nLBC5 = fread(paste0(out, \"/LBC1936_crossNeuroWave4.txt\"))\nLBC5$wave = 5\nLBC = rbind(LBC2, LBC5)\n\np1 = plot_hist(dat = LBC, var = \"diff\", split_sample_by = \"wave\")+\n          xlab(\"ICV - TBV\\n(raw difference score)\")+\n          theme(legend.position=\"none\")+\n          make_pretty()\n# delete SD stats\np1$layers[[2]]=NULL\n\np2 = plot_hist(dat = LBC, var = \"ratio\", split_sample_by = \"wave\")+\n          xlab(\"TBV / ICV\\n(raw ratio score)\")+\n          theme(legend.position=\"none\")+\n          make_pretty()\n# delete SD stats\np2$layers[[2]]=NULL\n\np3 = plot_hist(dat = LBC, var = \"resid\", split_sample_by = \"wave\")+\n          xlab(\"TBV ~ ICV\\n(raw residual score)\")+\n          make_pretty()\n# delete SD stats\np3$layers[[2]]=NULL\n\n# combine plots\nplot = plot_grid(p1, p2, p3, nrow = 1, labels = c(\"A\", \"B\", \"C\"), label_size = 6, rel_widths = c(1,1,1.5))\n# save plot\n#ggsave(paste0(wd, \"LBCDistributionsCross.jpg\"), plot = plot, width = 12, height = 5, units = \"cm\", dpi = 300)\nplot\n\n\n\n\n\n\n\n\nDistribution of atrophy scores derived from cross-sectional data\n\n\n\n\nDescriptive statistics (cross-sectional measures)\n\nCode\noptions(knitr.kable.NA = \"\")\n\n# calculate descriptives\ndes = descriptives(samples = c(\"LBC2\", \"LBC5\"))\n# cut-offs not needed\ndes = des[-which(des$Statistic == \"Cut off\"),]\n\nknitr::kable(des, col.names = c(\"Stats\",\"LBC (wave 1)\", \"LBC (wave 4)\"))\n\n\n\n\n\n\nStats\nLBC (wave 1)\nLBC (wave 4)\n\n\n\n\n1\nN\n634\n286\n\n\n2\nTBV: Mean (SD)\n1011.23 (97)\n935.48 (92)\n\n\n3\nICV: Mean (SD)\n1396.53 (140)\n1382.16 (150)\n\n\n4\ncor(ICV,TBV)\n0.81\n0.75\n\n\n5\nDifference score\n\n\n\n\n6\nMean (SD)\n385.31 (86.88)\n446.68 (101.51)\n\n\n7\nMedian\n382.43\n444.7\n\n\n8\nRange\n53.15 to 664.4\n100.58 to 882.81\n\n\n9\nVariance\n7500\n10000\n\n\n11\nRatio score\n\n\n\n\n12\nMean (SD)\n0.73 (0.05)\n0.68 (0.05)\n\n\n13\nMedian\n0.73\n0.68\n\n\n14\nRange\n0.6 to 0.95\n0.48 to 0.9\n\n\n15\nVariance\n0.0021\n0.0027\n\n\n17\nResidual score\n\n\n\n\n18\nMean (SD)\n0 (57.37)\n0 (61.08)\n\n\n19\nMedian\n0.46\n3.26\n\n\n20\nRange\n-175.03 to 178.61\n-259.78 to 151.3\n\n\n21\nVariance\n3300\n3700"
  },
  {
    "objectID": "LBC_neuro.html#extract-from-longitudinal-fs-processing-stream-output",
    "href": "LBC_neuro.html#extract-from-longitudinal-fs-processing-stream-output",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "Extract from longitudinal FS processing stream output",
    "text": "Extract from longitudinal FS processing stream output\nLBC1936 provides data that was processed with the longitudinal processing stream.\nData from the longitudinal processing stream should be used when we are interested in inter-individual changes across time (i.e., analyses not involving ICV).\n\n\nCode\n# https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.25572\n# this paper defines total brain volume as:\n# GMV (cortical and subcortical shoudl also include cerebellum) + cerebellum WMV + cerebral WMV\n# TotalGrayVol + Cerebellum.White.Matter + CorticalWhiteMatterVol\nint=\"/Brain_Imaging/LBC1936_FS_long/freesurfer_stats\"\nLBC = read.table(paste0(int,\"/freesurfer_stats_long_w2to5.csv\"), header=T, sep=\",\")\n\n# format each of those variables to long format\n## TotalGrayVol\nTotalGray = LBC[,c(1,grep(\"TotalGrayVol\", names(LBC)))]\nTotalGray = reshape2::melt(TotalGray, id = \"lbc36no\", variable = \"wave\")\nnames(TotalGray)[grep(\"value\", names(TotalGray))] = \"TotalGrayVol\"\nTotalGray$wave = as.numeric(str_remove(TotalGray$wave, pattern = \"TotalGrayVol_w\"))\n\n## Right.Cerebellum.White.Matter\nRCerebellumWM = LBC[,c(1,grep(\"Right.Cerebellum.White.Matter\", names(LBC)))]\nRCerebellumWM = reshape2::melt(RCerebellumWM, id = \"lbc36no\", variable = \"wave\")\nnames(RCerebellumWM)[grep(\"value\", names(RCerebellumWM))] = \"Right.Cerebellum.White.Matter\"\nRCerebellumWM$wave = as.numeric(str_remove(RCerebellumWM$wave, pattern = \"Right.Cerebellum.White.Matter_w\"))\n\n## Left.Cerebellum.White.Matter\nLCerebellumWM = LBC[,c(1,grep(\"Left.Cerebellum.White.Matter\", names(LBC)))]\nLCerebellumWM = reshape2::melt(LCerebellumWM, id = \"lbc36no\", variable = \"wave\")\nnames(LCerebellumWM)[grep(\"value\", names(LCerebellumWM))] = \"Left.Cerebellum.White.Matter\"\nLCerebellumWM$wave = as.numeric(str_remove(LCerebellumWM$wave, pattern = \"Left.Cerebellum.White.Matter_w\"))\n\n## CorticalWhiteMatterVol\n# find column names first\ncols = names(LBC)[c(1,grep(\"CorticalWhiteMatterVol\", names(LBC)))]\ncols = cols[-grep(\"rh\", cols)]\ncols = cols[-grep(\"lh\", cols)]\n# subset data\nCorticalWhite = LBC[,cols]\nCorticalWhite = reshape2::melt(CorticalWhite, id = \"lbc36no\", variable = \"wave\")\nnames(CorticalWhite)[grep(\"value\", names(CorticalWhite))] = \"CorticalWhiteMatterVol\"\nCorticalWhite$wave = as.numeric(str_remove(CorticalWhite$wave, pattern = \"CorticalWhiteMatterVol_w\"))\n\n## IntraCranialVol\nIntraCran = LBC[,c(1,grep(\"IntraCranialVol\", names(LBC)))]\nIntraCran = reshape2::melt(IntraCran, id = \"lbc36no\", variable = \"wave\")\nnames(IntraCran)[grep(\"value\", names(IntraCran))] = \"ICV\"\nIntraCran$wave = as.numeric(str_remove(IntraCran$wave, pattern = \"IntraCranialVol_w\"))\n#### only looking at IntraCranVol to see if ICV is stable across time - which it is\n# going forward, I will not use ICV from the longitudinal scans  because it is not suitable for cross-person comparisons - here we can only look at within-person changes of TBV\n# for that reason I am not including IntraCran in the merge list below\n\n#### merge the different variables\nDatList = list(TotalGray, RCerebellumWM, LCerebellumWM, CorticalWhite) \nLBC_merged = Reduce(function(x,y) merge(x, y, by = c(\"lbc36no\", \"wave\"), all = T), DatList)\n# no need for time points 3 and 4 for our study\nLBC_merged = LBC_merged[-which(LBC_merged$wave == 3),]\nLBC_merged = LBC_merged[-which(LBC_merged$wave == 4),]\n# get rid of all missing values because we can't confidently compute TBV if some parts of the equation re missing\nLBC_merged = na.omit(LBC_merged)\n\n# calculate sum of the regions\nLBC_merged$TBV = rowSums(LBC_merged[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n#length(unique(LBC_merged$lbc36no))\n# 460\n\n###### include CSF\n# for the analyses in aim 3.1 we only need CSF at wave 5\nCSF = LBC[,c(1,grep(\"CSF_w5\", names(LBC)), grep(\"CSF_w2\", names(LBC)))]\nCSF = reshape2::melt(CSF, id = \"lbc36no\", variable = \"wave\")\nnames(CSF)[grep(\"value\", names(CSF))] = \"CSF\"\nCSF$wave = as.numeric(str_remove(CSF$wave, pattern = \"CSF_w\"))\n### also realised later that I would not want to include CSF measures from longitudinal processing as it's not comparable between participants - will only use CSF estimates from cross-setional processing\n\n# convert mm3 estimates to more intuitive cm3 estimates\nLBC_merged$TBV = LBC_merged$TBV/1000\n\n# store as txt file\nfwrite(LBC_merged[,c(\"lbc36no\", \"wave\", \"TBV\")], paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\nFormat longitudinal change measures\n\n\nCode\n# read in LBC data \nneuro = fread(paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), data.table = F)\n# extract longitudinal atrophy from LBC data\n#### Difference score \n# Step 1: change to wide format\ntemp = reshape(neuro, idvar = \"lbc36no\", timevar = \"wave\", direction = \"wide\")\ntemp = temp[,c(\"lbc36no\", \"TBV.2\", \"TBV.5\")]\n\n# Step 2: calculate difference in TBV between wave 2 and wave 5\ntemp$TBVdiff_2to5 = temp$TBV.2 - temp$TBV.5\n\n###### Ratio score\n# Step 2: calculate difference in TBV between wave 2 and wave 5\ntemp$TBVratio_5to2 = temp$TBV.5 / temp$TBV.2\n\n###### Resid score\n# remove missing because results with missing produces weird dimensions\ntemp1 = temp[!is.na(temp$TBV.2),]\ntemp1 = temp1[!is.na(temp1$TBV.5),] \n# Step 2: calculate difference in TBV between wave 2 and wave 5\nmodel = lm(TBV.5 ~ TBV.2, data = temp1)\ntemp1$TBVresid_2to5 = resid(model)\n\n# merge back in with complete temp\ntemp = merge(temp, temp1[,c(\"lbc36no\", \"TBVresid_2to5\")], by = \"lbc36no\", all = T)\n\n# Step 3: merge data back with long data\n# changed my mind about that - keeping long and cross data separate will make it easier to treat them distinctly\n# neuro = merge(neuro, temp[,c(\"lbc36no\", \"TBVdiff_2to5\", \"TBVratio_5to2\", \"TBVresid_2to5\")], by = \"lbc36no\", all = T)\n\n# standardise variables\nneuro = temp\nneuro$TBVdiff_2to5_stand = as.numeric(scale(neuro$TBVdiff_2to5))\nneuro$TBVratio_5to2_stand = as.numeric(scale(neuro$TBVratio_5to2))\nneuro$TBVresid_2to5_stand = as.numeric(scale(neuro$TBVresid_2to5))\n\n# store as txt file\nfwrite(neuro, paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\nDisplay raw change in TBV\nThis is Supplementary Plot 10.\n\n\nCode\nlong = fread(paste0(out, \"/LBC1936_longTBVWaves2and5.txt\"))\n\nlong = reshape2::melt(long, id.vars = \"lbc36no\", measure.vars = c(\"TBV.2\", \"TBV.5\"), variable.name = \"wave\", value.name = \"TBV\")\nlong$wave = as.numeric(str_remove(long$wave, \"TBV.\"))\n\nplot = ggplot()+\n  geom_point(data = long, aes(x = wave, y = TBV, group = lbc36no),color = \"#82A0D8\", size = .5)+\n  geom_line(data = long, aes(x = wave, y = TBV, group = lbc36no), color = \"#8DDFCB\", linewidth = 0.2, alpha = .2) +\n  scale_x_continuous(breaks = c(2,5))+\n  ylab(bquote('TBV in '~mm^3))+\n  xlab(\"Assessment visit\")+\n    theme(legend.position = \"none\")+\n    theme_bw()+\n    theme(text = element_text(size=15),\n          plot.margin=unit(c(1, 1, 1, 1), \"cm\"),\n          axis.text.y = element_text(size =15),\n          axis.text.x = element_text(size =15),\n          panel.border = element_blank())\n\n# get average measures\nmean2 = mean(long$TBV[which(long$wave == 2)],  na.rm=T)\nlabel2 = paste0(\"Mean = \", round(mean2, digits = 2))\n\nmean3 = mean(long$TBV[which(long$wave == 5)],  na.rm=T)\nlabel3 = paste0(\"Mean = \", round(mean3, digits = 2))\n\navg <- data.frame(x = c(2,5),\n                  y = c(mean2,mean3),\n                  label = c(label2, label3))\n\navg$yLabel = min(long$TBV, na.rm=T)\n\nplot=plot + geom_point(data = avg, aes(x=x,y=y), shape = 4, color = \"red\")+\n  geom_line(data = avg, aes(x=x,y=y), color = \"red\")+\n  #geom_text(data = avg, aes(x=x,y=yLabel, label = label), angle = 90, color = \"red\", vjust = 0, hjust = 0)\n  annotate(\"text\", x = 2.1, y = min(long$TBV, na.rm=T)-50, label = paste0(\"Mean = \", round(mean2, digits = 2)), hjust = 0, color = \"red\", angle = 90)+\n  annotate(\"text\", x = 4.9, y = min(long$TBV, na.rm=T)-50, label = paste0(\"Mean = \", round(mean3, digits = 2)), hjust = 0, color = \"red\", angle = 90)\n\n#ggsave(paste0(out,\"phenotypic/LBC_longChange.jpg\"), bg = \"white\",plot = plot, width = 10, height = 10, units = \"cm\", dpi = 200)\nplot\n\n\n\n\n\n\n\nDisplay longitudinal atrophic changes\nFirst define plot_hist function.\n\n\nCode\nplot_hist <- function(dat = dat, var = \"diff_stand\", split_sample_by = NULL){\n  # install packages if they don't already exits\n  packages = c(\"ggplot2\",\"stringr\", \"tidyr\", \"dplyr\")\n  install.packages(setdiff(packages, rownames(installed.packages())))\n  # load packages\n  library(ggplot2)\n  library(stringr)\n  library(tidyr)\n  library(dplyr)\n\n  # make sure input data is data.frame\n  dat = as.data.frame(dat)\n  # rename for simplicity\n  dat$var = dat[,var]\n\n  # calculate summary stats\n    df_stats <-\n        dat %>%\n        summarize(\n          mean = mean(var, na.rm=T),\n          median = median(var, na.rm=T)\n        ) %>%\n        gather(key = Statistic, value = value, mean:median)\n\n    # calculate SD cutoffs\n    insert = c(\"+2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) + 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    insert = c(\"-2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) - 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    # format\n    df_stats$value <- as.numeric(df_stats$value)\n\n    # consider one-sided nature of cut-off\n    # if difference score, we use the upper 2 SD limit\n    # if ratio or residual score, we use the lower 2 SD limit\n    if(var == \"diff\" | var == \"diff_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"-2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }else if(var == \"ratio\" | var == \"resid\" | var == \"ratio_stand\" | var == \"resid_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"+2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }\n\n\n  # PLOT\n  # different output when there is a \"sample\" column\n  if(is.null(split_sample_by)){\n      plot = ggplot(dat, aes(x = var))+\n          geom_histogram(bins = 100, alpha = 0.5, fill = \"#56B4E9\")+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n\n\n  }else if(!is.null(split_sample_by)){\n\n    if(length(which(names(dat) == split_sample_by)) == 0){\n      message(paste0(\"You have indicated that you wanted to group plotted values by \", split_sample_by,\", but the data contains no such column.\")); break\n    }\n\n    # incorporate grouping variable\n    names(dat)[which(names(dat) == split_sample_by)] = \"split_sample_by\"\n    # make sure its a factor\n    dat$split_sample_by = as.factor(dat$split_sample_by)\n\n    colors = c(\"#56B4E9\",\"#009E73\", \"#E69F00\") # \"#79AC78\" #grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]\n    colors = colors[1:length(unique(dat$split_sample_by))]\n\n      plot = ggplot(dat)+\n          geom_histogram(aes(x = var, fill = split_sample_by), bins = 100, alpha = 0.5)+\n          scale_fill_manual(values = colors, name = split_sample_by)+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n  }\n\n    # make second x-axis if we're working with standardised variables\n    if(length(grep(\"_stand\", var)) != 0){\n\n      # calculate mean from original variable\n      varOr = str_remove(var, \"_stand\")\n      mean = mean(dat[,varOr], na.rm=T)\n      sd = sd(dat[,varOr], na.rm=T)\n\n      # add secondary x axis\n      plot = plot+\n         scale_x_continuous(sec.axis = sec_axis(name = \"Raw values\", trans=~.*sd+mean))\n\n    }\n\n  plot = plot+theme(panel.border = element_blank())\n\n  return(plot)\n}\n\n\nThis is Supplementary Table 11.\n\n\nCode\nlong = fread(paste0(out, \"/LBC1936_longTBVWaves2and5.txt\"))\n\n# difference score\np1 = plot_hist(dat = long, var = \"TBVdiff_2to5_stand\")+\n  xlab(\"TBV1 - TBV2\\n(Difference score)\")+\n  geom_histogram(fill = \"#D81B60\")+\n  make_pretty()\np1$layers[[2]] = NULL\n# ratio score\np2 = plot_hist(dat = long, var = \"TBVratio_5to2_stand\")+\n  xlab(\"TBV2 divided by TBV1\\n(Ratio score)\")+\n  geom_histogram(fill = \"#FFC107\")+\n  make_pretty()\np2$layers[[2]] = NULL\n# resid score\np3 = plot_hist(dat = long, var = \"TBVresid_2to5_stand\")+\n  xlab(\"TBV2 ~ TBV1\\n(Residual score)\")+\n  geom_histogram(fill = \"#004D40\")+\n  make_pretty()\np3$layers[[2]] = NULL\n\nplot = plot_grid(p1, p2, p3, nrow=1, labels = c(\"A\",\"B\",\"C\"), label_size = 6, rel_widths = c(1,1,1))\n\n#ggsave(paste0(out, \"phenotypic/LBClong_distribution.png\"), plot = plot, width = 11, height = 5, units = \"cm\", dpi = 600)\nplot"
  },
  {
    "objectID": "LBC_pheno.html",
    "href": "LBC_pheno.html",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "",
    "text": "Data prepared here was used as input into analyses presented here. The file containing all phenotypic variables was named LBC1936_allPheno.txt."
  },
  {
    "objectID": "LBC_pheno.html#load-packages",
    "href": "LBC_pheno.html#load-packages",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Load packages",
    "text": "Load packages\n\n\nCode\nlibrary(data.table)"
  },
  {
    "objectID": "LBC_pheno.html#function-for-longitudinal-plots",
    "href": "LBC_pheno.html#function-for-longitudinal-plots",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Function for longitudinal plots",
    "text": "Function for longitudinal plots\n\n\nCode\n# write function to plot longitudinal  data\nplot_long = function(dat = data, id.var = \"lbc36no\", var = \"matreas\"){\n\n    # make sure dat is data.frame\n    dat = as.data.frame(dat)\n  # select data for the chosen cognitive test\n  dat = dat[,c(which(names(dat) == id.var), grep(var, names(dat)))]\n\n  # transform wide to long format\n  long <- reshape2::melt(dat, id.vars = id.var, value.name = var)\n  names(long)[which(names(long) == \"variable\")] = \"Wave\"\n  names(long)[which(names(long) == var)] = \"var\"\n  names(long)[which(names(long) == id.var)] = \"id.var\"\n  # remove redundant naming from waves\n  long$Wave = as.numeric(sub(\".*_w\", \"\", long$Wave))\n\n  plot = ggplot(data = long, aes(x = Wave, y = var, group = id.var))+\n  geom_point(color = \"#82A0D8\", size = .5)+\n  geom_line(aes(group=as.factor(id.var)),method=\"lm\", se=F, color = \"#8DDFCB\", size = 0.2, alpha = .2, stat =  \"smooth\") +\n    theme(legend.position = \"none\")+\n    theme_bw()+\n    theme(text = element_text(size=20),\n          plot.margin=unit(c(1, 1, 1, 1), \"cm\"),\n          axis.text.y = element_text(size =20),\n          axis.text.x = element_text(size =20),\n          panel.border = element_blank())\n\n  return(plot)\n}"
  },
  {
    "objectID": "LBC_pheno.html#cognitive-tests-factor-scores",
    "href": "LBC_pheno.html#cognitive-tests-factor-scores",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Cognitive tests (factor scores)",
    "text": "Cognitive tests (factor scores)\nThis script was kindly provided by Joanna Moodie who had modelled a general factor of cognitive ability in the LBC1936 for one of her projects.\n\n\nCode\n#######################################\n## Read in and format data\n######################################\n\n# read data\ndata = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AF_07NOV2023.sav\"), to.data.frame=T)\n\n##########################################################\n### Format data\n##########################################################\n\ndata=data[,c(\"lbc36no\",\n              \"matreas_w1\",\"matreas_w2\",\"matreas_w3\",\"matreas_w4\",\"matreas_w5\",\n              \"blkdes_w1\",\"blkdes_w2\",\"blkdes_w3\",\"blkdes_w4\",\"blkdes_w5\",\n              \"spantot_w1\",\"spantot_w2\",\"spantot_w3\",\"spantot_w4\",\"spantot_w5\",\n              \"vpatotal_w1\",\"vpatotal_w2\",\"vpatotal_w3\",\"vpatotal_w4\",\"vpa_total_w5\",\n              \"lmtotal_w1\",\"lmtotal_w2\",\"lmtotal_w3\",\"lmtotal_w4\",\"lmtotal_w5\",\n              \"digback_w1\",\"digback_w2\",\"digback_w3\",\"digback_w4\",\"digback_w5\",\n              \"nart_w1\",\"nart_w2\",\"nart_total_w3\",\"nart_total_w4\",\"nart_total_w5\",\n              \"wtar_w1\",\"wtar_w2\",\"wtar_total_w3\",\"wtar_total_w4\",\"wtar_total_w5\",\n              \"vftot_w1\",\"vftot_w2\",\"vftot_w3\" ,\"vftot_w4\",\"vftot_w5\",\n              \"digsym_w1\",\"digsym_w2\",\"digsym_w3\",\"digsym_w4\",\"digsym_w5\",\n              \"symsear_w1\",\"symsear_w2\",\"symsear_w3\",\"symsear_w4\",\"symsear_w5\",\n              \"crtmean_w1\",\"crtmean_w2\",\"crtmean_w3\",\"crtmean_w4\",\"crtmean_w5\",\n              \"ittotal_w1\",\"ittotal_w2\",\"ittotal_w3\",\"ittotal_w4\",\"ittotal_w5\")]\n\n\n# recode missing values\ndata[data == -999] <- NA\ndata[data == -777] <- NA\ndata[data == 999] <- NA\ndata[data == 888] <- NA\ndata[data == -888] <- NA\n\n# assign correct variable classes\n# some numeric columns are coded as factors when some should be integers and some should be numeric\n# the code below transforms variables first into characters, then numeric because some of the values otherwise get corrupted\nIntNames = names(data)[-grep(\"crtmean\", names(data))]\nIntNames = IntNames[-which(IntNames == \"lbc36no\")]\ndata[IntNames] = lapply(data[IntNames], as.character)\ndata[IntNames] = lapply(data[IntNames], as.integer)\n\nNumNames = names(data)[grep(\"crtmean\", names(data))]\ndata[NumNames] = lapply(data[NumNames], as.character)\ndata[NumNames] = lapply(data[NumNames], as.numeric)\n\n# the symbol search variable has some impossible values (< 0).. these should be removed\nfor (i in names(data)[grep(\"symsear\", names(data))]) {\n  data[which(data[,i] < 0),i] <- NA\n}\n\n#rescaled some of the cognitive test variables so that variances are within a similar\n#range see http://www.statmodel.com/discussion/messages/11/1615.html?1335376547\n\ndset_mod <- mutate(data,\n                   blkdes_w1 = blkdes_w1/2,\n                   blkdes_w2 = blkdes_w2/2,\n                   blkdes_w3 = blkdes_w3/2,\n                   blkdes_w4 = blkdes_w4/2,\n                   blkdes_w5 = blkdes_w5/2,\n                   vftot_w1 = vftot_w1/2,\n                   vftot_w2 = vftot_w2/2,\n                   vftot_w3 = vftot_w3/2,\n                   vftot_w4 = vftot_w4/2,\n                   vftot_w5 = vftot_w5/2,\n                   lmtotal_w1 = lmtotal_w1/3,\n                   lmtotal_w2 = lmtotal_w2/3,\n                   lmtotal_w3 = lmtotal_w3/3,\n                   lmtotal_w4 = lmtotal_w4/3,\n                   lmtotal_w5 = lmtotal_w5/3,\n                   digback_w1 = 3*digback_w1,\n                   digback_w2 = 3*digback_w2,\n                   digback_w3 = 3*digback_w3,\n                   digback_w4 = 3*digback_w4,\n                   digback_w5 = 3*digback_w5,\n                   digsym_w1 = digsym_w1/2,\n                   digsym_w2 = digsym_w2/2,\n                   digsym_w3 = digsym_w3/2,\n                   digsym_w4 = digsym_w4/2,\n                   digsym_w5 = digsym_w5/2,\n                   ittotal_w1 = ittotal_w1/2,\n                   ittotal_w2 = ittotal_w2/2,\n                   ittotal_w3 = ittotal_w3/2,\n                   ittotal_w4 = ittotal_w4/2,\n                   ittotal_w5 = ittotal_w5/2,\n                   crtmean_w1 = -50 * crtmean_w1,\n                   crtmean_w2 = -50 * crtmean_w2,\n                   crtmean_w3 = -50 * crtmean_w3,\n                   crtmean_w4 = -50 * crtmean_w4,\n                   crtmean_w5 = -50 * crtmean_w5)\n\n#==================================================================\n# LBC1936 factor of curves model\n#==================================================================\nmodel <- '\n# test growth curves\nImatreas =~ 1*matreas_w1 + 1*matreas_w2 + 1*matreas_w3 + 1*matreas_w4 + 1*matreas_w5\nSmatreas =~ 0*matreas_w1 + 2.98*matreas_w2 + 6.75*matreas_w3 + 9.82*matreas_w4 + 12.54*matreas_w5\n\nIblkdes =~ 1*blkdes_w1 + 1*blkdes_w2 + 1*blkdes_w3 + 1*blkdes_w4 + 1*blkdes_w5\nSblkdes=~ 0*blkdes_w1 + 2.98*blkdes_w2 + 6.75*blkdes_w3 + 9.82*blkdes_w4 + 12.54*blkdes_w5\n\nIspantot =~ 1*spantot_w1 + 1*spantot_w2 + 1*spantot_w3 + 1*spantot_w4 + 1*spantot_w5\nSspantot=~ 0*spantot_w1 + 2.98*spantot_w2 + 6.75*spantot_w3 + 9.82*spantot_w4 + 12.54*spantot_w5\n\nInart =~ 1*nart_w1 + 1*nart_w2 + 1*nart_total_w3 + 1*nart_total_w4 + 1*nart_total_w5\nSnart =~ 0*nart_w1 + 2.98*nart_w2 + 6.75*nart_total_w3 + 9.82*nart_total_w4 + 12.54*nart_total_w5\n\nIwtar =~ 1*wtar_w1 + 1*wtar_w2 + 1*wtar_total_w3 + 1*wtar_total_w4 + 1*wtar_total_w5\nSwtar =~ 0*wtar_w1 + 2.98*wtar_w2 + 6.75*wtar_total_w3 + 9.82*wtar_total_w4 + 12.54*wtar_total_w5\n\nIvftot =~ 1*vftot_w1 + 1*vftot_w2 + 1*vftot_w3 + 1*vftot_w4 + 1*vftot_w5\nSvftot =~ 0*vftot_w1 + 2.98*vftot_w2 + 6.75*vftot_w3 + 9.82*vftot_w4 + 12.54*vftot_w5\n\nIvpatotal =~ 1*vpatotal_w1 + 1*vpatotal_w2 + 1*vpatotal_w3 + 1*vpatotal_w4 + 1*vpa_total_w5\nSvpatotal =~ 0*vpatotal_w1 + 2.98*vpatotal_w2 + 6.75*vpatotal_w3 + 9.82*vpatotal_w4 + 12.54*vpa_total_w5\n\nIlmtotal =~ 1*lmtotal_w1 + 1*lmtotal_w2 + 1*lmtotal_w3 + 1*lmtotal_w4 + 1*lmtotal_w5\nSlmtotal =~ 0*lmtotal_w1 + 2.98*lmtotal_w2 + 6.75*lmtotal_w3 + 9.82*lmtotal_w4 + 12.54*lmtotal_w5\n\nIdigback =~ 1*digback_w1 + 1*digback_w2 + 1*digback_w3 + 1*digback_w4 + 1*digback_w5\nSdigback =~ 0*digback_w1 + 2.98*digback_w2 + 6.75*digback_w3 + 9.82*digback_w4 + 12.54*digback_w5\n\nIsymsear =~ 1*symsear_w1 + 1*symsear_w2 + 1*symsear_w3 + 1*symsear_w4 + 1*symsear_w5\nSsymsear =~ 0*symsear_w1 + 2.98*symsear_w2 + 6.75*symsear_w3 + 9.82*symsear_w4 + 12.54*symsear_w5\n\nIdigsym =~ 1*digsym_w1 + 1*digsym_w2 + 1*digsym_w3 + 1*digsym_w4 + 1*digsym_w5\nSdigsym =~ 0*digsym_w1 + 2.98*digsym_w2 + 6.75*digsym_w3 + 9.82*digsym_w4 + 12.54*digsym_w5\n\nIittotal =~ 1*ittotal_w1 + 1*ittotal_w2 + 1*ittotal_w3 + 1*ittotal_w4 + 1*ittotal_w5\nSittotal =~ 0*ittotal_w1 + 2.98*ittotal_w2 + 6.75*ittotal_w3 + 9.82*ittotal_w4 + 12.54*ittotal_w5\n\nIcrtmean =~ 1*crtmean_w1 + 1*crtmean_w2 + 1*crtmean_w3 + 1*crtmean_w4 + 1*crtmean_w5\nScrtmean =~ 0*crtmean_w1 + 2.98*crtmean_w2 + 6.75*crtmean_w3 + 9.82*crtmean_w4 + 12.54*crtmean_w5\n\n# latent g intercept and slope \nIg =~  Iblkdes + Imatreas  + Ispantot + Ivftot + Ivpatotal + Ilmtotal +\n  Idigback + Isymsear + Idigsym + Icrtmean + Iittotal + Inart + Iwtar \n# \nSg =~ Sblkdes + Smatreas + Sspantot + Svftot + Svpatotal + Slmtotal +\n  Sdigback + Ssymsear + Sdigsym + Scrtmean + Sittotal + Snart + Swtar \n\n#indicator as scaling reference: loading=1, int=0\nIblkdes ~ 0*1\nSblkdes ~ 0*1 \n\n# within-wave covariances between nart and wtar\nnart_w1 ~~ wtar_w1\nnart_w2 ~~ wtar_w2\nnart_total_w3 ~~ wtar_total_w3\nnart_total_w4 ~~ wtar_total_w4\nnart_total_w5 ~~ wtar_total_w5\n\n# within-test intercept-slope covariances\nImatreas ~~ Smatreas\nIblkdes ~~ Sblkdes\n#Ispantot ~~Sspantot\nInart ~~ Snart\nIwtar ~~ Swtar\nIvftot ~~ Svftot\nIvpatotal ~~ Svpatotal\nIlmtotal ~~ Slmtotal\nIdigback ~~ Sdigback\nIsymsear ~~ Ssymsear\nIdigsym ~~ Sdigsym\nIittotal ~~ Sittotal\nIcrtmean ~~ Scrtmean\n\n\n# within-domain intercept-intercept and slope-slope covariances\nIblkdes ~~ Imatreas # Visuospatial domain\nIblkdes ~~ Ispantot\nImatreas ~~ Ispantot\nSblkdes ~~ Smatreas \n#Sblkdes ~~ Sspantot\n#Smatreas ~~ Sspantot\n\nInart ~~ Ivftot #Crystalized domain\nIwtar ~~ Ivftot\nIwtar ~~ Inart\nSnart ~~ Svftot\nSwtar ~~ Svftot\nSwtar ~~ Snart\n\nIlmtotal ~~ Ivpatotal # Verbal memory domain\nIlmtotal ~~ Idigback\nIvpatotal ~~ Idigback\nSlmtotal ~~ Svpatotal\nSlmtotal ~~ Sdigback\nSvpatotal ~~ Sdigback\n\nIittotal ~~ Idigsym #Processing speed domain\nIittotal ~~ Isymsear\nIittotal ~~ Icrtmean\nIdigsym ~~ Isymsear\nIdigsym ~~ Icrtmean\nIsymsear ~~ Icrtmean\nSittotal ~~ Sdigsym \nSittotal ~~ Ssymsear\nSittotal ~~ Scrtmean\nSdigsym ~~ Ssymsear\nSdigsym ~~ Scrtmean\nSsymsear ~~ Scrtmean\n\n#fixed negative residual variance to 0 \nSspantot ~~ 0*Sspantot\n'\n\n# fit model in lavaan \nfit <- growth(model = model, dset_mod,  missing = \"ml.x\")\n#save=standardizedsolution(fit, output=\"data.frame\")\nsummary(fit, fit.measures = T, standardized = T)\n\n#==================================================================\n# extract LBC1936 g intercepts and slopes\n#==================================================================\n\ncogscores <- dset_mod %>% select(contains(\"matreas\") | contains(\"blkdes\") | contains(\"spantot\") | contains(\"nart\") | contains(\"vftot\") | contains(\"vpa\") | contains(\"lmtotal\") | contains(\"digback\") | contains(\"symsear\") | contains(\"digsym\") | contains(\"ittotal\") | contains(\"crtmean\") )\nwavesindex <- rep(c(1,2,3,4,5), 12) # index the waves \n\n# find people that were only tested at one wave only to exclude them from prediction\n# and find people that were not tested at wave 1\nw1 <- matrix(NA, nrow(cogscores), 1)\nw2 <- matrix(NA, nrow(cogscores), 1)\nw3 <- matrix(NA, nrow(cogscores), 1)\nw4 <- matrix(NA, nrow(cogscores), 1)\nw5 <- matrix(NA, nrow(cogscores), 1)\nfor (i in 1:nrow(cogscores)) {\n  if (!all(is.na(cogscores[i, which(wavesindex == 1)]))) {\n    w1[i,1] <- 1 }\n  if (!all(is.na(cogscores[i, which(wavesindex == 2)]))) {\n    w2[i,1] <- 1 }\n  if (!all(is.na(cogscores[i, which(wavesindex == 3)]))) {\n    w3[i,1] <- 1 }\n  if (!all(is.na(cogscores[i, which(wavesindex == 4)]))) {\n    w4[i,1] <- 1 }\n  if (!all(is.na(cogscores[i, which(wavesindex == 5)]))) {\n    w5[i,1] <- 1 }\n}\n\nwavesample <- cbind(w1, w2, w3, w4, w5)\nrowSums(wavesample, na.rm = T)\nonewave <- which(rowSums(wavesample, na.rm = T) < 2) # these people were only tested at one wave\n\n\n# delete participant with all missing data\n#dset_mod1 = dset_mod[-which(rowSums(is.na(dset_mod)) >= (ncol(dset_mod)-1)),]\n\n# extract factor scores\nfactorScores <- data.frame(lavPredict(fit, dset_mod, \n                 type =\"lv\", \n                 method = \"regression\", \n                 label = TRUE))\n\n# set slopes for participants who were only tested at one wave to NA\nfactorScores$Sg[onewave] <- NA \n# set intercepts for participants who were not tested at w1 to NA \nfactorScores$Ig[which(is.na(w1))] <- NA \n# add participant ID back to factor scores\nfactorScores <- data.frame(lbc36no = dset_mod$lbc36no, Ig = factorScores$Ig, Sg = factorScores$Sg)\n\n# report Ns\ncbind(c(\"N intercepts included\", \"N intercepts excluded\", \"N slopes included\", \"N slopes excluded\"), as.numeric(c(length(which(is.na(factorScores$Ig) == F)), length(which(is.na(factorScores$Ig) == T)), length(which(is.na(factorScores$Sg) == F)), length(which(is.na(factorScores$Sg) == T))))) \n\n# write.table to dat\nwrite.table(factorScores, file = paste0(wd, \"/LBC1936_Cog_FactorScores.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n\n\nPlot cognitive scores\n\n\nCode\n# consistent variable names\nnames(data) <- gsub(\"vpa_tot\", \"vpatot\", colnames(data)) \n\n# plot the different cognitive tests\np_matreas <- plot_long(dat = data, id.var = \"lbc36no\", var = \"matreas\")+\n  xlab(\"Wave\")+\n  ylab(\"Matrix reasoning\\n(# correct)\\n'matreas'\")\n\np_blkdes <- plot_long(dat = data, id.var = \"lbc36no\", var = \"blkdes\")+\n  xlab(\"Wave\")+\n  ylab(\"Block design\\n(# correct)\\n'blkdes'\")\n\np_spantot <- plot_long(dat = data, id.var = \"lbc36no\", var = \"spantot\")+\n  xlab(\"Wave\")+\n  ylab(\"Spatial span\\n(# correct)\\n'spantot'\")\n\np_vpatotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"vpatotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Verbal paired associations\\n(# correct)\\n'vpatotal'\")\n\np__lmtotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"lmtotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Logical memory\\n(# details recalled)\\n'lmtotal'\")\n\np_digback <- plot_long(dat = data, id.var = \"lbc36no\", var = \"digback\")+\n  xlab(\"Wave\")+\n  ylab(\"Digit span backwards\\n(max length)\\n'digback'\")\n\np_nart <- plot_long(dat = data, id.var = \"lbc36no\", var = \"nart\")+\n  xlab(\"Wave\")+\n  ylab(\"National Adult\\nReading Test\\n(# correct) 'nart'\")\n\np_wtar <- plot_long(dat = data, id.var = \"lbc36no\", var = \"wtar\")+\n  xlab(\"Wave\")+\n  ylab(\"Wechsler Test of\\nAdult Reading\\n(# correct) 'wtar'\")\n\np_vftot <- plot_long(dat = data, id.var = \"lbc36no\", var = \"vftot\")+\n  xlab(\"Wave\")+\n  ylab(\"Verbal fluency\\n(# correct)\\n'vftot'\")\n\np_digsym<- plot_long(dat = data, id.var = \"lbc36no\", var = \"digsym\")+\n  xlab(\"Wave\")+\n  ylab(\"Digit-symbol substitution\\n(# matched pairs)\\n'digsym'\")\n\np_symsear <- plot_long(dat = data, id.var = \"lbc36no\", var = \"symsear\")+\n  xlab(\"Wave\")+\n  ylab(\"Symbol Search\\n(# correct)\\n'symsear'\")\n\np_crtmean <- plot_long(dat = data, id.var = \"lbc36no\", var = \"crtmean\")+\n  xlab(\"Wave\")+\n  ylab(\"Four-choice reaction\\ntime (ms)\\n'crtmean'\")\n\np_ittotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"ittotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Inspection time\\n(# correct)\\n'ittotal'\")\n\n# arrange all plots\nplots <- ls(pattern=\"p_\")\nplot_list <- list()\n\nfor(i in plots){\n  plot_list[[i]] <- get(i)\n}\n\n# arrange all plots\nggarrange(plotlist = plot_list, ncol = 3, nrow = 5)"
  },
  {
    "objectID": "LBC_pheno.html#clinically-ascertained-all-cause-dementia",
    "href": "LBC_pheno.html#clinically-ascertained-all-cause-dementia",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Clinically-ascertained all-cause dementia",
    "text": "Clinically-ascertained all-cause dementia\nThis variable is available through the LBC1936 and was derived as described here.\n\n\nCode\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_EarlyAccessDementiaAscertainment_AF_07DEC2023.sav\"), to.data.frame=T)\n\n# remove redundant spacing \nfor(i in names(file)){\n  file[,i] = stringr::str_remove_all(file[,i], pattern = \" \")\n}\n\n# remove NA coding\nfile[file == -999] <- NA\nfile[file == -888] <- NA\n\n# no participants with all missing data\n# which(is.na(file$dement_w1))\n\ntable(file$dementia_code)\n#  Dementia NoDementia \n#       118        747 \n\nfile$dementia_code[which(file$dementia_code == \"Dementia\")] <- 1\nfile$dementia_code[which(file$dementia_code == \"NoDementia\")] <- 0\n\n# binary variable as factor\nfile$dementia_code = as.factor(file$dementia_code)\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\", \"dementia_code\")], file = paste0(wd, \"/LBC1936_dementia.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#apoe-status",
    "href": "LBC_pheno.html#apoe-status",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "APOE status",
    "text": "APOE status\n\n\nCode\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AF_07NOV2023.sav\"), to.data.frame=T)\n\ntable(file[,grep(\"APOE\", names(file))])\n#APOEgenotype\n#APOEe4               ?     e2/e2 e2/e3 e2/e4 e3/e3 e3/e4 e4/e4\n#No e4 allele     0     0     5   120     0   597     0     0\n#e4 allele        0     0     0     0    23     0   262    21\n\n\ntable(file$APOEe4)\n#No e4 allele    e4 allele \n#722          306\n\n# recode\nfile$APOEe4 = as.numeric(file$APOEe4)-1\n\n# make factor\nfile$APOEe4 = as.factor(file$APOEe4)\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\", \"APOEe4\")], file = paste0(wd, \"/LBC1936_APOEe4.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#frailty",
    "href": "LBC_pheno.html#frailty",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Frailty",
    "text": "Frailty\n\n\nCode\n# names(file)[grep(\"Frailty\", names(file))]\nfrailty = file[,c(1,grep(\"Frailty\", names(file)))]\n\n# model slopes and intercepts\nfrailModel <- '\n          i =~ 1*FrailtyIndex_W1 + 1*FrailtyIndex_W2 + 1*FrailtyIndex_W3 + 1*FrailtyIndex_W4 + 1*FrailtyIndex_W5\n          s =~ 0*FrailtyIndex_W1 + 2.98*FrailtyIndex_W2 + 6.75*FrailtyIndex_W3 + 9.82*FrailtyIndex_W4 + 12.54*FrailtyIndex_W5\n          '\n\nfit = growth(frailModel, frailty, missing = \"ml.x\")\nsummary(fit, standardized = T)\n\n# estimate individual-level values for slope and intercept\nfrailtyPred = as.data.frame(lavPredict(fit,\n                 type =\"lv\", \n                 method = \"regression\", \n                 label = TRUE))\n\n# merge with participant ID\nfrailtyPred = cbind(frailty$lbc36no,frailtyPred)\nnames(frailtyPred) = c(\"lbc36no\", \"iFrail\",\"sFrail\")\n\n# save\nwrite.table(frailtyPred, file = paste0(wd, \"/LBC1936_Frailty.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n\n\nPlot frailty\n\n\nCode\n##### visualise \nfrailty = file[,c(1,grep(\"Frailty\", names(file)))]\n# unify naming (otherwise plotting function won't work)\nnames(frailty) = str_replace(names(frailty), pattern= \"_W\", replacement = \"_w\")\n\n# find people who were tested at one wave only to exclude them from slope prediction\nonewave = which(rowSums(is.na(frailty[,grep(\"_W\", names(frailty))])) == 4)\n\n# find people who were not tested at wave 1 to exclude from intercept\nNotWave1 = which(is.na(frailty$FrailtyIndex_W1))\n\n# set slopes for participants who were only tested at one wave to NA\nfrailtyPred$s[onewave] <- NA \n# set intercepts for participants who were not tested at w1 to NA \nfrailtyPred$i[NotWave1] <- NA \n\n##### inspect trajectory\n\np_frailty <- plot_long(dat = frailty, id.var = \"lbc36no\", var = \"FrailtyIndex\")+\n  xlab(\"Wave\")+\n  ylab(\"Frailty index\")\n\np_frailty"
  },
  {
    "objectID": "LBC_pheno.html#diabetes",
    "href": "LBC_pheno.html#diabetes",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Diabetes",
    "text": "Diabetes\n\n\nCode\n# no participants with all missing data\n# which(is.na(file$diab_w1))\n\n# first of all, all No\nfile$diab_life = 0\nfile$diab_life[which(file$diab_w1 == \"Yes\" | \n                         file$diab_w2 == \"Yes\" |\n                         file$diab_w3 == \"Yes\" | \n                         file$diab_w4 == \"Yes\" | \n                         file$diab_w5 == \"Yes\")] = 1\n\n# remove those with all missing\nallMissing = which(rowSums(!is.na(file[,grep(\"diab_w\", names(file))])) == 0) \nif(length(allMissing) != 0){file$diab_life[allMissing] = NA}\n\n\ntable(file$diab_life)\n# No Yes \n# 942 149 \n\nfile$diab_life = as.factor(file$diab_life)\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\", \"diab_life\")], file = paste0(wd, \"/LBC1936_diabetes.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#hypertension",
    "href": "LBC_pheno.html#hypertension",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Hypertension",
    "text": "Hypertension\n\n\nCode\n# no participants with all missing data\n# which(is.na(file$hibp_w1))\n\n# first of all, all No\nfile$hypertension_life = 0\nfile$hypertension_life[which(file$hibp_w1 == \"Yes\" | \n                       file$hibp_w2 == \"Yes\" |\n                       file$hibp_w3 == \"Yes\" | \n                       file$hibp_w4 == \"Yes\" | \n                       file$hibp_w5 == \"Yes\")] = 1\n\n# remove those with all missing\nallMissing = which(rowSums(!is.na(file[,grep(\"hibp_w\", names(file))])) == 0) \nif(length(allMissing) != 0){file$hypertension_life[allMissing] = NA}\n\nfile$hypertension_life = as.factor(file$hypertension_life)\n\ntable(file$hypertension_life)\n# No Yes \n# 448 643 \n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\", \"hypertension_life\")], file = paste0(wd, \"/LBC1936_hypertension.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#smoking-packyears",
    "href": "LBC_pheno.html#smoking-packyears",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Smoking (packyears)",
    "text": "Smoking (packyears)\nNumber of cigarettes per year x years of smoking / 20 (pack size)\n\n\nCode\n# keep variables of interest\nsmok = file[,c(1,grep(\"smo\", names(file)))]\n\n# wave 1 has 465 more non-missing values than wave 2 \n# all.equal(smok$smokagestop_w1, smok$smokagestop_w2)\n\n# keep only first wave\nsmok = smok[,c(1,grep(\"_w1\", names(smok)))]\n\n# identify participants who have starting year but not end year as they must still be smokers\nsmok$smokagestop = ifelse(smok$smokcat_w1 == \"current smoker\", 70, smok$smokagestop_w1)\n\n# extract years of smoking (so far, regardless if they stopped smoking or not)\nsmok$yearsSmok = smok$smokagestop - smok$smokagestart_w1\n\n# two entries are smoknumcigs == 0 which would null the equation\n# one of those participants have indicates to be a current smoker and the other one to hae never smoked (but still gave starting age)\n# as I don't know how that came about, I have deleted these two data points\nsmok[which(smok$smoknumcigs_w1 == 0),] = NA\n\n# calculate packyears\nsmok$packyears = (smok$smoknumcigs_w1 * 365 * smok$yearsSmok)/20\n\n# null the never smokers\nsmok$packyears[which(smok$smokcat_w1 == \"never smoked\")] = 0\n\nhist(smok$packyears)\n\n# write.table to dat\nwrite.table(smok[,c(\"lbc36no\", \"packyears\")], file = paste0(wd, \"/LBC1936_packYears.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#body-mass-index-bmi",
    "href": "LBC_pheno.html#body-mass-index-bmi",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Body mass index (BMI)",
    "text": "Body mass index (BMI)\n\n\nCode\n# names(file)[grep(\"bmi\", names(file))]\nbmi = file[,c(1,grep(\"bmi\", names(file)))]\n\n# recode missing\nbmi[bmi == -999] <- NA\n\n# model slopes and intercepts\nBMImodel <- '\n          i =~ 1*bmi_w1 + 1*bmi_w2 + 1*bmi_w3 + 1*bmi_w4 + 1*bmi_w5\n          s =~ 0*bmi_w1 + 2.98*bmi_w2 + 6.75*bmi_w3 + 9.82*bmi_w4 + 12.54*bmi_w5\n          '\n\nfit = growth(BMImodel, bmi, missing = \"ml.x\")\nsummary(fit, standardized = T)\n\n# estimate individual-level values for slope and intercept\nbmiPred = as.data.frame(lavPredict(fit,\n                 type =\"lv\", \n                 method = \"regression\", \n                 label = TRUE))\n\n# merge with participant ID\nbmiPred = cbind(bmi$lbc36no, bmiPred)\nnames(bmiPred) = c(\"lbc36no\", \"iBMI\", \"sBMI\")\n\n# write.table to dat\nwrite.table(bmiPred[,c(\"lbc36no\", \"iBMI\", \"sBMI\")], file = paste0(wd, \"/LBC1936_bmi.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n\n\nPlot BMI\n\n\nCode\n## visualise\nbmi = file[,c(1,grep(\"bmi\", names(file)))]\n\n# find people who were tested at one wave only to exclude them from slope prediction\nonewave = which(rowSums(is.na(bmi[,grep(\"_W\", names(bmi))])) == 4)\n\n# find people who were not tested at wave 1 to exclude from intercept\nNotWave1 = which(is.na(bmi$bmiIndex_W1))\n\n# set slopes for participants who were only tested at one wave to NA\nbmiPred$s[onewave] <- NA \n# set intercepts for participants who were not tested at w1 to NA \nbmiPred$i[NotWave1] <- NA \n\nsummary(bmiPred)\n\n# write.table to dat\nwrite.table(bmiPred, file = paste0(wd, \"/LBC1936_bmiFactorScores.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n##### inspect trajectory\nplot_long(dat = bmi, id.var = \"lbc36no\", var = \"bmi\")+\n  xlab(\"Wave\")+\n  ylab(\"bmi index\")"
  },
  {
    "objectID": "LBC_pheno.html#brain-age",
    "href": "LBC_pheno.html#brain-age",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Brain age",
    "text": "Brain age\n\n\nCode\n# read file\nfile = as.data.frame(foreign::read.spss(paste0(target, \"/BrainAgeVia2p1_AF_07DEC2023.sav\"), to.file.frame=T))\n\n# remove ID info when waves are indicated in \"JC_BrainAge_ID\"\nfile$JC_BrainAge_ID = as.numeric(gsub(\".*_w\", \"\", file$JC_BrainAge_ID))\n\n# rename columns more intuitively\nnames(file)[which(names(file) == \"JC_BrainAge_ID\")] = \"wave\"\nnames(file)[which(names(file) == \"JCBA_brain_age_W2\")] = \"brainAgeEst\"\n\n# read in age info \nageInfo = as.data.frame(foreign::read.spss(\"LBC1936_BrainAtrophy_AF_07NOV2023.sav\"))\n# strange name formatting\nageInfo$lbc36no =gsub(\" \", \"\", ageInfo$lbc36no)\n\n# merge info \nfile = merge(file, ageInfo[, c(\"lbc36no\", \"agedays_w2\")], by = \"lbc36no\")\n\n# get age in days in years to match estimated brain age\nfile$agedays_w2 = file$agedays_w2/365\n\n# get brain age gap which is the difference between brain age and chronological age\n# positive value should mean the particpant has a healthier looking brain tthan expected given their age \n# negative value should mean the participant has an unhealthier looking brain than expected\nfile$brainAge = file$agedays_w2 - file$brainAgeEst\n\n# summary stats\nsummary(file$brainAge)\nhist(file$brainAge)\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\",\"brainAge\")], file = paste0(wd, \"/LBC1936_brainAge.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#stroke",
    "href": "LBC_pheno.html#stroke",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Stroke",
    "text": "Stroke\n\n\nCode\n# set path to where data was saved \ntarget=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\n\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AF_07NOV2023.sav\"), to.data.frame=T)\n\n# names(file)[grep(\"stroke\", names(file))]\n\n# first of all, all No\nfile$strokemask_life = 0\n# identify people who had no scan\nfile$strokemask_life[which(is.na(file$stroke_mask_w2) & \n                             is.na(file$stroke_mask_w3) &\n                             is.na(file$stroke_mask_w4) &\n                             is.na(file$stroke_mask_w5))] = NA\n\nfile$strokemask_life[which(file$stroke_mask_w2 == \"Yes Stroke Mask - Had scan\" | \n                               file$stroke_mask_w3 == \"Yes Stroke Mask - Had scan\" |\n                               file$stroke_mask_w4 == \"Yes Stroke Mask - Had scan\" | \n                               file$stroke_mask_w5 == \"Yes Stroke Mask - Had scan\")] = 1\n\n\nfile$strokemask_life <- as.factor(file$strokemask_life)\n\ntable(file$strokemask_life)\n# No Yes \n# 544 156 \n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\",\"strokemask_life\")], file = paste0(wd, \"/LBC1936_strokemask.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#visual-rating-scales",
    "href": "LBC_pheno.html#visual-rating-scales",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Visual rating scales",
    "text": "Visual rating scales\nNote we did not have access to rating scales at wave 5, only wave 3.\n\n\nCode\n# set path to where data was saved \ntarget=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\n\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AtrophyRating_AF_03MAY2024.sav\"), to.data.frame=T)\n\n# strange name formatting\nfile$lbc36no = gsub(\" \", \"\", file$lbc36no)\n\n# atrophy deep and superficial are in great agreement (only 80 participants where they are not identical)\nsum(file$atrophy_deep_w3 == file$atrophy_superficial_w3, na.rm=T)\n\n# recode so the scales are numeric\nfile$atrophy_deep_recoded = NA\nfile$atrophy_deep_recoded[grepl(\"(<25th)\", file$atrophy_deep_w3)] = 1\nfile$atrophy_deep_recoded[grepl(\"(25-50th)\", file$atrophy_deep_w3)] = 2\nfile$atrophy_deep_recoded[grepl(\"(50-75th)\", file$atrophy_deep_w3)] = 3\nfile$atrophy_deep_recoded[grepl(\"(75-95th)\", file$atrophy_deep_w3)] = 4\nfile$atrophy_deep_recoded[grepl(\"(>95th)\", file$atrophy_deep_w3)] = 5\nfile$atrophy_deep_recoded[grepl(\"(>>5)\", file$atrophy_deep_w3)] = 6\n\n# same for atrophy superficial\nfile$atrophy_superficial_recoded = NA\nfile$atrophy_superficial_recoded[grepl(\"(<25th)\", file$atrophy_superficial_w3)] = 1\nfile$atrophy_superficial_recoded[grepl(\"(25-50th)\", file$atrophy_superficial_w3)] = 2\nfile$atrophy_superficial_recoded[grepl(\"(50-75th)\", file$atrophy_superficial_w3)] = 3\nfile$atrophy_superficial_recoded[grepl(\"(75-95th)\", file$atrophy_superficial_w3)] = 4\nfile$atrophy_superficial_recoded[grepl(\"(>95th)\", file$atrophy_superficial_w3)] = 5\nfile$atrophy_superficial_recoded[grepl(\"(>>5)\", file$atrophy_superficial_w3)] = 6\n\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\",\"atrophy_deep_recoded\",\"atrophy_superficial_recoded\")], file = paste0(target, \"/LBC1936_atrophyScales.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#age",
    "href": "LBC_pheno.html#age",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Age",
    "text": "Age\n\n\nCode\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AF_07NOV2023.sav\"), to.data.frame=T)\n\n# strange name formatting\nfile$lbc36no = gsub(\" \", \"\", file$lbc36no)\n\n# keep age variable for first neuroimaging visit\nfile = file[,c(\"lbc36no\", \"ageMRI_w2\")]\n\n# save file\nwrite.table(file, file = paste0(target, \"/LBC1936_age_w2.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#merge-all-lbc-variables-into-one-file",
    "href": "LBC_pheno.html#merge-all-lbc-variables-into-one-file",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Merge all LBC variables into one file",
    "text": "Merge all LBC variables into one file\n\n\nCode\n# it makes it more straightforward to conduct the following analyses if I merge all phenotypes into one file\n# Step 1: Read all phenotypes in\n# Step 2: Merge them\n# Step 3: Save\n\n# cognitive ability\ncog = fread(paste0(wd, \"/LBC1936_Cog_FactorScores.txt\"))\n# dementia\ndement = fread(paste0(wd, \"/LBC1936_dementia.txt\"))\n# APOE\nAPOE = fread(paste0(wd, \"/LBC1936_APOEe4.txt\"))\n# Frailty\nfrail = fread(paste0(wd, \"/LBC1936_Frailty.txt\"))\n# diabetes\ndiab = fread(paste0(wd, \"/LBC1936_diabetes.txt\"))\n# hyp\nhyp = fread(paste0(wd, \"/LBC1936_hypertension.txt\"))\n# packyears\nsmok = fread(paste0(wd, \"/LBC1936_packYears.txt\"))\n# bmi\nbmi = fread(paste0(wd, \"/LBC1936_bmi.txt\"))\n# brain age\nBrainAge = fread(paste0(wd, \"/LBC1936_brainAge.txt\"))\n# stroke\nstroke = fread(paste0(wd, \"/LBC1936_strokemask.txt\"))\n# atrophy scales\natrophy = fread(paste0(wd, \"/LBC1936_atrophyScales.txt\"))\n# age\nage = fread(paste0(wd, \"/LBC1936_age_w2.txt\"))\n\n# merge data\nDatList = list(cog, dement, APOE, frail, diab, hyp, smok, bmi, BrainAge, stroke, atrophy, age)\nLBC_merged = Reduce(function(x,y) merge(x, y, by = \"lbc36no\", all = T), DatList)\n\n# remove empty rows\nLBC_merged = LBC_merged[-which(rowSums(!is.na(LBC_merged)) == 0),]\n\n# choose prettier names\nnames(LBC_merged) = c(\"lbc36no\",\"iCog\",\"sCog\",\"dementia\",\"APOEe4\",\"iFrailty\",\"sFrailty\",\"diabetes\",\"hypertension\",\"packyears\",\"iBMI\",\"sBMI\",\"BrainAge\",\"Stroke\",\"VisualAtrophyDeep\",\"VisualAtrophySuperficial\",\"ageMRI_w2\")\n\n# remove empty rows\nLBC_merged = LBC_merged[rowSums(is.na(LBC_merged)) != ncol(LBC_merged),]\n\n# write\nfwrite(LBC_merged, file = paste0(wd, \"/LBC1936_allPheno.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "long_assocs.html",
    "href": "long_assocs.html",
    "title": "Lifetime brain atrophy is correlated with longitudinally-observed atrophic changes",
    "section": "",
    "text": "Results produced by the code below are described in the manuscript under section:\nLBA moderately captured observed atrophic changes that were longitudinally observed between two MRI scans nine years apart.\nThe outcome traits were derived using code displayed in ‘Data preparation’: LBC: neuroimaging & phenotypic data; UKB: neuroimaging & phenotypic data. Ratio and residual scores were not flipped in these analyses, like they were in the analyses presented in the main manuscript. This does not affect the directions of effects when we were comparing e.g., LBA residual score with longitudinal residual score."
  },
  {
    "objectID": "long_assocs.html#lbc",
    "href": "long_assocs.html#lbc",
    "title": "Lifetime brain atrophy is correlated with longitudinally-observed atrophic changes",
    "section": "LBC",
    "text": "LBC\nCross-sectional estimates considered from final neuroimaging visit (4th scan, 5th visit). Figures from this analysis are reported in the main paper, and this plot is in Supplementary Plot 4.\n\n\nCode\n# read in LBC data\n## longitudinal\nlong = fread(paste0(out, \"/LBC1936_longTBVWaves2and5.txt\"), select = c(\"lbc36no\",\"TBVdiff_2to5\",\"TBVratio_5to2\",\"TBVresid_2to5\"))\n## cross-sectional\ncross4 = fread(paste0(out, \"/LBC1936_crossNeuroWave4.txt\"), select = c(\"lbc36no\",\"ICV\",\"TBV\", \"diff\", \"ratio\", \"resid\"))\n\n# merge \nLBC = merge(long, cross4, by = \"lbc36no\")\n\n# number of participants\nn = nrow(LBC)\n\n# plot correlations between atrophy measures\np = plot_heatmap(dat = LBC[,c(\"ICV\",\"TBV\", \"diff\", \"ratio\", \"resid\",\"TBVdiff_2to5\",\"TBVratio_5to2\",\"TBVresid_2to5\")], \n                 axisNames = c(\"ICV\",\"TBV\",\"Difference\\nscore\\n(cross)\",\"Ratio\\nscore\\n(cross)\",\"Residual\\nscore\\n(cross)\",\"Difference\\nscore\\n(long)\",\"Ratio\\nscore\\n(long)\",\"Residual\\nscore\\n(long)\"))+\n  ggtitle(paste0(\"LBC Wave 5\\n(N = \",n,\")\"))\np\n\n\n\n\n\nCode\n#ggsave(paste0(out, \"phenotypic/LBC_corPlot.jpg\"), plot = p, width = 15, height = 15, units = \"cm\", dpi = 300)\n\n\nResidual score:\n\nr = 0.36; p = 9.9^{-10}\n\nRatio score:\n\nr = 0.29; p = 1.4^{-6}\n\nDifference score:\n\nr = 0.3; p = 5.6^{-7}"
  },
  {
    "objectID": "long_assocs.html#ukb",
    "href": "long_assocs.html#ukb",
    "title": "Lifetime brain atrophy is correlated with longitudinally-observed atrophic changes",
    "section": "UKB",
    "text": "UKB\nCross-sectional estimates considered from second neuroimaging visit. Figures from this analysis are reported in the main paper, and this plot is in Supplementary Plot 5.\n\n\nCode\n# read in UKB neuro data\nUKB = fread(paste0(out,\"/UKB_neuroNoLongProcess.txt\"))\n\n# restrict to second neuroimaging visit (i.e., third visit altogether)\nUKB3 = UKB[UKB$wave == 3,]\n\n# later edit: exclude extreme outliers from longitudinal data \nUKB3 <- UKB3[which(UKB3$TBVdiff_2to3_stand < 10),]\nUKB3 <- UKB3[which(UKB3$TBVdiff_2to3_stand > (-10)),]\n\nUKB3 <- UKB3[which(UKB3$TBVratio_3to2_stand < 10),]\nUKB3 <- UKB3[which(UKB3$TBVratio_3to2_stand > (-10)),]\n\nUKB3 <- UKB3[which(UKB3$TBVresid_2to3_stand < 10),]\nUKB3 <- UKB3[which(UKB3$TBVresid_2to3_stand > (-10)),]\n\n# now that more participants were excluded, need to re-calculate the residual score\nmodel <- lm(TBV ~ ICV, data = UKB3)\nUKB3$resid = resid(model)\n\nUKB3$resid_stand <- as.vector(scale(UKB3$resid))\n\n# number of participants\nn = nrow(UKB3)\n\n# plot correlations between atrophy measures\np = plot_heatmap(dat = UKB3[,c(\"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"TBVdiff_2to3\", \"TBVratio_3to2\", \"TBVresid_2to3\")], \n                 axisNames = c(\"ICV\", \"TBV\", \"CSF\", \"Difference\\nscore\\n(cross)\", \"Ratio\\nscore\\n(cross)\", \"Residual\\nscore\\n(cross)\", \"Difference\\nscore\\n(long)\", \"Ratio\\nscore\\n(long)\", \"Residual\\nscore\\n(long)\"))+\n  ggtitle(paste0(\"UKB\\nsecond neuroimaging visit\\n(N = \",n,\")\"))\np\n\n\n\n\n\nCode\n#ggsave(paste0(out, \"UKB_corPlot.jpg\"), plot = p, width = 15, height = 15, units = \"cm\", dpi = 300)\n\n\nResidual score:\n\nr = 0.29; p = 4^{-94}\n\nRatio score:\n\nr = 0.24; p = 3^{-64}\n\nDifference score:\n\nr = 0.21; p = 5.1^{-47}\n\n\nRepeat heatmap for residual score derived with T1-scaling factor\nFigures from this analysis are reported in the main paper, and this plot is in Supplementary Plot 6.\n\n\nCode\n### make same plot only for cross-sectional measures to compare to resid Scaling factor\nscaling = plot_heatmap(dat = UKB3[,c(\"ICV\", \"TBV\", \"T1ScalingFactor\", \"diff\", \"ratio\", \"resid\", \"residScalingFactor_stand\")], \n                 axisNames = c(\"ICV\", \"TBV\", \"T1 Scaling\\nFactor\", \"Difference\\nscore\\n(cross)\", \"Ratio\\nscore\\n(cross)\", \"Residual\\nscore\\n(cross)\", \"Residual score\\n(scaling factor)\"))+\n  ggtitle(paste0(\"UKB\\nsecond neuroimaging visit\\n(N = \",n,\")\"))\n\n#ggsave(paste0(out, \"phenotypic/UKB_scalingFactor.jpg\"), plot = scaling, width = 14, height = 14, units = \"cm\", dpi = 200)\nscaling"
  },
  {
    "objectID": "MRiShare.html",
    "href": "MRiShare.html",
    "title": "MRi-Share sample",
    "section": "",
    "text": "Load packages\n\n\nCode\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n\nGlobal measures from the MRi-Share sample can be freely downloaded online.\n\n\nRead in data\n\n\nCode\n# read in MRi-Share\nShare = fread(paste0(out, \"/MRiShare_global_IDPs_BSAF2021.csv\"))\nShare$TBV = Share$SPM_GM_Volume + Share$SPM_WM_Volume\nShare = Share[,c(\"ID\", \"Age\", \"Sex\", \"eTIV\", \"TBV\")]\nnames(Share) = c(\"ID\", \"Age\", \"Sex\", \"ICV\", \"TBV\")\n\n# convert mm3 estimates to more intuitive cm3 estimates\nShare$ICV = Share$ICV/1000\nShare$TBV = Share$TBV/1000\n\n# estimate brain atrophy from single MRI scan\nShare$diff = Share$ICV - Share$TBV\nShare$ratio = Share$TBV / Share$ICV\n\nmodel <- lm(TBV ~ ICV, data = Share)\nShare$resid = resid(model)\n\n# save intercept value from the regression\nShareintercept = summary(model)$coefficients[1,1]\n\n# standardise variables\nShare$diff_stand = as.vector(scale(Share$diff))\nShare$ratio_stand = as.vector(scale(Share$ratio))\nShare$resid_stand = as.vector(scale(Share$resid))\n\n# sanity check\n#sum((Share$diff < 0))\n\n\n\n\nPlot distributions colored by age\nShown in Supplementary Figure 3: Distributions of TBV, ICV, and lifetime brain atrophy estimated with the residual, ratio, and difference method. Histograms are coloured by age groups.\n\n\nCode\n# make age groups\nShare$Age_group <- NA\nShare$Age_group[Share$Age < 20] <- \"19 years and under\"\nShare$Age_group[Share$Age >= 20 & Share$Age < 23] <- \"20 - 22\"\nShare$Age_group[Share$Age >= 23 & Share$Age < 25] <- \"23 - 24\"\nShare$Age_group[Share$Age >= 25 & Share$Age < 27] <- \"25 - 26\"\nShare$Age_group[Share$Age >= 27 & Share$Age < 29] <- \"27 - 28\"\nShare$Age_group[Share$Age >= 29 & Share$Age < 31] <- \"29 - 30\"\nShare$Age_group[Share$Age >= 31] <- \"31 years and over\"\n\np1=ggplot(Share, aes(x=TBV, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"TBV\")+\n  theme_bw()\n\np2=ggplot(Share, aes(x=ICV, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"ICV\")+\n  theme_bw()\n\np3=ggplot(Share, aes(x=resid_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Residual score\")+\n  theme_bw()\n\np4=ggplot(Share, aes(x=ratio_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Ratio score\")+\n  theme_bw()\n\np5=ggplot(Share, aes(x=diff_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Difference score\")+\n  theme_bw()\n\npShare <- ggarrange(p1,p2,p3,p4,p5, nrow = 1, common.legend = T, legend = \"bottom\")\n# add title\npShare <- annotate_figure(pShare, top = text_grob(\"MRi-Share\",face = \"bold\", size = 14))\n\n#ggsave(paste0(out,\"phenotypic/Share_disttributions.jpg\"), bg = \"white\",plot = pShare, width = 30, height = 10, units = \"cm\", dpi = 300)\npShare"
  },
  {
    "objectID": "pheno_assocs.html",
    "href": "pheno_assocs.html",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "",
    "text": "Results produced by the code below are described in the manuscript under section:\nMeasures of LBA predict brain atrophy rated by neuroradiological experts, as well as other ageing-related health traits such as frailty and cognitive ability.\nThe outcome traits were derived using code displayed in ‘Data preparation’: LBC: neuroimaging & phenotypic data; UKB: neuroimaging & phenotypic data."
  },
  {
    "objectID": "pheno_assocs.html#lothian-birth-cohort-lbc-1936",
    "href": "pheno_assocs.html#lothian-birth-cohort-lbc-1936",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "Lothian Birth Cohort (LBC) 1936",
    "text": "Lothian Birth Cohort (LBC) 1936\n\nWave 2: Lifetime brain atrophy (LBA; cross-sectional)\n\n\nCode\n# read in LBC data \npheno = fread(paste0(wd, \"/LBC1936_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/LBC1936_crossNeuroWave1.txt\"), data.table = F)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"Stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"iCog\", \"sCog\", \"iFrailty\", \"sFrailty\", \"iBMI\", \"sBMI\", \"BrainAge\",\"ageMRI_w2\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$resid_stand = neuro$resid_stand*(-1)\nneuro$ratio_stand = neuro$ratio_stand*(-1)\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"VisualAtrophyDeep\", \"VisualAtrophySuperficial\"), modelType = c(\"linear\", \"linear\"))\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"diff_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"ratio_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"resid_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nraw2 = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nWave 5: LBA (cross-sectional)\n\n\nCode\npheno = fread(paste0(wd, \"/LBC1936_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/LBC1936_crossNeuroWave4.txt\"), data.table = F)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"Stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"iCog\", \"sCog\", \"iFrailty\", \"sFrailty\", \"iBMI\", \"sBMI\", \"BrainAge\",\"ageMRI_w2\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$resid_stand = neuro$resid_stand*(-1)\nneuro$ratio_stand = neuro$ratio_stand*(-1)\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"VisualAtrophyDeep\", \"VisualAtrophySuperficial\"), modelType = c(\"linear\", \"linear\"))\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"diff_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"ratio_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"resid_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nraw5 = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nLongitudinal data\n\n\nCode\n# read in LBC data \npheno = fread(paste0(wd, \"/LBC1936_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), data.table = F)\n\n# make sure binary variables are coded as factors\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"Stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"iCog\", \"sCog\", \"iFrailty\", \"sFrailty\", \"iBMI\", \"sBMI\", \"BrainAge\",\"ageMRI_w2\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$TBVresid_2to5_stand = neuro$TBVresid_2to5_stand*(-1)\nneuro$TBVratio_5to2_stand = neuro$TBVratio_5to2_stand*(-1)\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"VisualAtrophyDeep\", \"VisualAtrophySuperficial\"), modelType = c(\"linear\", \"linear\"))\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVdiff_2to5_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVratio_5to2_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVresid_2to5_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nrawObs = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nCode\n# save cross data\nraw2$wave = 2\nraw5$wave = 5\nraw = rbind(raw2, raw5)\nraw$purpose = \"Estimated Atrophy (cross-sectional)\"\nraw$purpose = \"Estimated Atrophy (cross-sectional)\"\nrawObs$purpose = \"Observed Atrophy (longitudinal)\"\nrawObs$wave = NA\nsave = rbind(raw, rawObs)\nfwrite(save, file = paste0(wd, \"/LBC1936_assocs_observed_vs_estimated_atrophy.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "pheno_assocs.html#uk-biobank-ukb",
    "href": "pheno_assocs.html#uk-biobank-ukb",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "UK Biobank (UKB)",
    "text": "UK Biobank (UKB)\n\nWave 2: LBA (cross-sectional)\n\n\nCode\n# read in LBC data \npheno = fread(paste0(wd, \"/UKB_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/UKB_neuroNoLongProcess.txt\"), data.table = F)\n# get wave of interest\nneuro = neuro[which(neuro$wave == 2),]\n\n# edit:30/09/2024 (realised later that I hadn't cleaned the longitudinal data the same as the cross-sectional data, 10SD cutoff which some participants violate with the longitudinal data) - removed 6 participants\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand < 10),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand > (-10)),]\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$resid_stand = neuro$resid_stand*(-1)\nneuro$ratio_stand = neuro$ratio_stand*(-1)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"cog\", \"BMI\", \"brainAge\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"packyears\", \"frailty\"), modelType = c(\"hurdle\", \"hurdle\")) # can overwrite with logistic of linear\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"diff_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"ratio_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"resid_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nraw2 = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nWave 3: LBA (cross-sectional)\n\n\nCode\n# read in data \npheno = fread(paste0(wd, \"/UKB_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/UKB_neuroNoLongProcess.txt\"), data.table = F)\n# get wave of interest\nneuro = neuro[which(neuro$wave == 3),]\n# edit:30/09/2024 (realised later that I hadn't cleaned the longitudinal data the same as the cross-sectional data, 10SD cutoff which some participants violate with the longitudinal data) - removed 6 participants\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand < 10),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand > (-10)),]\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$resid_stand = neuro$resid_stand*(-1)\nneuro$ratio_stand = neuro$ratio_stand*(-1)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"cog\", \"BMI\", \"brainAge\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"packyears\", \"frailty\"), modelType = c(\"hurdle\", \"hurdle\")) # can overwrite with logistic of linear\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"diff_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"ratio_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"resid_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nraw5 = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nLongitudinal atrophy scores\n\n\nCode\n# read in data \npheno = fread(paste0(wd, \"/UKB_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/UKB_neuroNoLongProcess.txt\"), data.table = F)\n# shouldn't make a difference which wave we keep because long is saved double\nneuro = neuro[which(neuro$wave == 2),]\n# edit:30/09/2024 (realised later that I hadn't cleaned the longitudinal data the same as the cross-sectional data, 10SD cutoff which some participants violate with the longitudinal data) - removed 6 participants\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand < 10),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand > (-10)),]\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$TBVresid_2to3_stand = neuro$TBVresid_2to3_stand*(-1)\nneuro$TBVratio_3to2_stand = neuro$TBVratio_3to2_stand*(-1)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"cog\", \"BMI\", \"brainAge\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"packyears\", \"frailty\"), modelType = c(\"hurdle\", \"hurdle\")) # can overwrite with logistic of linear\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVdiff_2to3_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVratio_3to2_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVresid_2to3_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nrawObs = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nCode\n# save cross data\nraw2$wave = 2\nraw5$wave = 3\nraw = rbind(raw2, raw5)\nraw$purpose = \"Estimated Atrophy (cross-sectional)\"\nraw$purpose = \"Estimated Atrophy (cross-sectional)\"\nrawObs$purpose = \"Observed Atrophy (longitudinal)\"\nrawObs$wave = NA\nsave = rbind(raw, rawObs)\nfwrite(save, file = paste0(wd, \"/UKB_assocs_observed_vs_estimated_atrophy.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "pheno_assocs.html#lbc1936",
    "href": "pheno_assocs.html#lbc1936",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "LBC1936",
    "text": "LBC1936\n\n\nCode\n# betas between atrophy scores and traits\nsetwd(wd)\nfile = list.files(pattern=\"LBC1936_assocs_observed_vs_estimated_atrophy\")\nassoc = fread(file)\n\n# brain age phenotype is not intuitively interpretable:\n# positive value should mean the participant has a healthier looking brain than expected given their age \n# negative value should mean the participant has an unhealthier looking brain than expected\n# Hence, here we just flip the assocs so that more LBA is associated with older brain age\nassoc$beta[which(assoc$Outcome == \"BrainAge\")] <- assoc$beta[which(assoc$Outcome == \"BrainAge\")]*(-1)\n\n# calculate lower and upper bounds of assoc\nassoc$ci_l = assoc$beta - (1.96*assoc$std.error)\nassoc$ci_u = assoc$beta + (1.96*assoc$std.error)\n\n# because beta shouldn't exceed 1, and it makes the plots look ugly, I will artificially reduce those here\nassoc$ci_l = ifelse(assoc$ci_l < -1.21, -1.21, assoc$ci_l)\nassoc$ci_u = ifelse(assoc$ci_u > 1.21, 1.21, assoc$ci_u)\n\n# make data frame for geom_label\nassoc$label = paste0(round(assoc$R2_percent, digits = 1), \"%\")\n# only print the label is correlation is significant\nsigBonferroni = 0.05/length(unique(assoc$Outcome))\nassoc$label[which(assoc$p.value > sigBonferroni)] = NA\n\n\n######################################################################################\n###### Continuous traits\n######################################################################################\n############## ESTIMATED ATROPHY\n### first, only estimated atrophy\n# only keep 'diff_stand', 'ratio_stand' or 'resid_stand'\nestimated = assoc[grepl(\"diff_stand|ratio_stand|resid_stand\",assoc$Predictor),]\n# restrict to wave 5 (more atrophy)\nestimated = estimated[grepl(\"5\", estimated$wave),]\n# restrict to continuous variables only\nestimated = estimated[grepl(\"Linear\", estimated$Stat.model)]\n\n# c(\"iCog\",\"sCog\",\"Dementia\",\"APOEe4\",\"iFrailty\",\"sFrailty\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"iBMI\",\"sBMI\",\"BrainAge\",\"Packyears\")\n\n# order phenotypes\nestimated$Outcomes <- factor(estimated$Outcome, \n                             levels=c(\"VisualAtrophySuperficial\",\"VisualAtrophyDeep\",\"iCog\",\"sCog\",\"iFrailty\",\"sFrailty\",\"iBMI\",\"sBMI\",\"BrainAge\"),\n                             labels = c(\"Visual atrophy\\nrating\\n (superficial)\",\"Visual atrophy\\nrating (deep)\",\"iCog\",\"sCog\",\"iFrailty\",\"sFrailty\",\"iBMI\",\"sBMI\",\"BrainAge\"),\n                             ordered=T)\n\n\n\nestLinear=ggplot(data=estimated, aes(y=Outcomes,x=beta),shape=3)+\n  xlim(-1.25, 1.25)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ci_l, xmax=ci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 0, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=-0.7, label = label, col=Predictor), size = 3.5, position=position_dodge(width=0.8))+\n  scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(-0.9, 0.9), breaks = seq(-1,1,by=0.4))+\n    theme_bw()+\n  ylab(\"Continuous outcome traits\")+\n  xlab(\"Linear regression:\\nbeta (95%CI)\")+\n  ggtitle(\"Lifetime brain atrophy\\nat age 82 years\\n(wave 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        axis.title.y = element_text(face=\"bold\", colour='black', size=12),\n        axis.title.x = element_text(colour='black', size=12),\n        panel.border = element_blank(),\n        plot.title = element_text(face = \"bold\", colour='black', size=12))+\n  theme(legend.position=\"none\")\n\n### why is diff score always the other way? Sensible because larger diff score is worse, but smaller ratio score is worse\n\n############## OBSERVED ATROPHY\n### only keep longitudinal atrophy\nobserved = assoc[grepl(\"TBV\",assoc$Predictor),]\n# restrict to continuous variables only\nobserved = observed[grepl(\"Linear\", observed$Stat.model)]\n\n# order phenotypes\nobserved$Outcomes <- factor(observed$Outcome, \n                            levels=c(\"VisualAtrophySuperficial\",\"VisualAtrophyDeep\",\"iCog\",\"sCog\",\"iFrailty\",\"sFrailty\",\"iBMI\",\"sBMI\",\"BrainAge\"),\n                            labels = c(\"Visual atrophy\\nrating\\n (superficial)\",\"Visual atrophy\\nrating (deep)\",\"iCog\",\"sCog\",\"iFrailty\",\"sFrailty\",\"iBMI\",\"sBMI\",\"BrainAge\"),\n                            ordered=T)\n\nobsLinear=ggplot(data=observed, aes(y=Outcomes,x=beta),shape=3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ci_l, xmax=ci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 0, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=-0.7, label = label, col=Predictor), size = 3.5,position=position_dodge(width=0.8))+\n    scale_y_discrete(limits = rev(levels(as.factor(observed$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(-0.9, 0.9), breaks = seq(-1,1,by=0.4))+\n  theme_bw()+\n  ylab(\"ontinuous outcome traits\")+\n  xlab(\"Linear regression:\\nbeta (95%CI)\")+\n  ggtitle(\"Observed atrophic changes\\nbetween age 73 and 82\\n(waves 2 to 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        axis.title.y = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=12))+\n  theme(legend.position=\"none\")\n\nplot1 = estLinear + \n        obsLinear + \n        plot_layout(ncol = 2)\n        #plot_annotation(title = \"Correlations with health-related outcomes in LBC\",\n                        #theme = theme(plot.title = element_text(face = \"bold\", colour = \"black\", size = 14, hjust = 0.5))) & theme(legend.position = 'bottom')\n\n#ggpubr::annotate_figure(plot, top = text_grob(\"Correlations with health-related outcomes in LBC\", \n#                                      color = \"black\", face = \"bold\", size = 14))\n\n######################################################################################\n###### Binary traits\n######################################################################################\n############## ESTIMATED ATROPHY\n### first, only estimated atrophy\n# only keep 'diff_stand', 'ratio_stand' or 'resid_stand'\nestimated = assoc[grepl(\"diff_stand|ratio_stand|resid_stand\",assoc$Predictor),]\n# restrict to wave 5 (more atrophy)\nestimated = estimated[grepl(\"5\", estimated$wave),]\n# restrict to continuous variables only\nestimated = estimated[grepl(\"Logistic|Hurdle\", estimated$Stat.model)]\n\n# transform beta to OR\nestimated$OR = exp(estimated$beta)\nestimated$ORci_l = exp(estimated$beta - (1.96 * estimated$std.error))\nestimated$ORci_u = exp(estimated$beta + (1.96 * estimated$std.error))\n\n# display as log odds\nestimated$logOdds = log(estimated$OR)\nestimated$logOdds_ci_l = log(estimated$ORci_l)\nestimated$logOdds_ci_u = log(estimated$ORci_u)\n\n# order phenotypes\nestimated$Outcomes <- factor(estimated$Outcome, \n                             levels=c(\"dementia\",\"APOEe4\",\"diabetes\",\"Stroke\",\"hypertension\",\"packyears\"),\n                             labels = c(\"Dementia\",\"APOEe4\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"Packyears\"),\n                             ordered=T)\n\nestLog=ggplot(data=estimated, aes(y=Outcomes,x=OR),shape=3)+\n  xlim(0, 2.3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ORci_l, xmax=ORci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 1, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=0.4, label = label, col=Predictor), size = 3.5,position=position_dodge(width=0.8))+\n    scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(0.3,3), breaks = seq(0,3,by=0.4))+\n  theme_bw()+\n  ylab(\"Binary outcome traits\")+\n  xlab(\"Logistic regression:\\nOdds ratio (95%CI)\")+\n  #ggtitle(\"'Estimated' brain atrophy\\n(at wave 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        panel.border = element_blank(),\n        axis.title.y = element_text(face=\"bold\", colour='black', size=12),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=13))+\n  theme(legend.position=\"none\")\n\n\n############## OBSERVED ATROPHY\n### only keep longitudinal atrophy\nobserved = assoc[grepl(\"TBV\",assoc$Predictor),]\n# restrict to continuous variables only\nobserved = observed[grepl(\"Logistic|Hurdle\", observed$Stat.model)]\n\n# transform beta to OR\nobserved$OR = exp(observed$beta)\nobserved$ORci_l = exp(observed$beta - (1.96 * observed$std.error))\nobserved$ORci_u = exp(observed$beta + (1.96 * observed$std.error))\n\n# order phenotypes\nobserved$Outcomes <- factor(observed$Outcome, \n                            levels=c(\"dementia\",\"APOEe4\",\"diabetes\",\"Stroke\",\"hypertension\",\"packyears\"),\n                            labels = c(\"Dementia\",\"APOEe4\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"Packyears\"),\n                            ordered=T)\n\nobsLog=ggplot(data=observed, aes(y=Outcomes,x=OR),shape=3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ORci_l, xmax=ORci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 1, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=0.4, label = label, col=Predictor), size = 3.5,position=position_dodge(width=1))+\n  scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(0,6.7), breaks = seq(0,6,by=1))+\n  theme_bw()+\n  ylab(\"Binary outcome traits\")+\n  xlab(\"Logistic regression:\\nOdds ratio (95%CI)\")+\n  #ggtitle(\"'Observed' brain atrophy\\n(wave 2 to 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        panel.border = element_blank(),\n        axis.title.y = element_blank(),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=13),\n        legend.position = \"bottom\")\n\nplot2 = estLog + \n  obsLog + \n  plot_layout(guides = \"collect\")\n\nlayout <- \"\nAB\nAB\nCD\n\"\n\nplot <- estLinear + obsLinear + estLog + obsLog + \n                    plot_layout(design = layout, guides = \"collect\")+\n                    plot_annotation(title = \"Brain atrophy associations with health-related outcomes in LBC\",\n                  caption = \"Note: R2 estimates are only printed if the corresponding association passed correction for multiple testing (p=0.05/15).\\n'Packyears' was analysed with Hurdle regression, but the coefficient printed here is from the negative binomial part of the equation,\\nhence on log-odds scale like the other binary traits. Pseudo-R2 estimates are for the full hurdle model.\",\n                    tag_levels = \"A\",\n                    theme = theme(legend.position = \"bottom\",plot.title = element_text(face = \"bold\", colour = \"black\", size = 14, hjust = 0.5))) \n\n\n#ggsave(paste0(out,\"phenotypic/LBC_assocs_cross_vs_long.jpg\"), bg = \"white\",plot = plot, width = 20, height = 28, units = \"cm\", dpi = 300)"
  },
  {
    "objectID": "pheno_assocs.html#ukb",
    "href": "pheno_assocs.html#ukb",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "UKB",
    "text": "UKB\nThis plot is in Supplementary Plot 7.\n\n\nCode\n# read in data \nsetwd(wd)\nfile = list.files(pattern=\"UKB_assocs_observed_vs_estimated_atrophy\")\nassoc = fread(file)\n\n# brain age phenotype is not intuitively interpretable:\n# positive value should mean the particpant has a healthier looking brain tthan expected given their age \n# negative value should mean the participant has an unhealthier looking brain than expected\n# Hence, here we just flip the assocs so that more LBA is associated with older brian age\nassoc$beta[which(assoc$Outcome == \"brainAge\")] <- assoc$beta[which(assoc$Outcome == \"brainAge\")]*(-1)\n\n# calculate lower and upper bounds of assoc\nassoc$ci_l = assoc$beta - (1.96*assoc$std.error)\nassoc$ci_u = assoc$beta + (1.96*assoc$std.error)\n\n# because beta shouldn't exceed 1, and it makes the plots look ugly, I will artifically reduce those here\nassoc$ci_l = ifelse(assoc$ci_l < -1.21, -1.21, assoc$ci_l)\nassoc$ci_u = ifelse(assoc$ci_u > 1.21, 1.21, assoc$ci_u)\n\n# make data frame for geom_label\nassoc$label = paste0(round(assoc$R2_percent, digits = 1), \"%\")\n# only print the label is correlation is significant\nsigBonferroni = 0.05/length(unique(assoc$Outcome))\nassoc$label[which(assoc$p.value > sigBonferroni)] = NA\n\n######################################################################################\n###### Continuous traits\n######################################################################################\n############## ESTIMATED ATROPHY\n### first, only estimated atrophy\n# only keep 'diff_stand', 'ratio_stand' or 'resid_stand'\nestimated = assoc[grepl(\"diff_stand|ratio_stand|resid_stand\",assoc$Predictor),]\n# restrict to wave 3 (more atrophy)\nestimated = estimated[grepl(\"3\", estimated$wave),]\n# restrict to continuous variables only\nestimated = estimated[grepl(\"Linear\", estimated$Stat.model)]\n\n# order phenotypes\nestimated$Outcomes <- factor(estimated$Outcome, \n                             levels=c(\"cog\",\"BMI\",\"brainAge\"),\n                             labels = c(\"Cog\",\"BMI\",\"BrainAge\"),\n                             ordered=T)\n\nestLinear=ggplot(data=estimated, aes(y=Outcomes,x=beta),shape=3)+\n  xlim(-1.25, 1.25)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ci_l, xmax=ci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 0, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=-0.7, label = label, col=Predictor), size = 3.5, position=position_dodge(width=0.8))+\n  scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(-0.9, 0.9), breaks = seq(-1,1,by=0.4))+\n    theme_bw()+\n  ylab(\"Continuous outcome traits\")+\n  xlab(\"Linear regression:\\nbeta (95%CI)\")+\n  ggtitle(\"Lifetime brain atrophy\\n(second neuroimaging visit)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        axis.title.y = element_text(face=\"bold\", colour='black', size=12),\n        axis.title.x = element_text(colour='black', size=12),\n        panel.border = element_blank(),\n        plot.title = element_text(face = \"bold\", colour='black', size=13))+\n  theme(legend.position=\"none\")\n\n############## OBSERVED ATROPHY\n### only keep longitudinal atrophy\nobserved = assoc[grepl(\"TBV\",assoc$Predictor),]\n# restrict to continuous variables only\nobserved = observed[grepl(\"Linear\", observed$Stat.model)]\n\n# order phenotypes\nobserved$Outcomes <- factor(observed$Outcome, \n                             levels=c(\"cog\",\"BMI\",\"brainAge\"),\n                             labels = c(\"Cog\",\"BMI\",\"BrainAge\"),\n                             ordered=T)\n\nobsLinear=ggplot(data=observed, aes(y=Outcomes,x=beta),shape=3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ci_l, xmax=ci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 0, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=-0.7, label = label, col=Predictor), size = 3.5,position=position_dodge(width=0.8))+\n    scale_y_discrete(limits = rev(levels(as.factor(observed$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(-0.9, 0.9), breaks = seq(-1,1,by=0.4))+\n  theme_bw()+\n  ylab(\"ontinuous outcome traits\")+\n  xlab(\"Linear regression:\\nbeta (95%CI)\")+\n  ggtitle(\"Observed atrophic changes\\n(first to second neuroimaging visit)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        axis.title.y = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=12))+\n  theme(legend.position=\"none\")\n\n######################################################################################\n###### Binary traits\n######################################################################################\n############## ESTIMATED ATROPHY\n### first, only estimated atrophy\n# only keep 'diff_stand', 'ratio_stand' or 'resid_stand'\nestimated = assoc[grepl(\"diff_stand|ratio_stand|resid_stand\",assoc$Predictor),]\n# restrict to visit 3 (more atrophy)\nestimated = estimated[grepl(\"3\", estimated$wave),]\n# restrict to continuous variables only\nestimated = estimated[grepl(\"Logistic|Hurdle\", estimated$Stat.model)]\n\n# transform beta to OR\nestimated$OR = exp(estimated$beta)\nestimated$ORci_l = exp(estimated$beta - (1.96 * estimated$std.error))\nestimated$ORci_u = exp(estimated$beta + (1.96 * estimated$std.error))\n\n# display as log odds\nestimated$logOdds = log(estimated$OR)\nestimated$logOdds_ci_l = log(estimated$ORci_l)\nestimated$logOdds_ci_u = log(estimated$ORci_u)\n\n# order phenotypes\nestimated$Outcomes <- factor(estimated$Outcome, \n                             levels=c(\"dementia\",\"APOEe4\",\"diabetes\",\"stroke\",\"hypertension\",\"packyears\",\"frailty\"),\n                             labels = c(\"Dementia\",\"APOEe4\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"Packyears\",\"Frailty\"),\n                             ordered=T)\n\nestLog=ggplot(data=estimated, aes(y=Outcomes,x=OR),shape=3)+\n  xlim(0, 2.3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ORci_l, xmax=ORci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 1, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=0.4, label = label, col=Predictor), size = 3.5,position=position_dodge(width=0.8))+\n    scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(0.3,3.1), breaks = seq(0,3,by=0.4))+\n  theme_bw()+\n  ylab(\"Binary outcome traits\")+\n  xlab(\"Logistic regression:\\nOdds ratio (95%CI)\")+\n  #ggtitle(\"'Estimated' brain atrophy\\n(at wave 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        panel.border = element_blank(),\n        axis.title.y = element_text(face=\"bold\", colour='black', size=12),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=13))+\n  theme(legend.position=\"none\")\n\n############## OBSERVED ATROPHY\n### only keep longitudinal atrophy\nobserved = assoc[grepl(\"TBV\",assoc$Predictor),]\n# restrict to continuous variables only\nobserved = observed[grepl(\"Logistic|Hurdle\", observed$Stat.model)]\n\n# transform beta to OR\nobserved$OR = exp(observed$beta)\nobserved$ORci_l = exp(observed$beta - (1.96 * observed$std.error))\nobserved$ORci_u = exp(observed$beta + (1.96 * observed$std.error))\n\n# order phenotypes\nobserved$Outcomes <- factor(observed$Outcome, \n                             levels=c(\"dementia\",\"APOEe4\",\"diabetes\",\"stroke\",\"hypertension\",\"packyears\",\"frailty\"),\n                             labels = c(\"Dementia\",\"APOEe4\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"Packyears\",\"Frailty\"),\n                             ordered=T)\n\nobsLog=ggplot(data=observed, aes(y=Outcomes,x=OR),shape=3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ORci_l, xmax=ORci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 1, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=0.4, label = label, col=Predictor), size = 3.5,position=position_dodge(width=1))+\n  scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(0.3,3.1), breaks = seq(0,3,by=0.4))+\n  theme_bw()+\n  ylab(\"Binary outcome traits\")+\n  xlab(\"Logistic regression:\\nOdds ratio (95%CI)\")+\n  #ggtitle(\"'Observed' brain atrophy\\n(wave 2 to 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        panel.border = element_blank(),\n        axis.title.y = element_blank(),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=13),\n        legend.position = \"bottom\")\n\n# plot all together\nlayout <- \"\nAB\nCD\nCD\n\"\n\nplot <- estLinear + obsLinear + estLog + obsLog + \n                    plot_layout(design = layout,guides = \"collect\")+\n                    plot_annotation(title = \"Brain atrophy correlations with health-related outcomes in UKB\",\n                  caption = \"Note: R2 estimates are only printed if the corresponding association passed correction for multiple testing (p=0.05/7).\\n'Packyears' & 'frailty' were analysed with Hurdle regression, but the coefficient printed here is from the negative binomial part of the equation,\\nhence on log-odds scale like the other binary traits. Pseudo-R2 estimates are for the full hurdle model.\",\n                    tag_levels = \"A\",\n                    theme = theme(legend.position = \"bottom\",plot.title = element_text(face = \"bold\", colour = \"black\", size = 14, hjust = 0.5))) \n\nggsave(paste0(out,\"phenotypic/UKB_assocs_cross_vs_long.jpg\"), bg = \"white\",plot = plot, width = 20, height = 28, units = \"cm\", dpi = 150)"
  },
  {
    "objectID": "rg.html#genetic-correlations-in-genomicsem",
    "href": "rg.html#genetic-correlations-in-genomicsem",
    "title": "Genetic correlations",
    "section": "Genetic correlations in GenomicSEM",
    "text": "Genetic correlations in GenomicSEM\n\nMunge\n\n\nCode\nlibrary(devtools)\n# install_github(\"GenomicSEM/GenomicSEM\")\nlibrary(GenomicSEM)\nlibrary(stringr)\nlibrary(data.table)\n\n#######################################################################\n### MUNGE\n#######################################################################\nfiles <- list.files(pattern = \"GWAS_brainAtrophy_\")\n#files <- files[grepl(\"males\", files)]\nhm3 <- \"/eur_w_ld_chr/w_hm3.snplist\"\nmaf.filter = 0.01\ninfo.filter = 0.9\n\n# cycle through each of the files\nfor(i in files){\n\n    # read in file first\n    file <- fread(i)\n    # if it's residual or ratio score, we flip the effect sizes because that was also done throughout the rest of manuscript to harmonise the scores (higher score, more atrophy)\n    if(grepl(\"ratio|resid\", i)){\n        file$BETA <- file$BETA * (-1)\n    }\n    # write temporarily because munge can't read in gz files\n    fwrite(file, paste0(\"temp\",i,\".txt\"), na = \"NA\", quote = F, sep = \"\\t\", row.names = FALSE, col.names = TRUE)\n\n    # isolate trait name\n    trait.name = str_remove(i, \"GWAS_brainAtrophy_\")\n    trait.name = str_remove(trait.name, \"_N43110.gz\")\n\n    # munge this file\n    munge(files = paste0(\"temp\",i,\".txt\"), \n            trait.names = trait.name,\n            hm3 = hm3, \n            maf.filter = maf.filter, \n            info.filter = info.filter,\n            log.name = paste0(trait.name),\n            column.names = list(SNP = \"ID\", \n                                A1 = \"ALLELE1\",\n                                A2 = \"ALLELE0\", \n                                effect = \"BETA\", \n                                P = \"P\", \n                                N = \"N\"))\n    # remove temp file again\n    file.remove(paste0(\"temp\",i,\".txt\"))\n}\n\n############################################################\n#### Munging difficult sumstats\n############################################################\nlibrary(devtools)\nlibrary(GenomicSEM)\nlibrary(stringr)\nlibrary(data.table)\nlibrary(tidyr)\n\n\nsetwd(\"/CCACE_Shared/Anna_F/BrainAtrophy/data/GWASsumstats\")\n\nhm3 <- \"/CCACE_Shared/Anna_F/BrainAtrophy/eur_w_ld_chr/w_hm3.snplist\"\nmaf.filter = 0.01\ninfo.filter = 0.9\n\ni=list.files(pattern = \"CHARGE\")\nfile <- fread(i, fill=TRUE)\n\n# separate fused column\nfile <- separate(file, col = \"N RSNUMBERS\", into = c(\"N\",\"RSNUMBER\"), sep = \" \")\n# some rows are empty and the entry has shifted to to column V13\nfile$RSNUMBER <- ifelse(is.na(file$RSNUMBER), file$V13, file$RSNUMBER)\n# problem with p-value column - rename\nnames(file)[grep(\"P-\", names(file))] <- \"P\"\n\n# write temporarily because munge can't read in gz files\nfwrite(file, paste0(\"temp\",i,\".txt\"), na = \"NA\", quote = F, sep = \"\\t\", row.names = FALSE, col.names = TRUE)\n\n# isolate trait name\ntrait.name = \"ICV_Adams.et.al\"\n\n# munge this file\nmunge(files = paste0(\"temp\",i,\".txt\"), \n        trait.names = trait.name,\n        hm3 = hm3, \n        maf.filter = maf.filter, \n        info.filter = info.filter,\n        log.name = paste0(trait.name),\n        column.names = list(SNP = \"RSNUMBER\", \n                            A1 = \"Allele1\",\n                            A2 = \"Allele2\", \n                            Z = \"Zscore\", \n                            P = \"P\", \n                            N = \"Weight\"))\n\n# remove temp file again\nfile.remove(paste0(\"temp\",i,\".txt\"))\n\n\n\n\nGenetic correlations\n\n\nCode\n#######################################################################\n### LDSC\n#######################################################################\nlibrary(devtools)\nlibrary(GenomicSEM)\nlibrary(stringr)\n\n# vector of munged sumstats\ntraits <- list.files(pattern = \".sumstats.gz\")\n# remove log files\ntraits <- traits[!grepl(\".log\", traits)]\n# remove BrainChange GWAS because they produce negative h2 estimate\ntraits = traits[!grepl(\"BrainChange\", traits)]\n# remove male and female specific sumstats\ntraits = traits[!grepl(\"males\", traits)]\n# remove SNP-by-age analyses\ntraits = traits[!grepl(\"SNPxage\", traits)]\n\n# trait.names\ntrait.names <- str_remove(traits, pattern = \".sumstats.gz\")\n# folder with ld scores\nld = \"eur_w_ld_chr/\"\n# folder with ld weights\nwld = \"eur_w_ld_chr/\"\n\n# run ldsc\nLDSCoutput <- ldsc(traits = traits, \n                    ld = ld, wld = wld, \n                    sample.prev = rep(NA, length(traits)),\n                    population.prev = rep(NA, length(traits)),\n                    trait.names = trait.names,\n                    stand = T)\n\n# dimnames\ndimnames(LDSCoutput$S)[[1]] <- dimnames(LDSCoutput$S)[[2]]\ndimnames(LDSCoutput$S_Stand)[[1]] <- dimnames(LDSCoutput$S)[[2]]\ndimnames(LDSCoutput$S_Stand)[[2]] <- dimnames(LDSCoutput$S)[[2]]\ndimnames(LDSCoutput$I)[[1]] <- dimnames(LDSCoutput$S)[[2]]\ndimnames(LDSCoutput$I)[[2]] <- dimnames(LDSCoutput$S)[[2]]\n\nsave(LDSCoutput, file = \"LDSCoutput_neurodegenrative.RData\")\n#save(LDSCoutput, file = \"LDSCoutput_sexsplit.RData\")\n\n###########################################################################\n#### Calculate associated matrix of Z statistics for this rG matrix\n###########################################################################\nlibrary(gdata)\nS_LD<-LDSCoutput$S\n\n#pull V: the sampling covariance matrix\nV_LD<-LDSCoutput$V\n\n#standardize S [equivalent of cov2cor]\nD=sqrt(diag(diag(S_LD)))\nS_Stand=solve(D)%*%S_LD%*%solve(D)\nrownames(S_Stand)<-rownames(S_LD)\ncolnames(S_Stand)<-colnames(S_Stand)\n\n#obtain diagonals of the original V matrix and take their sqrt to get SE's\nDvcov<-sqrt(diag(V_LD))\n\n#calculate the ratio of the rescaled and original S matrices\nscaleO=as.vector(lowerTriangle((S_Stand/S_LD),diag=T))\n\n## Make sure that if ratio in NaN (devision by zero) we put the zero back in\nscaleO[is.nan(scaleO)] <- 0\n\n#rescale the SEs by the same multiples that the S matrix was rescaled by\nDvcovl<-as.vector(Dvcov*t(scaleO))\n\n#obtain the sampling correlation matrix by standardizing the original V matrix\nVcor<-cov2cor(V_LD)\n\n#rescale the sampling correlation matrix by the appropriate diagonals\nV_stand<-diag(Dvcovl)%*%Vcor%*%diag(Dvcovl)\n\n#calculate SEs of rg\nk<-nrow(LDSCoutput$S)\nSE_stand<-matrix(0, k, k)\nSE_stand[lower.tri(SE_stand,diag=TRUE)] <-sqrt(diag(V_stand))\n\n#calculate Z of rg matrix\nZ_cor<-S_Stand/SE_stand\n\n\n\n\nPlot results (full sample)\n\n\nCode\n###########################################################################\n#### Genetic correlation matrix\n###########################################################################\n### INCLUDE NEURODEGENERATIVE DISEASES\n\nload(\"LDSCoutput_neurodegenrative.RData\")\n\n# pull out correlation matrix\ncormat <- LDSCoutput$S_Stand\n\n# re-roder\ncormat = cormat[c(\"ALZ\", \"ALZ_2019\", \"PD\", \"ALS\", \"ICV_Adams.et.al\", \"TBV_Zhao.et.al\", \"TBV_Smith.et.al\", \"PC1_whole_brain_greymatter\", \"brainage\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"diff_stand\", \"ratio_stand\", \"resid_stand\"), c(\"ALZ\", \"ALZ_2019\", \"PD\", \"ALS\", \"ICV_Adams.et.al\", \"TBV_Zhao.et.al\", \"TBV_Smith.et.al\", \"PC1_whole_brain_greymatter\", \"brainage\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"diff_stand\", \"ratio_stand\", \"resid_stand\")]\n\n# re-name\ndimnames(cormat)[[1]] <-c(\"Alzheimer's & related dementias\\n(Bellenguez et al., 2021)\", \"Alzheimer's\\n(Kunkle et al., 2019)\", \"Parkinson's\\n(Nalls et al., 2019)\", \"Amyotrophic lateral sclerosis\\n(van Rheenen et al. 2021)\", \"ICV\\n(Adams et al., 2016)\", \"TBV\\n(Zhao et al., 2018)\", \"TBV\\n(Smith et al., 2021)\", \"Grey matter whole brain PC1\\n(Furtjes et al., 2023)\", \"Brain age\\n(Kaufmann et al., 2019)\",\"ICV\", \"TBV\", \"CSF\", \"Difference score\\n(cross)\", \"Ratio score\\n(cross)\", \"Residual score\\n(cross)\")\ndimnames(cormat)[[2]] <-c(\"Alzheimer's & related dementias\\n(Bellenguez et al., 2021)\", \"Alzheimer's\\n(Kunkle et al., 2019)\", \"Parkinson's\\n(Nalls et al., 2019)\", \"Amyotrophic lateral sclerosis\\n(van Rheenen et al. 2021)\", \"ICV\\n(Adams et al., 2016)\", \"TBV\\n(Zhao et al., 2018)\", \"TBV\\n(Smith et al., 2021)\", \"Grey matter whole brain PC1\\n(Furtjes et al., 2023)\", \"Brain age\\n(Kaufmann et al., 2019)\",\"ICV\", \"TBV\", \"CSF\", \"Difference score\\n(cross)\", \"Ratio score\\n(cross)\", \"Residual score\\n(cross)\")\n\n# define function to obtain lower triangle\n  get_lower_tri<-function(cormat){\n    cormat[upper.tri(cormat)] <- NA\n    return(cormat)\n  }\n\n  # get correlation matrix for both samples together\n  cor = get_lower_tri(cormat)\n  # melt matrix\n  melted = reshape2::melt(cor)\n    # one value is > 1 which can happen sometimes\n    melted$value[melted$value > 1.01] <- 1\n    # get rounded value\n  melted$value_round = round(melted$value, digit = 2)\n  melted$distance0 = abs(melted$value)\n\n    # for some reason one corr over 1 doesn't get colored\n    melted$value <- ifelse(melted$value > 1, 1, melted$value)\n\n    # pull out standard errors\n    # SEs will be listed in the same order as they are listed in the genetic covariance matrix  \n    r<-nrow(LDSCoutput$S)\n    SE_Stand<-matrix(0, r, r)\n    SE_Stand[lower.tri(SE_Stand,diag=TRUE)] <-sqrt(diag(LDSCoutput$V_Stand))\n    dimnames(SE_Stand)<- dimnames(LDSCoutput$S)\n\n    # re-roder\n    SE_Stand = SE_Stand[c(\"ALZ\",\"ALZ_2019\",\"PD\",\"ALS\",\"ICV_Adams.et.al\",\"TBV_Zhao.et.al\", \"TBV_Smith.et.al\", \"PC1_whole_brain_greymatter\",\"brainage\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"diff_stand\", \"ratio_stand\", \"resid_stand\"), c(\"ALZ\",\"ALZ_2019\",\"PD\",\"ALS\",\"ICV_Adams.et.al\", \"TBV_Zhao.et.al\", \"TBV_Smith.et.al\", \"PC1_whole_brain_greymatter\",\"brainage\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"diff_stand\", \"ratio_stand\", \"resid_stand\")]\n\n    # re-name\n    dimnames(SE_Stand)[[1]] <-c(\"Alzheimer's & related dementias\\n(Bellenguez et al., 2021)\", \"Alzheimer's\\n(Kunkle et al., 2019)\", \"Parkinson's\\n(Nalls et al., 2019)\", \"Amyotrophic lateral sclerosis\\n(van Rheenen et al. 2021)\", \"ICV\\n(Adams et al., 2016)\", \"TBV\\n(Zhao et al., 2018)\", \"TBV\\n(Smith et al., 2021)\", \"Grey matter whole brain PC1\\n(Furtjes et al., 2023)\", \"Brain age\\n(Kaufmann et al., 2019)\",\"ICV\", \"TBV\", \"CSF\", \"Difference score\\n(cross)\", \"Ratio score\\n(cross)\", \"Residual score\\n(cross)\")\n    dimnames(SE_Stand)[[2]] <-c(\"Alzheimer's & related dementias\\n(Bellenguez et al., 2021)\", \"Alzheimer's\\n(Kunkle et al., 2019)\", \"Parkinson's\\n(Nalls et al., 2019)\", \"Amyotrophic lateral sclerosis\\n(van Rheenen et al. 2021)\", \"ICV\\n(Adams et al., 2016)\", \"TBV\\n(Zhao et al., 2018)\", \"TBV\\n(Smith et al., 2021)\", \"Grey matter whole brain PC1\\n(Furtjes et al., 2023)\", \"Brain age\\n(Kaufmann et al., 2019)\",\"ICV\", \"TBV\", \"CSF\", \"Difference score\\n(cross)\", \"Ratio score\\n(cross)\", \"Residual score\\n(cross)\")\n    \n\n    #melt data\n    SEmelted <- reshape2::melt(SE_Stand)\n    names(SEmelted)[grepl(\"value\", names(SEmelted))] <- \"SE\"\n    # merge with corr data\n    both = merge(melted, SEmelted, by = c(\"Var1\", \"Var2\"), all.x = T)\n    # work out confidence intervals\n    both$ci_u <- both$value + (1.96*both$SE)\n    both$ci_l <- both$value - (1.96*both$SE)\n\n    # get sig assocs\n    both$sig <- \"\"\n    both$sig[which(both$ci_u > 0 & both$ci_l < 0)] <- \"\\n(ns.)\"\n    both$sig[which(both$ci_u < 0 & both$ci_l > 0)] <- \"\\n(ns.)\"\n    both$sig[is.na(both$value)] <- NA\n    both$value_round <- paste0(both$value_round, \" \", both$sig)\n    both$value_round[is.na(both$value)] <- NA\n    #both$value_round[grep(\"NA\", both$value_round)] <- str_remove(both$value_round[grep(\"NA\", both$value_round)], pattern = \" NA\")\n\n  # plot\n  library(ggplot2)\n\n  p = ggplot()+\n    geom_point(data = both, aes(x = Var1, y = Var2, shape = value, fill = value, size = distance0), shape = 21, alpha = 0.7, colour = \"white\") +\n    scale_fill_gradient2(low = \"grey45\", mid = \"white\", high = \"grey45\",limit = c(-1,1), space = \"Lab\" ,name=\"Correlation\", guide = \"legend\")+\n    scale_size_continuous(range = c(1, 15), guide = \"none\")+\n    geom_text(data = both, aes(Var1, Var2, label = value_round), color = \"black\", size = 3)+\n    geom_rect(aes(xmax = stage(\"Residual score\\n(cross)\", after_scale(xmax+0.5)), \n                    xmin = stage(\"Residual score\\n(cross)\", after_scale(xmin-0.5)), \n                    ymax = stage(\"Residual score\\n(cross)\", after_scale(ymax+0.5)), \n                    ymin = stage(\"Alzheimer's & related dementias\\n(Bellenguez et al., 2021)\", after_scale(ymin-0.5))), fill = \"#004D40\", alpha = 0.3)+\n    geom_rect(aes(xmax = stage(\"Ratio score\\n(cross)\", after_scale(xmax+0.5)), \n                    xmin = stage(\"Ratio score\\n(cross)\", after_scale(xmin-0.5)), \n                    ymax = stage(\"Ratio score\\n(cross)\", after_scale(ymax+0.5)), \n                    ymin = stage(\"Alzheimer's & related dementias\\n(Bellenguez et al., 2021)\", after_scale(ymin-0.5))), fill = \"#FFC107\", alpha = 0.3)+\n    geom_rect(aes(xmax = stage(\"Difference score\\n(cross)\", after_scale(xmax+0.5)), \n                    xmin = stage(\"Difference score\\n(cross)\", after_scale(xmin-0.5)), \n                    ymax = stage(\"Difference score\\n(cross)\", after_scale(ymax+0.5)), \n                    ymin = stage(\"Alzheimer's & related dementias\\n(Bellenguez et al., 2021)\", after_scale(ymin-0.5))), fill = \"#D81B60\", alpha = 0.3)+\n    xlab(\"\")+\n    ylab(\"\")+\n    #scale_x_discrete(labels = axisNames)+\n    #scale_y_discrete(labels = axisNames)+\n    guides(fill = \"none\")+\n    theme_bw()+\n    theme(panel.border = element_blank(),\n            axis.text.x = element_text(angle = 270, vjust = 0, hjust =0))+\n    ggtitle(\"Genetic correlations\")\n\nggsave(\"genetic_corr_neurodeg.png\", plot = p, width = 22, height = 22, units = \"cm\", dpi = 600)\n\n\n\n\n\nPlot sex-split correlations\n\n\nCode\n###########################################################################\n#### Genetic correlation matrix\n#### Sex split\n###########################################################################\nload(\"LDSCoutput_sexsplit.RData\")\n\n# pull out correlation matrix\ncormat <- LDSCoutput$S_Stand\n\n# re-roder\ncormat = cormat[c(\"ICVstand\",\"TBVstand\",\"CSFstand\",\"diff_stand\", \"ratio_stand\", \"resid_stand\", \"diff_stand_males\", \"ratio_stand_males\",\"resid_stand_males\",\"diff_stand_females\", \"ratio_stand_females\", \"resid_stand_females\"), c(\"ICVstand\",\"TBVstand\",\"CSFstand\",\"diff_stand\", \"ratio_stand\", \"resid_stand\", \"diff_stand_males\", \"ratio_stand_males\",\"resid_stand_males\",\"diff_stand_females\", \"ratio_stand_females\", \"resid_stand_females\")]\n\n# re-name\ndimnames(cormat)[[1]] <-c(\"ICV (all)\",\"TBV (all)\",\"CSF (all)\",\"Difference Score (all)\", \"Ratio score (all)\", \"Residual score (all)\", \"Difference Score (males)\", \"Ratio score (males)\",\"Residual score (males)\",\"Difference Score (females)\", \"Ratio score (females)\", \"Residual score (females)\")\ndimnames(cormat)[[2]] <-c(\"ICV (all)\",\"TBV (all)\",\"CSF (all)\",\"Difference Score (all)\", \"Ratio score (all)\", \"Residual score (all)\", \"Difference Score (males)\", \"Ratio score (males)\",\"Residual score (males)\",\"Difference Score (females)\", \"Ratio score (females)\", \"Residual score (females)\")\n\n# define function to obtain lower triangle\n  get_lower_tri<-function(cormat){\n    cormat[upper.tri(cormat)] <- NA\n    return(cormat)\n  }\n\n  # get correlation matrix for both samples together\n  cor = get_lower_tri(cormat)\n  # melt matrix\n  melted = reshape2::melt(cor)\n    # one value is > 1 which can happen sometimes\n    melted$value[melted$value > 1.01] <- 1\n    # get rounded value\n  melted$value_round = round(melted$value, digit = 2)\n  melted$distance0 = abs(melted$value)\n\n    # for some reason one corr over 1 doesn't get colored\n    melted$value <- ifelse(melted$value > 1, 1, melted$value)\n\n    # pull out standard errors\n    # SEs will be listed in the same order as they are listed in the genetic covariance matrix  \n    r<-nrow(LDSCoutput$S)\n    SE_Stand<-matrix(0, r, r)\n    SE_Stand[lower.tri(SE_Stand,diag=TRUE)] <-sqrt(diag(LDSCoutput$V_Stand))\n    dimnames(SE_Stand)<- dimnames(LDSCoutput$S)\n\n    # re-roder\n    SE_Stand = SE_Stand[c(\"ICVstand\",\"TBVstand\",\"CSFstand\",\"diff_stand\", \"ratio_stand\", \"resid_stand\", \"diff_stand_males\", \"ratio_stand_males\",\"resid_stand_males\",\"diff_stand_females\", \"ratio_stand_females\", \"resid_stand_females\") ,c(\"ICVstand\",\"TBVstand\",\"CSFstand\",\"diff_stand\", \"ratio_stand\", \"resid_stand\", \"diff_stand_males\", \"ratio_stand_males\",\"resid_stand_males\",\"diff_stand_females\", \"ratio_stand_females\", \"resid_stand_females\")]\n\n    # re-name\n    dimnames(SE_Stand)[[1]] <-c(\"ICV (all)\",\"TBV (all)\",\"CSF (all)\",\"Difference Score (all)\", \"Ratio score (all)\", \"Residual score (all)\", \"Difference Score (males)\", \"Ratio score (males)\",\"Residual score (males)\",\"Difference Score (females)\", \"Ratio score (females)\", \"Residual score (females)\")\n    dimnames(SE_Stand)[[2]] <-c(\"ICV (all)\",\"TBV (all)\",\"CSF (all)\",\"Difference Score (all)\", \"Ratio score (all)\", \"Residual score (all)\", \"Difference Score (males)\", \"Ratio score (males)\",\"Residual score (males)\",\"Difference Score (females)\", \"Ratio score (females)\", \"Residual score (females)\")\n    \n\n    #melt data\n    SEmelted <- reshape2::melt(SE_Stand)\n    names(SEmelted)[grepl(\"value\", names(SEmelted))] <- \"SE\"\n    # merge with corr data\n    both = merge(melted, SEmelted, by = c(\"Var1\", \"Var2\"), all.x = T)\n    # work out confidence intervals\n    both$ci_u <- both$value + (1.96*both$SE)\n    both$ci_l <- both$value - (1.96*both$SE)\n\n    # get sig assocs\n    both$sig <- \"\"\n    both$sig[which(both$ci_u > 0 & both$ci_l < 0)] <- \"\\n(ns.)\"\n    both$sig[which(both$ci_u < 0 & both$ci_l > 0)] <- \"\\n(ns.)\"\n    both$sig[is.na(both$value)] <- NA\n    both$value_round <- paste0(both$value_round, \" \", both$sig)\n    both$value_round[is.na(both$value)] <- NA\n    #both$value_round[grep(\"NA\", both$value_round)] <- str_remove(both$value_round[grep(\"NA\", both$value_round)], pattern = \" NA\")\n\n  # plot\n  library(ggplot2)\n\n  p = ggplot()+\n    geom_point(data = both, aes(x = Var1, y = Var2, shape = value, fill = value, size = distance0), shape = 21, alpha = 0.7, colour = \"white\") +\n    scale_fill_gradient2(low = \"#91D2D9\", mid = \"white\", high = \"#FFC4C4\",limit = c(-1,1), space = \"Lab\" ,name=\"Correlation\", guide = \"legend\")+\n    scale_size_continuous(range = c(1, 15), guide = \"none\")+\n    geom_text(data = both, aes(Var1, Var2, label = value_round), color = \"black\", size = 3)+\n    xlab(\"\")+\n    ylab(\"\")+\n    #scale_x_discrete(labels = axisNames)+\n    #scale_y_discrete(labels = axisNames)+\n    guides(fill = \"none\")+\n    theme_bw()+\n    theme(panel.border = element_blank(),\n            axis.text.x = element_text(angle = 270, vjust = 0, hjust =0))+\n    ggtitle(\"Genetic correlations\")\n\nggsave(\"genetic_corr_sexsplit.png\", plot = p, width = 20, height = 20, units = \"cm\", dpi = 600)"
  },
  {
    "objectID": "rg.html#gwas-by-subtraction-in-genomicsem",
    "href": "rg.html#gwas-by-subtraction-in-genomicsem",
    "title": "Genetic correlations",
    "section": "GWAS-by-subtraction in GenomicSEM",
    "text": "GWAS-by-subtraction in GenomicSEM\n\n\nCode\nsetwd(wd)\nlibrary(devtools)\nlibrary(GenomicSEM)\n\n#### Step 2\ntraits <- c(\"TBVstand.sumstats.gz\",\"ICVstand.sumstats.gz\",\"resid_stand.sumstats.gz\")\nsample.prev <- rep(NA, length(traits))\npopulation.prev <- rep(NA, length(traits))\nld<-\"eur_w_ld_chr/\"\nwld <- \"eur_w_ld_chr/\"\ntrait.names<-c(\"TBV\", \"ICV\", \"resid\")\n\nLDSCoutput <- ldsc(traits = traits, \n                   sample.prev = sample.prev, \n                   population.prev = population.prev, \n                   ld=ld, \n                   wld=wld, \n                   trait.names=trait.names,\n                    stand = T)\n\n\nsave(LDSCoutput, file=\"LDSCoutput_TBV_ICV.RData\")\n\n###### Step 3\nload(file=paste0(wd, \"/LDSCoutput_TBV_ICV.RData\"))\n\n# BaseICV is captured by both diff atrophy score and ICV\n# Atrophy however should only be indicated by diff atrophy score and should be independent of ICV\nmodel<-'\n        BaseICV=~NA*TBV + ICV\n        Atrophy=~NA*TBV\n         \n        # make sure all variances and covariances are captured by the latent factors\n         Atrophy ~~ 1*Atrophy\n         BaseICV ~~ 1*BaseICV\n        # forcing atrophy and BaseICV to be uncorrelated\n         Atrophy~~0*BaseICV\n\n         ICV ~~ 0*TBV\n         ICV ~~ 0*ICV\n         TBV ~~ 0*TBV\n\n        # add in resid to how correlated it is with Atrophy\n        Atrophy ~~ resid\n        BaseICV ~~ resid\n'\n\n# this is a fully saturated model (which is why no fit statistics are printed)\noutput<-usermodel(LDSCoutput,estimation=\"DWLS\",model=model)\n\n## Step 4\n# files to prepare\nfiles = c(paste0(GWASwd, \"/GWAS_brainAtrophy_TBVstand_N43110.gz\"), \n        paste0(GWASwd, \"/GWAS_brainAtrophy_ICVstand_N43110.gz\"))\n\nfile.exists(files)\n\nref = \"reference.1000G.maf.0.005.txt.gz\"\ntrait.names = c(\"TBV\",\"ICV\")\n# SEs are not on a logistic scale\nse.logit = c(F,F)\ninfo.filter = 0.6\nmaf.filter = 0.01\n#betas = c(\"BETA\",\"BETA\")\n\nsumstats <- sumstats(files = files, \n                        ref = ref, \n                        trait.names = trait.names, \n                        se.logit = se.logit, \n                        info.filter = info.filter, \n                        maf.filter = maf.filter, \n                        #betas = betas,\n                        OLS=c(T,T),\n                        linprob=NULL,N=c(43110,43110), \n                        parallel = F)\n\nsave(sumstats, file=\"Sumstats_TBV_ICV.RData\")\n\nfile.exists(\"Sumstats_TBV_ICV.RData\")\n\n\n\nSNP-level analysis\nThe script below makes it easier to parallelise SNP-level GWAS-by-subtraction analyses in GenomicSEM (make sure you change the model in the script when using this). Run this with: Rscript GWASbySubtractionPARALLEL.R --chr 3 --RData Sumstats_TBV_ICV.RData --LDSCout LDSCoutput_TBV_ICV.RData --outPath GWASbySubtraction\n\n\nCode\n#!/usr/bin/Rscript\n# Run the 5th step of GWAS-by-subtraction to parallelise each chromosome \n\nif (!require(\"optparse\")) install.packages(\"optparse\", repos=\"https://cran.rstudio.com/\")\nsuppressMessages(library(\"optparse\"))\n\n\n# SET OPTIONS\noption_list = list(\n  make_option(\"--chr\", action = \"store\", default = NA, type = \"character\",\n              help = \"Chrosomome to be processed [required]\"),\n    make_option(\"--trait1\", action = \"store\", default = NA, type = \"character\",\n              help = \"Trait 1 [required]\"),\n    make_option(\"--trait2\", action = \"store\", default = NA, type = \"character\",\n              help = \"Trait 2 [required]\"),\n  make_option(\"--RData\", action = \"store\", default = NA, type = \"character\",\n              help = \"RData containing cleaned sumstats [required]\"),\n    make_option(\"--LDSCout\", action = \"store\", default = NA, type = \"character\",\n              help = \"LDSC output [required]\"),\n    make_option(\"--outPath\", action = \"store\", default = NA, type = \"character\",\n              help = \"Output file name [required]\")\n)\n\nopt = parse_args(OptionParser(option_list=option_list))\n\nlibrary(devtools)\nlibrary(GenomicSEM)\n\n# FEEDBACK ON OPTIONS\nprint(Sys.time())\nprint(paste0(\"GWAS-by-subtraction CHR = \", opt$chr))\n\nprint(opt$RData)\nprint(paste0(opt$outPath,\"/Atrophy_GWASbySubtraction\", opt$chr,\".RData\"))\n\n## Step 5\n# load LDSC output\nload(file=opt$LDSCout)\n\n# load in RData file with cleaned SNP data\nfile = opt$RData\nload(file)\n# keep only one chromosome\nsumstats = sumstats[which(sumstats$CHR == opt$chr),]\n\nprint(table(sumstats$CHR))\n\n# SNP model\nmodelSNP<-'\n        BaseICV=~NA*TBV + start(0.8)*ICV\n        Atrophy=~NA*TBV\n         \n        # make sure all variances and covariances are captured by the latent factors\n         Atrophy ~~ 1*Atrophy\n         BaseICV ~~ 1*BaseICV\n         ICV ~~ 0*TBV\n         ICV ~~ 0*ICV\n         TBV ~~ 0*TBV\n\n        # forcing atrophy and BaseICV to be uncorrelated\n         Atrophy~~0*BaseICV\n\n        # add in SNP effect\n        Atrophy ~ SNP\n        BaseICV ~ SNP\n        SNP ~~ SNP\n'\n\n\n\n#Run the Genomic SEM GWAS\noutputGWAS<-userGWAS(covstruc=LDSCoutput,\n                    SNPs=sumstats,\n                    estimation=\"ML\",\n                    sub =c(\"Atrophy ~ SNP\"),\n                    Q_SNP = T,\n                    smooth_check = T, \n                    parallel = T,\n                    GC = \"standard\",\n                    MPI = F,\n                    fix_measurement = T,\n                    model=modelSNP,\n                    cores = 5,\n                    toler = F, \n                    SNPSE = F) # printwarn = FALSE\n\nsave(outputGWAS, file=paste0(opt$outPath,\"/Atrophy_GWASbySubtraction_\", opt$chr,\".RData\"))\n\n\n##### Run with \n# Rscript GWASbySubtractionPARALLEL.R --chr 3 --RData Sumstats_TBV_ICV.RData --LDSCout LDSCoutput_TBV_ICV.RData --outPath GWASbySubtraction"
  },
  {
    "objectID": "STRADL.html",
    "href": "STRADL.html",
    "title": "Generation Scotland Subsample (STRADL)",
    "section": "",
    "text": "Load packages\n\n\nCode\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n\nThe STRADL data was kindly provided by Joanna Moodie where TBV and ICV had already been extracted.\n\n\nExtract lifetime brain atrophy variables\n\n\nCode\n# add in STRADL\nSTRADL = fread(paste0(STRADLdir, \"/\", list.files(path = STRADLdir, pattern = \"STRADL\")))\n\nnames(STRADL) = c(\"ID\", \"Age\", \"Sex\", \"TBV\", \"ICV\")\n\n# convert mm3 estimates to more intuitive cm3 estimates\nSTRADL$ICV = STRADL$ICV/1000\nSTRADL$TBV = STRADL$TBV/1000\n\n# estimate brain atrophy from single MRI scan\nSTRADL$diff = STRADL$ICV - STRADL$TBV\nSTRADL$ratio = STRADL$TBV / STRADL$ICV\n\n# remove participants with zero estimates for TBV and ICV (11 participants)\nSTRADL = STRADL[STRADL$TBV != 0,]\nSTRADL = STRADL[STRADL$ICV != 0,]\n\n\n# remove participants where ICV is smaller than TBV (excluding 45 participants)\nSTRADL = STRADL[which(STRADL$diff > 0),]\n\nmodel <- lm(TBV ~ ICV, data = STRADL)\nSTRADL$resid = resid(model)\n\n# standardise variables\nSTRADL$diff_stand = as.vector(scale(STRADL$diff))\nSTRADL$ratio_stand = as.vector(scale(STRADL$ratio))\nSTRADL$resid_stand = as.vector(scale(STRADL$resid))\n\n\n\n\nPlot distributions colored by age\nShown in Supplementary Figure 3: Distributions of TBV, ICV, and lifetime brain atrophy estimated with the residual, ratio, and difference method. Histograms are coloured by age groups.\n\n\nCode\n####################################################\n# make age groups\nSTRADL$Age_group <- NA\nSTRADL$Age_group[STRADL$Age < 30] <- \"29 years and under\"\nSTRADL$Age_group[STRADL$Age >= 30 & STRADL$Age < 40] <- \"30 - 39\"\nSTRADL$Age_group[STRADL$Age >= 40 & STRADL$Age < 50] <- \"40 - 49\"\nSTRADL$Age_group[STRADL$Age >= 50 & STRADL$Age < 60] <- \"50 - 59\"\nSTRADL$Age_group[STRADL$Age >= 60 & STRADL$Age < 70] <- \"60 - 69\"\nSTRADL$Age_group[STRADL$Age >= 70 & STRADL$Age < 80] <- \"70 - 79\"\nSTRADL$Age_group[STRADL$Age >= 80] <- \"80 years and over\"\n\np1=ggplot(STRADL, aes(x=TBV, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"TBV\")+\n  theme_bw()\n\np2=ggplot(STRADL, aes(x=ICV, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"ICV\")+\n  theme_bw()\n\np3=ggplot(STRADL, aes(x=resid_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Residual score\")+\n  theme_bw()\n\np4=ggplot(STRADL, aes(x=ratio_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Ratio score\")+\n  theme_bw()\n\np5=ggplot(STRADL, aes(x=diff_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Difference score\")+\n  theme_bw()\n\npSTRADL <- ggarrange(p1,p2,p3,p4,p5, nrow = 1, common.legend = T, legend = \"bottom\")\n\n# add title\npSTRADL <- annotate_figure(pSTRADL, top = text_grob(\"STRADL\",face = \"bold\", size = 14))\n\n#ggsave(paste0(out,\"phenotypic/STRADL_disttributions.jpg\"), bg = \"white\",plot = pSTRADL, width = 30, height = 10, units = \"cm\", dpi = 300)\npSTRADL"
  },
  {
    "objectID": "UKB_geneticQC.html#initial-sample-level-quality-control-qc",
    "href": "UKB_geneticQC.html#initial-sample-level-quality-control-qc",
    "title": "UK Biobank: Genetic data cleaning",
    "section": "Initial sample-level quality control (QC)",
    "text": "Initial sample-level quality control (QC)\n\n\nCode\n# Initial sample QC (R code)\n## 1. Do 4 means clustering to get europeans only\n## 2. Get variable with het & miss info\n## 3. Generate a keep list \n\n# load packages\nlibrary(data.table)\nlibrary(ggplot2)\n\n# get all participant IDs that have neuroimaging data (this is the phenotypic data prepared as GWAS input)\nneuro = fread(\"UKB_CrossNeuroIDP.txt\", select = 1:2)\n\n# get PC1:40 data using ukbtools package\nukb_sqc=fread(\"UKBB_500K/Decrypted/ukb_sqc_v2.txt\")\n\n# https://kenhanscombe.github.io/ukbtools/articles/explore-ukb-data.html\n# With ukb_sqc_v2.txt read into the dataframe my_sqc_data\nukb_sqc <- ukb_gen_sqc_names(ukb_sqc)\n\n# genetic file is organised in the same order as application-specific ref fam file\nref = fread(\"UKBB_500K/Decrypted/ukb1027_cal_chr1_v2_s488374.fam\")\n\n# merge ref file to get IDs\nukb_sqc = cbind(ref[,c(\"V1\")], ukb_sqc)\n# re-name ID column that comes from the fam file\nnames(ukb_sqc)[which(names(ukb_sqc) == \"V1\")] = \"IID\"\n\n###############################\n# Get variable with het & miss info\n###############################\ntable(ukb_sqc$het_missing_outliers)\n\nprint(paste(\"We lose \",nrow(neuro)-sum(neuro$IID %in% ukb_sqc$IID), \" participants from the neuroimaging data set because these IDs are not available in the genetic data. (N = \", sum(neuro$IID %in% ukb_sqc$IID),\")\"))\n#[1] \"We lose  1220  participants from the neuroimaging data set because these IDs are not available in the genetic data. (N =  45598 )\"\n\nneuro = merge(neuro, ukb_sqc[,c(\"IID\",\"het_missing_outliers\")], by = \"IID\")\n\nprint(paste(\"We lose \", sum(neuro$het_missing_outliers == 1),\" participants because they were outliers in heterozygosity and missinggness. N = \", sum(neuro$het_missing_outliers == 0), \"\"))\n#[1] \"We lose  83  participants because they were outliers in heterozygosity and missinggness. N =  45515 \"\n\n# exclude particpants that were labeled as outliers by UKB core team\nneuro = neuro[which(neuro$het_missing_outlier == 0),]\n\n################################\n# do 4 means clustering\n################################\n##Read in PCs, remove NAs, rename\n#PCs<-fread(args[1], data.table=F)\ncolNames = c(\"V1\", paste0(\"pc\",1:40))\nPCs <- ukb_sqc[, ..colNames]\nPCs<-na.omit(PCs)\nnames(PCs)<-c(\"ID\",\"PC.1\",\"PC.2\",\"PC.3\",\"PC.4\",\"PC.5\",\"PC.6\",\"PC.7\",\"PC.8\",\"PC.9\",\"PC.10\",\"PC.11\",\"PC.12\",\"PC.13\",\"PC.14\",\"PC.15\",\"PC.16\",\"PC.17\",\"PC.18\",\"PC.19\",\"PC.20\",\"PC.21\",\"PC.22\",\"PC.23\",\"PC.24\",\"PC.25\",\"PC.26\",\"PC.27\",\"PC.28\",\"PC.29\",\"PC.30\",\"PC.31\",\"PC.32\",\"PC.33\",\"PC.34\",\"PC.35\",\"PC.36\",\"PC.37\",\"PC.38\",\"PC.39\",\"PC.40\")\n\n##Set seed\nset.seed(1204688)\n\n##K means clustering on each PC\nK_MEAN <- 4\n\nPC1_K<-kmeans(PCs$PC.1, K_MEAN)\nPC2_K<-kmeans(PCs$PC.2, K_MEAN)\n\n##Add clusters to PC dataframe\nPCs$PC1.Cluster<-PC1_K$cluster\nPCs$PC2.Cluster<-PC2_K$cluster\nPCs$Clusters<-as.factor(paste(PC1_K$cluster,PC2_K$cluster,sep=\".\"))\n\n##WWE group is the majority\n\nMAX_PC1<-ifelse(match(max(table(PCs$PC1.Cluster, PCs$PC2.Cluster)), table(PCs$PC1.Cluster, PCs$PC2.Cluster)) %% K_MEAN == 0, K_MEAN, match(max(table(PCs$PC1.Cluster, PCs$PC2.Cluster)), table(PCs$PC1.Cluster, PCs$PC2.Cluster)) %% K_MEAN)\n\nMAX_PC2<-ceiling(match(max(table(PCs$PC1.Cluster, PCs$PC2.Cluster)), table(PCs$PC1.Cluster, PCs$PC2.Cluster))/K_MEAN)\n\n##Make lists of WWE IDs\nWWE<-as.data.frame(PCs[PCs$PC1.Cluster == MAX_PC1 & PCs$PC2.Cluster == MAX_PC2,1])\nnames(WWE)<-\"ID\"\n\n## Get overlap with participants of interest\nWWE = WWE[WWE$ID %in% neuro$FID,]\nprint(paste(\"We excluded \", nrow(neuro)-length(WWE),\"participants based on 4-means clustering. N = \", length(WWE), \"\"))\n#[1] \"We excluded  1332 participants based on 4-means clustering. N =  44183 \"\n\nWWE_PLINK<-as.data.frame(cbind(WWE,WWE))\nnames(WWE_PLINK)<-c(\"FID\",\"IID\")\n\n##Write to file\nwrite.table(WWE_PLINK, file=paste0(out,\"/ukb_neuroimaging_4MeansClustering_excl_het_miss.txt\"), row.names=F, col.names=T, quote = F)\n\n##Plot\npdf(paste0(out,\"/ukb_neuroimaging_4MeansClustering\", \".pdf\"))\nwith(PCs, print(qplot(PC.1, PC.2, colour=Clusters)))\ndev.off()\n\n\n#############################\n### check how many of those participants now have self-reported different ancestry\n#############################\n# this function identifies the path to the most recent download file on our server\ngetFieldLoc = function(path = path, fileName = fileName, fieldID = fieldID){\n  library(stringr)\n  # read all the field.ukb files\n  files_to_read = list.files(\n    path = path,\n    pattern = fileName,\n    recursive = T,\n    full.names = T\n  )\n  # read all files\n  dat = lapply(files_to_read, fread)\n  names(dat) = files_to_read\n  \n  # search for field ID of interest \n  candidates = names(dat)[grep(fieldID, dat)]\n  # figure out which one is from the most recent file (i.e., highest number)\n  candidates = str_remove(candidates, paste0(path, \"/\"))\n  candidates = str_remove(candidates, paste0(\"/\", fileName))\n  candidates = unique(as.numeric(sapply(str_extract_all(candidates, \"\\\\d+\"), tail , 1)))\n  most_recent = max(candidates, na.rm =T)\n  \n  return(paste0(path, most_recent))\n}\n\npath1 = getFieldLoc(path = path, \n            fileName = \"fields.ukb\", \n            fieldID = 21000)\n\nfileID = list.files(pat=path1,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path1, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.21000\", names(file))\nfile = file[, c(..id, ..Cols)]\nnames(file)[which(names(file) == \"f.eid\")] = \"IID\"\n\n# merge file with neuro but only keep the IDs available in neuro\nneuro1 = merge(neuro, file, by = \"IID\", all.x=T)\n\n# make a table for the remaining particpants to see what ethnicity they reported\ntable(neuro1$f.21000_0.0)\n\n# make variable that encodes European ancestry\nneuro1$European <- ifelse(neuro1$f.21000_0.0 == 1 | neuro1$f.21000_0.0 == 1001 | neuro1$f.21000_0.0 == 1002 | neuro1$f.21000_0.0 == 1003,1,0)\n\nprint(paste(\"We're losing \", sum(neuro1$European == 0, na.rm=T),\" participants because they self-reported to be non-European.\",sum(is.na(neuro1$European)),\"are missing this info and will also be excluded. N remaining = \", sum(neuro1$European ==1, na.rm=T)))\n#[1] \"We're losing  1470  participants because they self-reported to be non-European. 9 are missing this info and will also be excluded. N remaining =  44036\"\n\n# delete non-European\nneuro1 = neuro1[which(neuro1$European == 1),]\n\nwrite.table(neuro1, file=paste0(out,\"/ukb_neuroimaging_4MeansClustering_excl_het_missEUR.txt\"), row.names=F, col.names=T, quote = F)\n\n##### At this point, we now have a list of IDs saved in ukb_neuroimaging_4MeansClustering_excl_het_miss.txt that are:\n# People with neuroimaging data\n# Have genetic data\n# Have not been labeled unusual by the UKB core team for missingness and heterozygosity\n# Have been included based on 4 means clustering\n# Have self-reported that they are of European descent"
  },
  {
    "objectID": "UKB_geneticQC.html#format-genetic-data",
    "href": "UKB_geneticQC.html#format-genetic-data",
    "title": "UK Biobank: Genetic data cleaning",
    "section": "Format genetic data",
    "text": "Format genetic data\n\n\nCode\n##########################################################################################################\n# Step 0: Only include participants with neuroimaging data \n## this set of individuals will already exclude non-Europeans (PCs and self-report)\n## extreme scores on hetezygosity and missingness (see above)\n\n# copy bim bed and fam files to get consistent naming across bim and bed files\n## bed files\nfor CHR in {1..22}\ndo\ncp ${sourceBED}/ukb_cal_chr${CHR}_v2.bed ${target}/ukb_chr${CHR}_v2.bed\ndone\n## bim files\nfor CHR in {1..22}\ndo\ncp ${sourceBIM}/ukb_snp_chr${CHR}_v2.bim ${target}/ukb_chr${CHR}_v2.bim\ndone\n## fam file\ncp /GWAS_Source/UB_BB/UKBB_500K/Decrypted/ukb1027_cal_chr1_v2_s488374.fam ${target}/ukb_v2.fam\n\n# do fam file for each chromosome\nfor CHR in {1..22}\ndo\ncp ${target}/ukb_v2.fam ${target}/ukb_chr${CHR}_v2.fam\ndone\n\n## put data from each chromosome together into one bim, bed and fam file\n## and filter participants of interest\ncd $target\nphenoIDs=\"/CCACE_Shared/Anna_F/BrainAtrophy/data/geneticQC/ukb_neuroimaging_4MeansClustering_excl_het_missEUR.txt\"\n\nplink19 \\\n--merge-list $target/allGenoFiles.txt \\\n--make-bed \\\n--keep $phenoIDs \\\n--out ukb_neuroimaging_reQC\n\n## starting out with 784256 variants and 488377 participants, that are getting cut down to the 44036 participants that survived cleaning above\n\n#################################################\n# I realised later that this is not including the sex-chromosomes which are needed for sex-check later (this merging back and forth could have been done in one step, but I am saving resources to not to it from scratch again)\n# Copy X, Y and XY chromosome info first\n## bed files\nfor CHR in X XY Y\ndo\ncp ${sourceBED}/ukb_cal_chr${CHR}_v2.bed ${target}/ukb_chr${CHR}_v2.bed\ndone\n## bim files\nfor CHR in X XY Y\ndo\ncp ${sourceBIM}/ukb_snp_chr${CHR}_v2.bim ${target}/ukb_chr${CHR}_v2.bim\ndone\n## fam file for each chromosome\nfor CHR in X XY Y\ndo\ncp ${target}/ukb_v2.fam ${target}/ukb_chr${CHR}_v2.fam\ndone\n\n## merge X, Y, XY files together\nplink19 \\\n--merge-list ${target}/allXYfiles.txt \\\n--make-bed \\\n--keep $phenoIDs \\\n--out ukb_neuroimaging_sexchrom\n\n# now merge sex chromosomes with other data\nplink19 \\\n--merge-list ${target}/allAuto_and_Sex.txt \\\n--make-bed \\\n--keep $phenoIDs \\\n--out ukb_neuroimaging_autosomal_sex_preQC\n\n## now altogether, we have 44036 partcicipants left, and 805161 variants"
  },
  {
    "objectID": "UKB_geneticQC.html#filter-for-missing-genotype-data-and-minor-allele-frequency",
    "href": "UKB_geneticQC.html#filter-for-missing-genotype-data-and-minor-allele-frequency",
    "title": "UK Biobank: Genetic data cleaning",
    "section": "Filter for missing genotype data and minor allele frequency",
    "text": "Filter for missing genotype data and minor allele frequency\n\n\nCode\n# Step 1: Get SNP-list based on geno filter\nplink19 \\\n--bed ${target}/ukb_neuroimaging_autosomal_sex_preQC.bed \\\n--bim ${target}/ukb_neuroimaging_autosomal_sex_preQC.bim \\\n--fam ${target}/ukb_neuroimaging_autosomal_sex_preQC.fam \\\n--geno 0.02 \\\n--write-snplist \\\n--out ${target}/ukb_neuroimaging_GENO0.02\n# removed 104462 variants, 700699 remaining\n# no participants removed (44036 remaining)\n\n## this creates a snplist that feeds into next step\n\n# Step 2: apply various cleaning steps to get ID list (.fam) to feed into relatedness analysis\nplink19 \\\n--bed ${target}/ukb_neuroimaging_autosomal_sex_preQC.bed \\\n--bim ${target}/ukb_neuroimaging_autosomal_sex_preQC.bim \\\n--fam ${target}/ukb_neuroimaging_autosomal_sex_preQC.fam \\\n--geno 0.02 \\\n--extract ukb_neuroimaging_GENO0.02.snplist \\\n--write-snplist \\\n--make-just-fam \\\n--freq --maf 0.01 \\\n--out ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR\n\n# this fam file still contains 44036 participants\n# MAF filter removed 103158 variants, 597541 remaining"
  },
  {
    "objectID": "UKB_geneticQC.html#remove-related-individuals",
    "href": "UKB_geneticQC.html#remove-related-individuals",
    "title": "UK Biobank: Genetic data cleaning",
    "section": "Remove related individuals",
    "text": "Remove related individuals\nOur server was incompatible with the greedyRelated software, which is why I use R package here instead that runs the same analysis.\n\n\nCode\nlibrary(ukbtools)\nlibrary(data.table)\n# read in ukb relatedness file\nrel = fread(\"/UKBB_500K/Decrypted/ukb1027_rel_s488374.dat\")\nneuro = fread(\"ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR.fam\")\nsum(neuro$V1 %in% rel$ID1)\n# 7509\nsum(neuro$V1 %in% rel$ID2)\n# 7748\n\n# identify degree of relatedness \nukb_gen_rel_count(rel)\nukb_gen_rel_count(rel, plot = TRUE)\n\n# generate a list of participants to exclude based on relatedness\n# default cut-off > 0.0884 King coefficient corresponding to 3rd degree relatedness\nIDtoRemove = ukb_gen_samples_to_remove(rel, ukb_with_data = neuro$V1)\n\n# get PLINK format\nIDtoRemove<-as.data.frame(cbind(IDtoRemove,IDtoRemove))\nnames(IDtoRemove)<-c(\"FID\",\"IID\")\n\n# write table \nwrite.table(IDtoRemove, file=paste0(out,\"/IDstoRemove_related.txt\"), row.names=F, col.names=T, quote = F)\n# identified 622 IDs to remove due to relatedness\n\n\nThis next step removes IDs of related individuals just identified in the previous step.\n\n\nCode\n# Step 5: Make fam ID list excluding related individuals\nrelatedIDs=\"/CCACE_Shared/Anna_F/BrainAtrophy/data/geneticQC/sourceCopy/IDstoRemove_related.txt\"\n\nplink19 \\\n--bed ${target}/ukb_neuroimaging_autosomal_sex_preQC.bed \\\n--bim ${target}/ukb_neuroimaging_autosomal_sex_preQC.bim \\\n--fam ${target}/ukb_neuroimaging_autosomal_sex_preQC.fam \\\n--geno 0.02 \\\n--extract ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR.snplist \\\n--keep ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR.fam \\\n--write-snplist \\\n--make-just-fam \\\n--remove $relatedIDs \\\n--hardy --hwe 0.00000001 \\\n--out ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001\n\n# after removing 622 IDs due to relatedness, we have 43414 participants remaining\n# 84 variants removed due to missing genotype data\n# 11398 variants removed due to Hardy-Weinberg exact test (running this analysis throws a warning that \"--hwe observation counts vary by more than 10%, due to the X chromosome.  You may want to use a less stringent --hwe p-value threshold for X chromosome variants\" - however, without the X chromsome, this filter removes 11169 variants which is about the same so will leave this\n# 586058 variants remaining (which is about the same as in my previous study in another application)\n# the resulting .fam and .snplist files could now be fed into regenie, but first we'll also do sex-check"
  },
  {
    "objectID": "UKB_geneticQC.html#perform-sex-check",
    "href": "UKB_geneticQC.html#perform-sex-check",
    "title": "UK Biobank: Genetic data cleaning",
    "section": "Perform sex-check",
    "text": "Perform sex-check\n\n\nCode\n# Step 6: Prune SNPs for sex-check\n# to do sex-check, we prune SNP data to be independent and exclude high LD regions\n# this is because patterns of LD will impair chromosome-specific tests of homozygosity\nawk -f ${target}/highLDregions4bim_b37.awk ${target}/ukb_neuroimaging_reQC.bim > ${target}/ukb_neuroimaging_High_LD_Regions_To_Exclude.txt\n# this file contains 28923 SNPs to exclude from sex-check analysis\n\nplink19 \\\n--bed ${target}/ukb_neuroimaging_autosomal_sex_preQC.bed \\\n--bim ${target}/ukb_neuroimaging_autosomal_sex_preQC.bim \\\n--fam ${target}/ukb_neuroimaging_autosomal_sex_preQC.fam \\\n--maf 0.05 \\\n--hwe 0.001 \\\n--geno 0.02 \\\n--thin-indiv-count 300 \\\n--indep-pairphase 200 100 0.2 \\\n--seed 1204688 \\\n--exclude ${target}/ukb_neuroimaging_High_LD_Regions_To_Exclude.txt \\\n--extract ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001.snplist \\\n--keep ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001.fam \\\n--out ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001_LD1_ALL\n\n\n\n# Step 7: Perform sex-check\n# F statistic used here is a function of teh deviation of the observed number of heterozygote variants from that expected under Hardy-Weinberg equilibrium\n# males should have F ~1 because all X chromosome variants are hemizygous and no heterozygotes can be observed\n# females should have lower values of F, distributed around 0 (but females with very high F stats have been observed)\n# this script shouldn't need a seperate --split-x analysis because the X chromosome's pseudoautosomal region is already presented as XY chromosome\n\nplink19 \\\n--bed ${target}/ukb_neuroimaging_autosomal_sex_preQC.bed \\\n--bim ${target}/ukb_neuroimaging_autosomal_sex_preQC.bim \\\n--fam ${target}/ukb_neuroimaging_autosomal_sex_preQC.fam \\\n--check-sex \\\n--keep ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001.fam \\\n--extract ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001_LD1_ALL.prune.in \\\n--out ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001_PRUNE_SEX\n\n\n\nRemove individuals that did not pass sex-check\n\n\nCode\n### Extract participants that have been labeled as PROBLEM\nR\nlibrary(data.table)\ndat=fread(\"ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001_PRUNE_SEX.sexcheck\")\n\nIDstoRemove=dat[which(dat$STATUS == \"PROBLEM\"),c(\"FID\",\"IID\")]\n# 22 \nwrite.table(IDstoRemove, file=paste0(out,\"/sexCheck_toRemove.txt\"), row.names=F, col.names=T, quote = F)"
  },
  {
    "objectID": "UKB_geneticQC.html#generate-final-.fam-and-.snplist-files",
    "href": "UKB_geneticQC.html#generate-final-.fam-and-.snplist-files",
    "title": "UK Biobank: Genetic data cleaning",
    "section": "Generate final .fam and .snplist files",
    "text": "Generate final .fam and .snplist files\n\n\nCode\n## Step 8: Generate final .fam and .snplist to feed into regenie (excluding failed sex checks)\nplink19 \\\n--bed ${target}/ukb_neuroimaging_autosomal_sex_preQC.bed \\\n--bim ${target}/ukb_neuroimaging_autosomal_sex_preQC.bim \\\n--fam ${target}/ukb_neuroimaging_autosomal_sex_preQC.fam \\\n--extract ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001.snplist \\\n--keep ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001.fam \\\n--write-snplist \\\n--make-just-fam \\\n--remove ${target}/sexCheck_toRemove.txt \\\n--out ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001_sexcheck"
  },
  {
    "objectID": "UKB_geneticQC.html#generate-final-bed-file",
    "href": "UKB_geneticQC.html#generate-final-bed-file",
    "title": "UK Biobank: Genetic data cleaning",
    "section": "Generate final bed file",
    "text": "Generate final bed file\n\n\nCode\n# Step 9: Generate final bed files that will be used as input\nfinal=\"/CCACE_Shared/Anna_F/BrainAtrophy/data/geneticQC\"\n\nplink19 \\\n--bed ${target}/ukb_neuroimaging_autosomal_sex_preQC.bed \\\n--bim ${target}/ukb_neuroimaging_autosomal_sex_preQC.bim \\\n--fam ${target}/ukb_neuroimaging_autosomal_sex_preQC.fam \\\n--extract ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001_sexcheck.snplist \\\n--keep ${target}/ukb_neuroimaging_MAF0.01_GENO0.02_QA_EUR_HWE0.00000001_sexcheck.fam \\\n--chr 1-22 \\\n--make-bed \\\n--out ${final}/ukb_neuroimaging_brainAtrophy_GWASinput"
  },
  {
    "objectID": "UKB_neuro.html",
    "href": "UKB_neuro.html",
    "title": "UK Biobank: Neuroimaging data preparation",
    "section": "",
    "text": "Code displayed here was used to obtain neuroimaging measures: TBV, ICV, LBA (difference, ratio, residual scores)."
  },
  {
    "objectID": "UKB_neuro.html#load-packages",
    "href": "UKB_neuro.html#load-packages",
    "title": "UK Biobank: Neuroimaging data preparation",
    "section": "Load packages",
    "text": "Load packages\n\n\nCode\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(cowplot)"
  },
  {
    "objectID": "UKB_neuro.html#define-functions",
    "href": "UKB_neuro.html#define-functions",
    "title": "UK Biobank: Neuroimaging data preparation",
    "section": "Define functions",
    "text": "Define functions\nFunctions plot_hist and descriptives expect input data set to contain variables called diff, ratio, resid. plot_hist can also handle diff_stand, ratio_stand, resid_stand and will add an extra x-axis if input are standardised variables.\ndescriptives gives a table of descriptive statistics for TBV, ICV and LBA phenotypes.\n\n\nCode\nplot_hist <- function(dat = dat, var = \"diff_stand\", split_sample_by = NULL){\n  # install packages if they don't already exits\n  packages = c(\"ggplot2\",\"stringr\", \"tidyr\", \"dplyr\")\n  install.packages(setdiff(packages, rownames(installed.packages())))\n  # load packages\n  library(ggplot2)\n  library(stringr)\n  library(tidyr)\n  library(dplyr)\n\n  # make sure input data is data.frame\n  dat = as.data.frame(dat)\n  # rename for simplicity\n  dat$var = dat[,var]\n\n  # calculate summary stats\n    df_stats <-\n        dat %>%\n        summarize(\n          mean = mean(var, na.rm=T),\n          median = median(var, na.rm=T)\n        ) %>%\n        gather(key = Statistic, value = value, mean:median)\n\n    # calculate SD cutoffs\n    insert = c(\"+2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) + 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    insert = c(\"-2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) - 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    # format\n    df_stats$value <- as.numeric(df_stats$value)\n\n    # consider one-sided nature of cut-off\n    # if difference score, we use the upper 2 SD limit\n    # if ratio or residual score, we use the lower 2 SD limit\n    if(var == \"diff\" | var == \"diff_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"-2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }else if(var == \"ratio\" | var == \"resid\" | var == \"ratio_stand\" | var == \"resid_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"+2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }\n\n\n  # PLOT\n  # different output when there is a \"sample\" column\n  if(is.null(split_sample_by)){\n      plot = ggplot(dat, aes(x = var))+\n          geom_histogram(bins = 100, alpha = 0.5, fill = \"#56B4E9\")+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n\n\n  }else if(!is.null(split_sample_by)){\n\n    if(length(which(names(dat) == split_sample_by)) == 0){\n      message(paste0(\"You have indicated that you wanted to group plotted values by \", split_sample_by,\", but the data contains no such column.\")); break\n    }\n\n    # incorporate grouping variable\n    names(dat)[which(names(dat) == split_sample_by)] = \"split_sample_by\"\n    # make sure its a factor\n    dat$split_sample_by = as.factor(dat$split_sample_by)\n\n    colors = c(\"#56B4E9\",\"#009E73\", \"#E69F00\") # \"#79AC78\" #grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]\n    colors = colors[1:length(unique(dat$split_sample_by))]\n\n      plot = ggplot(dat)+\n          geom_histogram(aes(x = var, fill = split_sample_by), bins = 100, alpha = 0.5)+\n          scale_fill_manual(values = colors, name = split_sample_by)+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n  }\n\n    # make second x-axis if we're working with standardised variables\n    if(length(grep(\"_stand\", var)) != 0){\n\n      # calculate mean from original variable\n      varOr = str_remove(var, \"_stand\")\n      mean = mean(dat[,varOr], na.rm=T)\n      sd = sd(dat[,varOr], na.rm=T)\n\n      # add secondary x axis\n      plot = plot+\n         scale_x_continuous(sec.axis = sec_axis(name = \"Raw values\", trans=~.*sd+mean))\n\n    }\n\n  plot = plot+theme(panel.border = element_blank())\n\n  return(plot)\n}\n\n# this onyl works for the correct naming of the variable names to diff, ratio and resid\ndescriptives = function(samples = c(\"HCP\", \"Share\", \"both\")){\n  # define statistics to include\n  stats = c(\"N\", \"TBV: Mean (SD)\", \"ICV: Mean (SD)\", \"cor(ICV,TBV)\",\n            \"*Difference score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\",\n            \"*Ratio score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\",\n            \"*Residual score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\")\n\n  # object to hold results\n  res = as.data.frame(matrix(ncol = length(samples)+1, nrow = length(stats)))\n  names(res) = c(\"Statistic\", samples)\n  res$Statistic = stats\n\n  for(i in samples){\n    # pull sample\n    dat = as.data.frame(get(i))\n\n    # N\n    N = sum(!is.na(dat$diff))\n    res[which(res$Statistic == \"N\"), which(names(res) == i)] = N\n\n    # TBV: Mean (SD)\n    mean = round(mean(dat$TBV, na.rm = T), digits = 2)\n    SD = signif(sd(dat$TBV, na.rm = T), digits = 2)\n    res[which(res$Statistic == \"TBV: Mean (SD)\"), which(names(res) == i)] = paste0(mean, \" (\", SD,\")\")\n\n    # ICV: Mean (SD)\n    mean = round(mean(dat$ICV, na.rm = T), digits = 2)\n    SD = signif(sd(dat$ICV, na.rm = T), digits = 2)\n    res[which(res$Statistic == \"ICV: Mean (SD)\"), which(names(res) == i)] = paste0(mean, \" (\", SD,\")\")\n\n    # ICV TBV correlation\n    cor = round(cor.test(dat$ICV, dat$TBV)$estimate, digits = 2)\n    res[which(res$Statistic == \"cor(ICV,TBV)\"), which(names(res) == i)] = cor\n\n    # Cycle through different scores\n    for(j in c(\"Difference\", \"Ratio\", \"Resid\")){\n        # determine variable that matches the right score\n        if(j == \"Difference\"){\n          VarName = \"diff\"\n        }else if(j == \"Ratio\"){\n          VarName = \"ratio\"\n        }else if(j == \"Resid\"){\n          VarName = \"resid\"\n        }\n\n        dat$var = dat[,VarName]\n\n        ### Calculate mean and SD\n        mean = round(mean(dat$var, na.rm=T), digits = 2)\n        sd = round(sd(dat$var, na.rm=T), digits = 2)\n        # find correct position in res to store result\n        index = grep(j, res$Statistic)\n        Cand = grep(\"Mean\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        # store mean result\n        res[pos, which(names(res) == i)] = paste0(mean, \" (\", sd, \")\")\n\n        ### Calculate median\n        median = round(median(dat$var, na.rm=T), digits = 2)\n        #store median result\n        Cand = grep(\"Median\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = median\n\n        ### Calculate range\n        min = round(min(dat$var, na.rm = T), digits = 2)\n        max = round(max(dat$var, na.rm = T), digits = 2)\n        # store results\n        Cand = grep(\"Range\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = paste0(min, \" to \", max)\n\n        ## Calculate variance\n        variance = signif(var(dat$var, na.rm = T), digit = 2)\n        # store variance result\n        Cand = grep(\"Variance\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = variance\n\n        ### calculate cut-off\n        if(j == \"Difference\"){\n          cutOff = mean(dat$var, na.rm = T)+(2*sd(dat$var, na.rm = T))\n        }else{\n            cutOff = mean(dat$var, na.rm = T)-(2*sd(dat$var, na.rm = T))\n        }\n        # store results\n        Cand = grep(\"Cut\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = round(cutOff, digit = 1)\n    }\n  }\n\n  return(res)\n}\n\n\n# define function to make ggplots prettier\nmake_pretty <- function(){\n  theme(text = element_text(size=6),\n        axis.text.x = element_text(size=4, colour='#696969'),\n        axis.text.y = element_blank(),\n        plot.title = element_text(face=\"bold\", colour='#1A1A1A', size=6, hjust = 0.5),\n        axis.title.x = element_text(face=\"bold\", colour='#1A1A1A', size=6),\n        axis.title.y = element_text(face=\"bold\", colour='#1A1A1A', size=6),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x.top = element_text(color = \"grey\", size=6, hjust=0))\n}"
  },
  {
    "objectID": "UKB_neuro.html#extract-cross-sectionally-processed-neuroimaging-data",
    "href": "UKB_neuro.html#extract-cross-sectionally-processed-neuroimaging-data",
    "title": "UK Biobank: Neuroimaging data preparation",
    "section": "Extract cross-sectionally processed neuroimaging data",
    "text": "Extract cross-sectionally processed neuroimaging data\nHere we aggregate neuroimaging measures to calculate lifetime atrophy scores (ICV, TBV), in addition to CSF, and T1-scaling factor (N = 46836). This was the phenotypic input data for the GWAS.\n\n\nCode\n##############################\n# aim is to extract neuroimaging data for UKB from the IDP variables : ICV & TBV\n# also add T1 volumetric scaling factor (field ID 25000) & CSF (field ID: 26527)\n# downloaded N = 46836\nfile = fread(paste0(wd, list.files(path = wd, pattern = \"RAP_download_08022024_neuro\")))\nnames(file) = paste0(\"f.\", names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# keep ID, 26515 & 26521\nCols = grepl(\"f.eid|f.26515_2.0|26521_2.0|f.25000_2|26527_2\", names(file))\n\n# select columns of interest\nfile = file[, ..Cols]\n\n# name variables TBV and icv\nnames(file)[grep(\"f.26515\", names(file))] = \"TBV\"\nnames(file)[grep(\"f.26521\", names(file))] = \"ICV\"\nnames(file)[grep(\"f.25000\", names(file))] = \"T1ScalingFactor\"\nnames(file)[grep(\"f.26527\", names(file))] = \"CSF\"\n\n#######################\n# Quality control: \n# something must have gone wrong if TBV is larger than ICV - delete\ndelete = sum(file$ICV - file$TBV < 0, na.rm=T)\nprint(paste(delete, \" people have larger TBV than ICV, and will therefore be removed from the sample.\"))\nfile = file[file$ICV - file$TBV >= 0,]\n\n# also a participant has ICV > 5000 which would be 5 times thesize of the smaller brains in the sample - delete \nprint(paste(sum(file$ICV > 5000000), \" people have ICV > 5000000 which is 5 time larger than the average brain in the sample, and will therefore be removed from the sample.\"))\nfile = file[file$ICV <= 5000000,]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\nfile$ICV = file$ICV/1000\nfile$TBV = file$TBV/1000\n\n# estimate brain atrophy from single MRI scan\nfile$diff = file$ICV - file$TBV\nfile$ratio = file$TBV / file$ICV\n\nmodel <- lm(TBV ~ ICV, data = file)\nfile$resid = resid(model)\n\nfileNoMiss = file[!is.na(file$T1ScalingFactor),]\n\nmodel <- lm(TBV ~ T1ScalingFactor, data = fileNoMiss)\nfileNoMiss$residScalingFactor = resid(model)\n\n# merge back in with file\nfile = merge(file, fileNoMiss[,c(\"f.eid\", \"residScalingFactor\")], by = \"f.eid\", all.x=T)\n\n\n# standardise variables within one time-point\nfile$resid_stand = as.vector(scale(file$resid))\nfile$diff_stand = as.vector(scale(file$diff))\nfile$ratio_stand = as.vector(scale(file$ratio))\nfile$TBVstand = as.vector(scale(file$TBV))\nfile$ICVstand = as.vector(scale(file$ICV))\nfile$residScalingFactor_stand = as.vector(scale(file$residScalingFactor))\nfile$CSFstand = as.vector(scale(file$CSF))\n\n# for regenie to recognise, need to name ID column IID and add FID\nnames(file)[grep(\"f.eid\", names(file))] = \"IID\"\nfile$FID = file$IID\n\n# change order of the columns\norderedNames = c(\"FID\", \"IID\", names(file)[2:(length(names(file)) -1)])\nfile = file[, ..orderedNames]\n\n# write file\nfwrite(file, paste0(wd, \"/UKB_CrossNeuroIDP.txt\"), quote = F, col.names = T, sep = \"\\t\", na = \"NA\")\n\n\nUpon inspection, I noticed that there are two pretty severe outliers: outside of 10 SDs. Remove those here because they had some impossible CSF values which were larger than ICV.\n\n\nCode\n# read in data with all participants\ndat = fread(paste0(wd, \"/UKB_CrossNeuroIDP.txt\"))\n\n# delete all participants that have difference score larger than 10 SDs\ndat = dat[which(dat$diff_stand < 10),]\n# 2487172 2595043 are both not available for raw data so I can't look at whether anything has gone wrong with processing\n\n# there are also two participants with CSFstand > 10 which skew the distribution\ndat = dat[which(dat$CSFstand < 10),]\n\n# re-calculate reisudal measures after those deletions\n## first delete all residu measures\ndat$resid = NULL\ndat$residScalingFactor = NULL\n\n## second recaculate all resid measures\nmodel <- lm(TBV ~ ICV, data = dat)\ndat$resid = resid(model)\n\nfileNoMiss = dat[!is.na(dat$T1ScalingFactor),]\n\nmodel <- lm(TBV ~ T1ScalingFactor, data = fileNoMiss)\nfileNoMiss$residScalingFactor = resid(model)\n\n# merge back in with file\ndat = merge(dat, fileNoMiss[,c(\"FID\", \"residScalingFactor\")], by = \"FID\", all.x=T)\n\n# standardise variables within one time-point\ndat$resid_stand = as.vector(scale(dat$resid))\ndat$diff_stand = as.vector(scale(dat$diff))\ndat$ratio_stand = as.vector(scale(dat$ratio))\ndat$TBVstand = as.vector(scale(dat$TBV))\ndat$ICVstand = as.vector(scale(dat$ICV))\ndat$residScalingFactor_stand = as.vector(scale(dat$residScalingFactor))\ndat$CSFstand = as.vector(scale(dat$CSF))\n\nfwrite(dat, \"UKB_CrossNeuroIDP_noOutliers.txt\", quote = F, col.names = T, sep = \"\\t\", na = \"NA\")"
  },
  {
    "objectID": "UKB_neuro.html#extract-covariates",
    "href": "UKB_neuro.html#extract-covariates",
    "title": "UK Biobank: Neuroimaging data preparation",
    "section": "Extract covariates",
    "text": "Extract covariates\nThese are the GWAS covariates.\n\n\nCode\n# sex: 31 (676893)\n# acquisition site: 54 (676893)\n# scanning day: 53 (676893) Day2day: investigating daily variability of MRI measures over half a year, Filevich et al., 2017; but actually studies suggest its mainly the time of day that matters (Identifying predictors of within-person variance in MRI-based brain volume estimates, Karch et al., 2019)\n# scanning month: 53\n### scan positions: 25756, 25757, 25758 (670476)\n\n#### extract scanning positions\nfile2 = fread(paste0(wd, list.files(path = wd, pattern = \"RAP_download_08022024_neuro\")))\nnames(file2) = paste0(\"f.\", names(file2))\nnames(file2) = gsub(\"-\", \"_\", names(file2), fixed = T)\n\n# keep ID,  & columns of interest\nCols = grepl(\"f.eid|f.25756_2|f.25757_2|f.25758_2\", names(file2))\n\n# select columns of interest\nfile2 = file2[, ..Cols]\n\n# change column names\nnames(file2)[grep(\"f.25756_2\", names(file2))] = \"xCoord\"\nnames(file2)[grep(\"f.25757_2\", names(file2))] = \"yCoord\"\nnames(file2)[grep(\"f.25758_2\", names(file2))] = \"zCoord\"\n\n#### extract sex, site, month, age\nfile = fread(paste0(wd, list.files(path = wd, pattern = \"RAP_download_08022024_covariates\")))\nnames(file) = paste0(\"f.\", names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# name variables TBV and icv\nnames(file)[grep(\"f.31\", names(file))] = \"sex\"\nnames(file)[grep(\"f.54\", names(file))] = \"site\"\n#names(file)[grep(\"f.53\", names(file))] = \"day\"\n#names(file)[grep(\"f.21022\", names(file))] = \"age_at_recruitment\"\nnames(file)[grep(\"f.52\", names(file))] = \"birth_month\"\nnames(file)[grep(\"f.53\", names(file))] = \"date_of_assessment\"\nnames(file)[grep(\"f.34\", names(file))] = \"birth_year\"\n\n# make sure site is categorical and represented in numbers\nfile$site[grep(\"Cheadle\", file$site)] = \"1\"\nfile$site[grep(\"Bristol\", file$site)] = \"2\"\nfile$site[grep(\"Newcastle\", file$site)] = \"3\"\nfile$site[grep(\"Reading\", file$site)] = \"4\"\nfile$site = as.factor(file$site)\n\n# work out assessment month\nfile$assessmentMonth = as.numeric(format(as.POSIXct(file$date_of_assessment), \"%m\"))\n\n# transform birth_month into numerics\nfile$birth_month[which(file$birth_month == \"January\")] = 1\nfile$birth_month[which(file$birth_month == \"February\")] = 2\nfile$birth_month[which(file$birth_month == \"March\")] = 3\nfile$birth_month[which(file$birth_month == \"April\")] = 4\nfile$birth_month[which(file$birth_month == \"May\")] = 5\nfile$birth_month[which(file$birth_month == \"June\")] = 6\nfile$birth_month[which(file$birth_month == \"July\")] = 7\nfile$birth_month[which(file$birth_month == \"August\")] = 8\nfile$birth_month[which(file$birth_month == \"September\")] = 9\nfile$birth_month[which(file$birth_month == \"October\")] = 10\nfile$birth_month[which(file$birth_month == \"November\")] = 11\nfile$birth_month[which(file$birth_month == \"December\")] = 12\n\nfile$birthday = 1\n\nfile$birth_date = as.Date(ISOdate(year = file$birth_year,\n                    month = file$birth_month, \n                    day = file$birthday))\n\n##### Work out age as the difference between date attended assessment center and birthday (we have month and year)\nfile$date_of_assessment = as.Date(file$date_of_assessment)\nfile$age = as.numeric(difftime(file$date_of_assessment, file$birth_date, units = \"days\"))/(365.5/12)\n\n# merge the two data files\nfile = merge(file, file2, by = \"f.eid\")\n\n# select columns of interest\nfile = file[,c(\"f.eid\", \"age\", \"sex\", \"assessmentMonth\", \"site\",\"xCoord\", \"yCoord\", \"zCoord\")]\n\n# make sex & assessment month a factor\nfile$assessmentMonth = as.factor(file$assessmentMonth)\nfile$sex[grep(\"Female\", file$sex)] = \"1\"\nfile$sex[grep(\"Male\", file$sex)] = \"0\"\nfile$sex = as.factor(file$sex)\n\n# genetic covariates saved in charleys file\ngenCovar = fread(\"/Cluster_Filespace/charley_ccace/Charley_UKB_OCT2020/Sample_QC_with_IDs_REM_19July2017.csv\")\nnames(genCovar)[which(names(genCovar) == \"ukb_id\")] = \"f.eid\"\n\n# store column names of columns of interest\ncovarNames = c(\"f.eid\", \"genotyping.array\", \"Batch\", paste0(\"PC\", 1:40))\ngenCovar = genCovar[, ..covarNames]\n\n# format factor variables\nnames(genCovar)[which(names(genCovar) == \"genotyping.array\")] = \"array\"\ngenCovar$array[grep(\"UKBB\", genCovar$array)] = \"0\"\ngenCovar$array[grep(\"UKBL\", genCovar$array)] = \"1\"\ngenCovar$array = as.factor(genCovar$array)\n\nnames(genCovar)[which(names(genCovar) == \"Batch\")] = \"batch\"\ngenCovar$batch = as.factor(genCovar$batch)\n\n# merge in with file\nfile = merge(file, genCovar, by = \"f.eid\")\n\n# for regenie to recognise, need to name ID column IID and add FID\nnames(file)[grep(\"f.eid\", names(file))] = \"IID\"\nfile$FID = file$IID\n\n# change order of the columns\norderedNames = c(\"FID\", \"IID\", names(file)[2:(length(names(file)) -1)])\nfile = file[, ..orderedNames]\n\n# write file\nfwrite(file, paste0(wd, \"/UKB_covarGWAS.txt\"), quote = F, col.names = T, sep = \"\\t\", na = \"NA\")\n# 45616 rows"
  },
  {
    "objectID": "UKB_neuro.html#correlations-between-phenotypes-of-interest-and-covariates",
    "href": "UKB_neuro.html#correlations-between-phenotypes-of-interest-and-covariates",
    "title": "UK Biobank: Neuroimaging data preparation",
    "section": "Correlations between phenotypes of interest and covariates",
    "text": "Correlations between phenotypes of interest and covariates\n\n\nCode\n##### this next section of code has been taken from REGENIE_step1_reAnalyse_resid.sh\n# read in pheno file\nall = fread(\"/CCACE_Shared/Anna_F/BrainAtrophy/data/UKB_CrossNeuroIDP_noOutliers.txt\")\n# read in fam file that restricts participants\nfam = fread(\"/CCACE_Shared/Anna_F/BrainAtrophy/data/geneticQC/ukb_neuroimaging_brainAtrophy_GWASinput.fam\")\n\n# keep only fam particiants\nall = all[all$FID %in% fam$V1,]\n\n# read in covar file to keep all non-missing participants\ncovar = fread(\"/CCACE_Shared/Anna_F/BrainAtrophy/data/UKB_covarGWAS.txt\")\ncovar = covar[complete.cases(covar),]\n\n# only keep complte covar cases\nall = merge(all, covar, by = c(\"IID\", \"FID\"))\n\n# recalculate resid score\nmodel <- lm(TBV ~ ICV, data = all)\nall$resid <- as.vector(resid(model))\n\n# standardised (this is the final phenotype modelled in GWAS)\nall$resid_stand = scale(all$resid)\n##################################\n\n# standardise some more covariates\nall$xCoord_stand <- scale(all$xCoord)\nall$yCoord_stand <- scale(all$yCoord)\nall$zCoord_stand <- scale(all$zCoord)\nall$age_stand <- scale(all$age)\n\n\n# get all possible combinations between variable names we want to consider (exclude genetic PCs to keep an overview - PCs will be fine)\nNames <- names(all)[!grepl(\"PC\", names(all))]\n# also exclude ID variables\nNames <- Names[!grepl(\"ID\", Names)] \n#only keep stand variables\nNames <- Names[grepl(\"stand\", Names)]\n\n# get all combinations of traits\ntraits <- expand.grid(Names, Names)\n# transofmr factor into character vectors\ntraits <- data.frame(lapply(traits, as.character), stringAsFactors = F)\n# delete combinations where trait is paired with itself\ntraits <- traits[traits$Var1 != traits$Var2,]\n\n# build data frame to hold output values for the correlations between traits\nresNames <- c(\"predictor\", \"outcome\", \"assoc\", \"se\", \"p\") \nres <- data.frame(matrix(nrow = nrow(traits), ncol = length(resNames)))\nnames(res) <- resNames\n\nres[,c(\"predictor\", \"outcome\")] <- traits[,c(\"Var1\", \"Var2\")]\n\n# cycle through the three phenos of interest\nfor(i in 1:nrow(res)){\n\n    # extract the var names\n    vars <- res[i, c(\"predictor\", \"outcome\")]\n\n    # build model (the way expand.grid arranged the traits meant that all the continious traits of interest are in the second column - so here they are placed as the outcome variable in the model)\n    mod <- lm(as.formula(paste0( as.character(vars[2]), \" ~ \", as.character(vars[1]))), data = all)\n\n    # get summary of model\n    modR <- summary(mod)\n    \n    # extract and store results\n    res[i, \"assoc\"] <- modR$coefficients[2,1]\n    res[i, \"se\"] <- modR$coefficients[2,2]\n    res[i, \"p\"] <- modR$coefficients[2,4]\n}\n\nfwrite(res, \"UKB_covar_assocs.table\", quote = F, col.names = T, sep = \"\\t\", na = \"NA\")"
  },
  {
    "objectID": "UKB_neuro.html#extract-longitudinal-estimates-from-this-cross-sectional-data",
    "href": "UKB_neuro.html#extract-longitudinal-estimates-from-this-cross-sectional-data",
    "title": "UK Biobank: Neuroimaging data preparation",
    "section": "Extract longitudinal estimates from this cross-sectional data",
    "text": "Extract longitudinal estimates from this cross-sectional data\nWe had planned to longitudinally process UKB data from the initial and second neuroimaging visit. This however was not possible due to missing files for all but ~600 participants from field ID 20263 (missing files were mri/orig/001.mgz that need to be present in the FS output directory to run the longitudinal processing pipeline). We noticed this during the time that UKB stopped data downloads. Hence, we were not given permission to download raw MRI files (field 20253), meaning that, unfortunately, we had to work with tabulated data only, even to extract longitudinal estimates of brain change. For this reason, the output file created here is called: ‘UKB_neuroNoLongProcess.txt’.\n\n\nCode\n# it's aim is to extract neuroimaging data for UKB from the IDP variables \nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# keep ID, 26515 & 26521\nCols = grepl(\"f.eid|f.26515|26521\", names(file))\n\n# select columns of interest\nfile = file[, ..Cols]\n\n# melt data to get wave column\nfile = melt(file, id.vars = \"f.eid\", measure.vars = list(c(\"f.26515_2.0\", \"f.26515_3.0\"), c(\"f.26521_2.0\", \"f.26521_3.0\")), value.name = c(\"f.26515\", \"f.26521\"))\n\n# re-name varibale to wave\nnames(file)[which(names(file) == \"variable\")] = \"wave\"\n\n# add +1 for correct wave\nfile$wave = as.numeric(file$wave) + 1\n\n# name variables TBV and icv\nnames(file)[grep(\"f.26515\", names(file))] = \"TBV\"\nnames(file)[grep(\"f.26521\", names(file))] = \"ICV\"\n\n##############################\n# also add T1 volumetric scaling factor (field ID 25000) & CSF (field ID: 26527)\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nmore = fread(paste0(path, \"/\", fileID))\n# R doesnt like column names that start with number and it doesnt like -\nnames(more) = paste0(\"f.\",names(more))\nnames(more) = gsub(\"-\", \"_\", names(more), fixed = T)\n\n# keep ID, 26515 & 26521\nCols = grepl(\"f.eid|f.25000|26527\", names(more))\n\n# select columns of interest\nmore = more[, ..Cols]\n\n# melt data to get wave column\nmore = melt(more, id.vars = \"f.eid\", measure.vars = list(c(\"f.25000_2.0\", \"f.25000_3.0\"), c(\"f.26527_2.0\", \"f.26527_3.0\")), value.name = c(\"f.25000\", \"f.26527\"))\n\n# re-name varibale to wave\nnames(more)[which(names(more) == \"variable\")] = \"wave\"\n\n# add +1 for correct wave\nmore$wave = as.numeric(more$wave) + 1\n\n# name variables TBV and icv\nnames(more)[grep(\"f.25000\", names(more))] = \"T1ScalingFactor\"\nnames(more)[grep(\"f.26527\", names(more))] = \"CSF\"\n\n# merge with other data\nfile = merge(file, more, by = c(\"f.eid\", \"wave\"),all = T)\n##############################\n\n# Quality control: \n# something must have gone wrong if TBV is larger than ICV - delete\ndelete = sum(file$ICV - file$TBV < 0, na.rm=T)\nprint(paste(delete, \" people have larger TBV than ICV, and will therefore be removed from the sample.\")) #17 participants\nfile = file[file$ICV - file$TBV >= 0,]\n\n# also a participant has ICV > 5000 which would be 5 times thesize of the smaller brains in the sample - delete \nfile = file[file$ICV <= 5000000,]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\nfile$ICV = file$ICV/1000\nfile$TBV = file$TBV/1000\n\n# estimate brain atrophy from single MRI scan\nfile$diff = file$ICV - file$TBV\nfile$ratio = file$TBV / file$ICV\n\n##### derive the residuals for each time point separately \n## first wave (named wave 2 in UKB)\nfile1 = file[which(file$wave == 2),]\n\nmodel <- lm(TBV ~ ICV, data = file1)\nfile1$resid = resid(model)\n\n# also derive residual model for scaling factor\n# for some reason here we have an issue with missing data, so delete and re-merge\nfileNoMiss = file1[!is.na(file1$T1ScalingFactor),]\n\nmodel <- lm(TBV ~ T1ScalingFactor, data = fileNoMiss)\nfileNoMiss$residScalingFactor = resid(model)\n\n# merge back in with file\nfile1 = merge(file1, fileNoMiss[,c(\"f.eid\", \"residScalingFactor\")], by = \"f.eid\", all.x=T)\n\nsum(!is.na(fileNoMiss$residScalingFactor))\nsum(!is.na(file1$residScalingFactor))\n\n# standardise variables within one time-point\nfile1$resid_stand = as.vector(scale(file1$resid))\nfile1$diff_stand = as.vector(scale(file1$diff))\nfile1$ratio_stand = as.vector(scale(file1$ratio))\nfile1$TBVstand = as.vector(scale(file1$TBV))\nfile1$ICVstand = as.vector(scale(file1$ICV))\nfile1$residScalingFactor_stand = as.vector(scale(file1$residScalingFactor))\nfile1$CSFstand = as.vector(scale(file1$CSF))\n\n# wave 4\nfile3 = file[which(file$wave == 3),]\nmodel <- lm(TBV ~ ICV, data = file3)\nfile3$resid = resid(model)\n\n# also derive residual model for scaling factor\n# for some reason here we have an issue with missing data, so delete and re-merge\nfileNoMiss = file3[!is.na(file3$T1ScalingFactor),]\n\nmodel <- lm(TBV ~ T1ScalingFactor, data = fileNoMiss)\nfileNoMiss$residScalingFactor = resid(model)\n\n# merge back in with file\nfile3 = merge(file3, fileNoMiss[,c(\"f.eid\", \"residScalingFactor\")], by = \"f.eid\", all.x=T)\n\nsum(!is.na(fileNoMiss$residScalingFactor))\nsum(!is.na(file3$residScalingFactor))\n\n\n# standardise variables within one time-point\nfile3$resid_stand = as.vector(scale(file3$resid))\nfile3$diff_stand = as.vector(scale(file3$diff))\nfile3$ratio_stand = as.vector(scale(file3$ratio))\nfile3$TBVstand = as.vector(scale(file3$TBV))\nfile3$ICVstand = as.vector(scale(file3$ICV))\nfile3$residScalingFactor_stand = as.vector(scale(file3$residScalingFactor))\nfile3$CSFstand = as.vector(scale(file3$CSF))\n\n#merge the two waves back together\nfile = rbind(file1, file3)\n\n# only keep participants that have both measurement points\nfile = file[file$f.eid %in% file$f.eid[duplicated(file$f.eid)],]\n\n# store as txt file\nfwrite(file[,c(\"f.eid\", \"wave\", \"ICV\", \"TBV\", \"CSF\",  \"T1ScalingFactor\",\"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\", \"residScalingFactor_stand\")], paste0(wd, \"/UKB_crossNeuroWave2_3.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n# make wide format \ntemp = reshape(file[, c(\"f.eid\", \"wave\", \"TBV\")], idvar = \"f.eid\", timevar = \"wave\", direction = \"wide\")\n\n#### Difference score \n# Step 2: calculate difference in TBV between wave 2 and wave 5\ntemp$TBVdiff_2to3 = temp$TBV.2 - temp$TBV.3\n\n###### Ratio score\n# Step 2: calculate difference in TBV between wave 2 and wave 5\ntemp$TBVratio_3to2 = temp$TBV.3 / temp$TBV.2\n\n###### Resid score\n# remove missing because results with missing produces weird dimensions\n#temp = temp[!is.na(temp$TBV.2),]\n#temp = temp[!is.na(temp$TBV.3),] \n# Step 2: calculate difference in TBV between wave 2 and wave 5\nmodel = lm(TBV.3 ~ TBV.2, data = temp)\ntemp$TBVresid_2to3 = resid(model)\n\n# standardise variables\ntemp$TBV.2_stand = scale(temp$TBV.2)\ntemp$TBV.3_stand = scale(temp$TBV.3)\ntemp$TBVdiff_2to3_stand = scale(temp$TBVdiff_2to3)\ntemp$TBVratio_3to2_stand = scale(temp$TBVratio_3to2)\ntemp$TBVresid_2to3_stand = scale(temp$TBVresid_2to3)\n\n# no need to keep TBV.2 and TBV.3\ntemp = temp[, c(-2,-3)]\n\n# merge back in with neuro\ntemp = merge(neuro, temp, by = \"f.eid\")\n\n# store as txt file\nfwrite(temp, paste0(wd, \"/UKB_neuroNoLongProcess.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\nUKB (initial scan)\nShown in Supplementary Figure 3: Distributions of TBV, ICV, and lifetime brain atrophy estimated with the residual, ratio, and difference method. Histograms are coloured by age groups.\n\n\nCode\n####################################################\nUKB = fread(paste0(out, \"/UKB_CrossNeuroIDP_noOutliers.txt\"))\nage = fread(paste0(out, \"/UKB_covarGWAS.txt\"))\nUKB = merge(UKB, age[,c(\"FID\", \"age\")], by = \"FID\")\nUKB$Sample = \"UKB\"\nnames(UKB)[which(names(UKB) == \"IID\")] = \"ID\"\nnames(UKB)[which(names(UKB) == \"age\")] = \"Age\"\n\nUKB$Age <- UKB$Age / 12\n\n####################################################\n# make age groups\nUKB$Age_group <- NA\nUKB$Age_group[UKB$Age < 55] <- \"55 years and under\"\nUKB$Age_group[UKB$Age >= 55 & UKB$Age < 60] <- \"55 - 59\"\nUKB$Age_group[UKB$Age >= 60 & UKB$Age < 65] <- \"60 - 64\"\nUKB$Age_group[UKB$Age >= 65 & UKB$Age < 70] <- \"65 - 69\"\nUKB$Age_group[UKB$Age >= 70 & UKB$Age < 75] <- \"70 - 74\"\nUKB$Age_group[UKB$Age >= 75 & UKB$Age < 80] <- \"75 - 79\"\nUKB$Age_group[UKB$Age >= 80] <- \"80 years and over\"\n\np1=ggplot(UKB, aes(x=TBV, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"TBV\")+\n  theme_bw()\n\np2=ggplot(UKB, aes(x=ICV, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"ICV\")+\n  theme_bw()\n\np3=ggplot(UKB, aes(x=resid_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Residual score\")+\n  theme_bw()\n\np4=ggplot(UKB, aes(x=ratio_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Ratio score\")+\n  theme_bw()\n\np5=ggplot(UKB, aes(x=diff_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Difference score\")+\n  theme_bw()\n\npUKB <- ggarrange(p1,p2,p3,p4,p5, nrow = 1, common.legend = T, legend = \"bottom\")\n# add title\npUKB <- annotate_figure(pUKB, top = text_grob(\"UKB (inital neuroimaging visit)\",face = \"bold\", size = 14))\n\n#ggsave(paste0(out,\"phenotypic/UKB_disttributions.jpg\"), bg = \"white\",plot = pUKB, width = 30, height = 10, units = \"cm\", dpi = 300)\npUKB\n\n\n\n\n\n\n\nUKB (second scan)\nShown in Supplementary Figure 3: Distributions of TBV, ICV, and lifetime brain atrophy estimated with the residual, ratio, and difference method. Histograms are coloured by age groups.\n\n\nCode\n####################################################\nUKB = fread(paste0(out, \"/UKB_neuroNoLongProcess.txt\"))\nnames(UKB)[grepl(\"f.eid\", names(UKB))] <- \"FID\"\n# restrict to second neuroimaging visit (i.e., third visit altogether)\nUKB3 = UKB[UKB$wave == 3,]\n# add age info\nage = fread(paste0(out, \"/UKB_covarGWAS.txt\"))\nUKB3 = merge(UKB3, age[,c(\"FID\", \"age\")], by = \"FID\")\nnames(UKB3)[which(names(UKB3) == \"age.x\")] = \"Age\"\n\nUKB3$Age <- UKB3$Age / 12\n\n####################################################\n# make age groups\nUKB3$Age_group <- NA\nUKB3$Age_group[UKB3$Age < 55] <- \"55 years and under\"\nUKB3$Age_group[UKB3$Age >= 55 & UKB3$Age < 60] <- \"55 - 59\"\nUKB3$Age_group[UKB3$Age >= 60 & UKB3$Age < 65] <- \"60 - 64\"\nUKB3$Age_group[UKB3$Age >= 65 & UKB3$Age < 70] <- \"65 - 69\"\nUKB3$Age_group[UKB3$Age >= 70 & UKB3$Age < 75] <- \"70 - 74\"\nUKB3$Age_group[UKB3$Age >= 75 & UKB3$Age < 80] <- \"75 - 79\"\nUKB3$Age_group[UKB3$Age >= 80] <- \"80 years and over\"\n\np1=ggplot(UKB3, aes(x=TBV, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"TBV\")+\n  theme_bw()\n\np2=ggplot(UKB3, aes(x=ICV, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"ICV\")+\n  theme_bw()\n\np3=ggplot(UKB3, aes(x=resid_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Residual score\")+\n  theme_bw()\n\np4=ggplot(UKB3, aes(x=ratio_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Ratio score\")+\n  theme_bw()\n\np5=ggplot(UKB3, aes(x=diff_stand, fill=Age_group)) +\n  geom_histogram()+\n  scale_fill_manual(\"Age groups\", values = c(\"#292f56\", \"#1e4572\", \"#005c8b\", \"#008ba0\", \"#00bca1\",\"#69e882\", \"#acfa70\"))+\n  xlab(\"Difference score\")+\n  theme_bw()\n\npUKB3 <- ggarrange(p1,p2,p3,p4,p5, nrow = 1, common.legend = T, legend = \"bottom\")\n# add title\npUKB3 <- annotate_figure(pUKB3, top = text_grob(\"UKB (second neuroimaging visit)\",face = \"bold\", size = 14))\n\n#ggsave(paste0(out,\"phenotypic/UKB3_disttributions.jpg\"), bg = \"white\",plot = pUKB3, width = 30, height = 10, units = \"cm\", dpi = 300)\npUKB3\n\n\n\n\n\n\n\nDisplay raw change in TBV\n\n\nCode\n# read in UKB neuro data\nUKB = fread(paste0(out, \"/UKB_neuroNoLongProcess.txt\"))\n\nplot = ggplot()+\n  geom_point(data = UKB, aes(x = wave, y = TBV, group = f.eid),color = \"#82A0D8\", size = .5)+\n  geom_line(data = UKB, aes(x = wave, y = TBV, group = f.eid), color = \"#8DDFCB\", linewidth = 0.2, alpha = .2) +\n  scale_x_continuous(breaks = c(2,3))+\n  ylab(bquote('TBV in '~mm^3))+\n  xlab(\"Assessment visit\")+\n    theme(legend.position = \"none\")+\n    theme_bw()+\n    theme(text = element_text(size=15),\n          plot.margin=unit(c(1, 1, 1, 1), \"cm\"),\n          axis.text.y = element_text(size =15),\n          axis.text.x = element_text(size =15),\n          panel.border = element_blank())\n\n# get average measures\nmean2 = mean(UKB$TBV[which(UKB$wave == 2)],  na.rm=T)\nlabel2 = paste0(\"Mean = \", round(mean2, digits = 2))\n\nmean3 = mean(UKB$TBV[which(UKB$wave == 3)],  na.rm=T)\nlabel3 = paste0(\"Mean = \", round(mean3, digits = 2))\n\navg <- data.frame(x = c(2,3),\n                  y = c(mean2,mean3),\n                  label = c(label2, label3))\n\navg$yLabel = min(UKB$TBV, na.rm=T)\n\nplot=plot + geom_point(data = avg, aes(x=x,y=y), shape = 4, color = \"red\")+\n  geom_line(data = avg, aes(x=x,y=y), color = \"red\")+\n  #geom_text(data = avg, aes(x=x,y=yLabel, label = label), angle = 90, color = \"red\", vjust = 0, hjust = 0)\n  annotate(\"text\", x = 2.05, y = min(UKB$TBV, na.rm=T)-50, label = paste0(\"Mean = \", round(mean2, digits = 2)), hjust = 0, color = \"red\", angle = 90)+\n  annotate(\"text\", x = 2.95, y = min(UKB$TBV, na.rm=T)-50, label = paste0(\"Mean = \", round(mean3, digits = 2)), hjust = 0, color = \"red\", angle = 90)\n\n#ggsave(paste0(out,\"phenotypic/UKB_longChange.jpg\"), bg = \"white\",plot = plot, width = 10, height = 10, units = \"cm\", dpi = 200)\nplot\n\n\n\n\n\n\n\nFinal distributions for longitudinally-observed atrophic change measures\nFirst define plot_hist function.\n\n\nCode\nplot_hist <- function(dat = dat, var = \"diff_stand\", split_sample_by = NULL){\n  # install packages if they don't already exits\n  packages = c(\"ggplot2\",\"stringr\", \"tidyr\", \"dplyr\")\n  install.packages(setdiff(packages, rownames(installed.packages())))\n  # load packages\n  library(ggplot2)\n  library(stringr)\n  library(tidyr)\n  library(dplyr)\n\n  # make sure input data is data.frame\n  dat = as.data.frame(dat)\n  # rename for simplicity\n  dat$var = dat[,var]\n\n  # calculate summary stats\n    df_stats <-\n        dat %>%\n        summarize(\n          mean = mean(var, na.rm=T),\n          median = median(var, na.rm=T)\n        ) %>%\n        gather(key = Statistic, value = value, mean:median)\n\n    # calculate SD cutoffs\n    insert = c(\"+2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) + 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    insert = c(\"-2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) - 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    # format\n    df_stats$value <- as.numeric(df_stats$value)\n\n    # consider one-sided nature of cut-off\n    # if difference score, we use the upper 2 SD limit\n    # if ratio or residual score, we use the lower 2 SD limit\n    if(var == \"diff\" | var == \"diff_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"-2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }else if(var == \"ratio\" | var == \"resid\" | var == \"ratio_stand\" | var == \"resid_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"+2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }\n\n\n  # PLOT\n  # different output when there is a \"sample\" column\n  if(is.null(split_sample_by)){\n      plot = ggplot(dat, aes(x = var))+\n          geom_histogram(bins = 100, alpha = 0.5, fill = \"#56B4E9\")+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n\n\n  }else if(!is.null(split_sample_by)){\n\n    if(length(which(names(dat) == split_sample_by)) == 0){\n      message(paste0(\"You have indicated that you wanted to group plotted values by \", split_sample_by,\", but the data contains no such column.\")); break\n    }\n\n    # incorporate grouping variable\n    names(dat)[which(names(dat) == split_sample_by)] = \"split_sample_by\"\n    # make sure its a factor\n    dat$split_sample_by = as.factor(dat$split_sample_by)\n\n    colors = c(\"#56B4E9\",\"#009E73\", \"#E69F00\") # \"#79AC78\" #grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]\n    colors = colors[1:length(unique(dat$split_sample_by))]\n\n      plot = ggplot(dat)+\n          geom_histogram(aes(x = var, fill = split_sample_by), bins = 100, alpha = 0.5)+\n          scale_fill_manual(values = colors, name = split_sample_by)+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n  }\n\n    # make second x-axis if we're working with standardised variables\n    if(length(grep(\"_stand\", var)) != 0){\n\n      # calculate mean from original variable\n      varOr = str_remove(var, \"_stand\")\n      mean = mean(dat[,varOr], na.rm=T)\n      sd = sd(dat[,varOr], na.rm=T)\n\n      # add secondary x axis\n      plot = plot+\n         scale_x_continuous(sec.axis = sec_axis(name = \"Raw values\", trans=~.*sd+mean))\n\n    }\n\n  plot = plot+theme(panel.border = element_blank())\n\n  return(plot)\n}\n\n\nNote that I noticed the extreme outliers only later on in the analysis process, which is why this first step cleaning the measures is included (nature of an organic document I guess, sorry!).\n\n\nCode\nUKB = fread(paste0(out, \"/UKB_neuroNoLongProcess.txt\"))\n# keep only one wave (long data is duplicated here)\nUKB2 = UKB[which(UKB$wave == 2), ]\n\nUKB2 <- UKB2[which(UKB2$TBVdiff_2to3_stand < 10),]\nUKB2 <- UKB2[which(UKB2$TBVdiff_2to3_stand > (-10)),]\n\nUKB2 <- UKB2[which(UKB2$TBVratio_3to2_stand < 10),]\nUKB2 <- UKB2[which(UKB2$TBVratio_3to2_stand > (-10)),]\n\nUKB2 <- UKB2[which(UKB2$TBVresid_2to3_stand < 10),]\nUKB2 <- UKB2[which(UKB2$TBVresid_2to3_stand > (-10)),]\n\n\n# difference score\np1 = plot_hist(dat = UKB2, var = \"TBVdiff_2to3_stand\")+\n  xlab(\"TBV1 - TBV2\\n(Difference score)\")+\n  geom_histogram(fill = \"#D81B60\")+\n  make_pretty()\np1$layers[[2]] = NULL\n# ratio score\np2 = plot_hist(dat = UKB2, var = \"TBVratio_3to2_stand\")+\n  xlab(\"TBV2 divided by TBV1\\n(Ratio score)\")+\n  geom_histogram(fill = \"#FFC107\")+\n  make_pretty()\np2$layers[[2]] = NULL\n# resid score\np3 = plot_hist(dat = UKB2, var = \"TBVresid_2to3_stand\")+\n  xlab(\"TBV2 ~ TBV1\\n(Residual score)\")+\n  geom_histogram(fill = \"#004D40\")+\n  make_pretty()\np3$layers[[2]] = NULL\n\nplot = plot_grid(p1, p2, p3, nrow=1, labels = c(\"A\",\"B\",\"C\"), label_size = 6, rel_widths = c(1,1,1))\n\n#ggsave(paste0(out, \"phenotypic/UKBlong_distribution.png\"), plot = plot, width = 11, height = 5, units = \"cm\", dpi = 600)\nplot"
  },
  {
    "objectID": "UKB_pheno.html",
    "href": "UKB_pheno.html",
    "title": "UKB: Phenotypic data preparation",
    "section": "",
    "text": "Data prepared here was used as input into analyses presented here. The file containing all phenotypic variables was named UKB_allPheno.txt."
  },
  {
    "objectID": "UKB_pheno.html#load-packages",
    "href": "UKB_pheno.html#load-packages",
    "title": "UKB: Phenotypic data preparation",
    "section": "Load packages",
    "text": "Load packages\n\n\nCode\nlibrary(data.table)\nlibrary(lavaan)"
  },
  {
    "objectID": "UKB_pheno.html#define-function",
    "href": "UKB_pheno.html#define-function",
    "title": "UKB: Phenotypic data preparation",
    "section": "Define function",
    "text": "Define function\nThe function getFieldLoc identifies the directory with the most recent UKB download. We have multiple files on the server from different downloads, and I want to be sure I use the most recent one (i.e., the one saved in a directory with the highest number).\n\n\nCode\n# script that searches in our available data if we have a certain data field\ngetFieldLoc = function(path = path, fileName = fileName, fieldID = fieldID){\n  \n  # read all the field.ukb files\n  files_to_read = list.files(\n    path = path,\n    pattern = fileName,\n    recursive = T,\n    full.names = T\n  )\n  # read all files\n  dat = lapply(files_to_read, fread)\n  names(dat) = files_to_read\n  \n  # search for field ID of interest \n  candidates = names(dat)[grep(fieldID, dat)]\n  # figure out which one is from the most recent file (i.e., highest number)\n  candidates = str_remove(candidates, paste0(path, \"/\"))\n  candidates = str_remove(candidates, paste0(\"/\", fileName))\n  candidates = unique(as.numeric(sapply(str_extract_all(candidates, \"\\\\d+\"), tail , 1)))\n  most_recent = max(candidates, na.rm =T)\n  \n  return(paste0(path, most_recent))\n}"
  },
  {
    "objectID": "UKB_pheno.html#cognitive-test-factor-scores",
    "href": "UKB_pheno.html#cognitive-test-factor-scores",
    "title": "UKB: Phenotypic data preparation",
    "section": "Cognitive test (factor scores)",
    "text": "Cognitive test (factor scores)\nInput data used here was kindly provided by Joanna Moodie who had modeled the g-factor for one of her previous projects. The factor was modeled on the whole UKB sample, and then restricted to the participants with available neuroimaging data. This was deemed appropriate because the variables remained normally distributed in the restricted sample. Modelling the factor in the full sample will hopefully mean that the factor was based on a more representative sample than relying on participants who returned for their second neuroimaigng visit (healthy selection bias).\n\n\nCode\n### This script was adapted from Joanna Moodies script\nlibrary(lavaan)\n\n# read in the data\nUKBcog <- read.csv(\"/CCACE_Shared/Joanna/ForShare/UKB_g/UKB_cogtests.csv\", sep = \" \")\n# name ID column\nnames(UKBcog)[which(names(UKBcog) == \"ID\")] = \"f.eid\"\n\n# re-formatting data\nUKBcog$cog_trailB_log[which(UKBcog$cog_trailB_log == 0)] <- NA\nUKBcog$cog_prosmem[which(UKBcog$cog_prosmem == 2)] <- 0\nUKBcog$cog_pairedAss <- exp(UKBcog$cog_pairedAss_log)\n\n#UKBcog <- merge(UKBcog, ' ', by = \"ID\") # select the participants you want in the sample\n\n# keep varibales of interest\nUKBcog = UKBcog[,c(\"f.eid\",\n                            \"cog_RT_log\",\n                            \"cog_numeric_memory\",\n                            \"cog_fluid_intelligence\",\n                            \"cog_trailB_log\",\n                            \"cog_matrix_pattern_correct\",\n                            \"cog_tower\",\n                            \"cog_digsym\",\n                            \"cog_pairedAss\",\n                            \"cog_prosmem\",\n                            \"cog_pairsmatch_incorrect_log\")]\n\n# define general factor model\ncogmodel <- 'g =~  cog_RT_log +\n                    cog_numeric_memory + \n                    cog_fluid_intelligence + \n                    cog_trailB_log + \n                    cog_matrix_pattern_correct + \n                    cog_tower + \n                    cog_digsym + \n                    cog_pairsmatch_incorrect_log +\n                    cog_prosmem +\n                    cog_pairedAss\n                    '\n\n# fit general factor model\nfit = cfa(cogmodel, data = UKBcog, missing=\"ML\")\nsummary(fit, fit.measures=TRUE, standardized=T)\n\n# save fit for later\nsave(fit, file = paste0(out,\"/fit.RData\"))\n\n# predict individual-level factor scores \ngPheno <- lavPredict(fit, UKBcog)\ngPheno <- as.data.frame(cbind(UKBcog$f.eid, gPheno))\n\n# re-name columns\nnames(gPheno) = c(\"f.eid\",\"g\")\n\n# hist(gPheno$g)\n\n# standardise\ngPheno$gStand = scale(gPheno$g)\n\n# save in separate file \nwrite.table(gPheno[,c(\"f.eid\", \"gStand\")], file = paste0(out, \"/UKB_gFactor.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#dementia-status",
    "href": "UKB_pheno.html#dementia-status",
    "title": "UKB: Phenotypic data preparation",
    "section": "Dementia status",
    "text": "Dementia status\nNote that according to this definition only 5 participants a diagnosis, and the remaining ones are proxy cases.\n\n\nCode\n## find items and where they are stored on the server\n## dementia is defined based on multiple sources of information we have in the UKV\n## 1. ICD-10 diagnoses: field ID 41270\n## 2. Self-reported illnesses (dementia, alzheimers, cognitive impairment): field ID 20002\n## 3. Source of all cause dementia report: field ID 42019\n## 4. Contributing causes of death: field ID 40001 & 40002\n## 5. Date F00 first reported: field ID 130836\n\n#############################################\n## 1. ICD-10 diagnoses: field ID 41270\n#############################################\n# determine file paths\npath1 = getFieldLoc(path = path, \n            fileName = \"fields.ukb\", \n            fieldID = 41270)\n\nfileID = list.files(pat=path1,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path1, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\n#names(file) = str_replace(names(file), pattern = \"-\", replacement = \"_\")\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = grep(\"f.41270\", names(file))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# format \"\" to be NA\nfor(i in names(dement)[grep(\"f.41270\", names(dement))]){\n  dement[which(dement[,i] == \"\"),i] = NA\n}\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(dement[,-which(names(dement) == \"f.eid\")])) == 0) \n# 55361\n\n# determine ICD codes of interest to find in field ID 41270\nICDcode=c(\"F00\",\"F000\",\"F001\",\"F002\",\"F009\",\"G30\",\"G300\",\"G301\",\"G308\",\"G309\",\"F01\",\"F010\",\"F011\",\"F012\",\"F013\",\"F018\",\"F019\",\"I673\",\"F03\",\"G311\",\"G318\")\n\n# match ICDcodes with entries in all f.41270 variables filtered in 'dement'\ndement$countCodes = rowSums(!is.na(sapply(dement[,-which(names(dement) == \"f.eid\")], match, ICDcode)))\n\n# if 1 or more entries, consisder a case\ndement$dementICD41270 = as.factor(ifelse(dement$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\ndement$dementICD41270[allMissing] = NA\n\n# save this variable to a seperate file\nfwrite(dement[,c(\"f.eid\", \"dementICD41270\")], file = paste0(out, \"/UKB_dement_ICD41270.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")\n\n#############################################\n## 2. Self-reported illnesses (dementia, alzheimers, cognitive impairment, code = 1263): field ID 20002\n#############################################\nfileID = list.files(pat=path2,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path2, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\n#names(file) = str_replace(names(file), pattern = \"-\", replacement = \"_\")\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = grep(\"f.20002\", names(file))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# identify those who have fully missing data (except their participant ID)\nallMissing = which(rowSums(!is.na(dement[,-which(names(dement) == \"f.eid\")])) == 0) \n# 113066\n\n# determine code of interest to find in this field ID\ncode = \"1263\"\n\n# match ICDcodes with entries in all f.41270 variables filtered in 'dement'\ndement$countCodes = rowSums(!is.na(sapply(dement[,-which(names(dement) == \"f.eid\")], match,code)))\n\n# if 1 or more entries, consisder a case\ndement$dement20002 = as.factor(ifelse(dement$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\ndement$dement20002[allMissing] = NA\n\n# append final column to file\nfile$dement20002 = dement$dement20002\n\n#################################################\n## 3. Source of all cause dementia report: field ID 42019\n################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = grep(\"f.42019\", names(file))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# only record whether participants have an entry, which is indicative of dementia\n# however the absence of a code does not mean they don't have dementia\ndement$dement42019 = as.factor(ifelse(!is.na(dement$f.42019_0.0), 1, NA))\n\n# append final column to file\nfile$dement42019 = dement$dement42019\n\n###################################################\n## 4. Contributing causes of death: field ID 40001 & 40002\n###################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = c(grep(\"f.40001\", names(file)), grep(\"f.40002\", names(file)))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# format \"\" to be NA\nfor(i in names(dement)[c(grep(\"f.40001\", names(dement)), grep(\"f.40002\", names(dement)))]){\n  dement[which(dement[,i] == \"\"),i] = NA\n}\n\n# determine ICD codes of interest to find in field ID \nICDcode=c(\"F00\",\"F000\",\"F001\",\"F002\",\"F009\",\"G30\",\"G300\",\"G301\",\"G308\",\"G309\",\"F01\",\"F010\",\"F011\",\"F012\",\"F013\",\"F018\",\"F019\",\"I673\",\"F03\",\"G311\",\"G318\")\n\n# match ICDcodes with entries in all f.41270 variables filtered in 'dement'\ndement$countCodes = rowSums(!is.na(sapply(dement[,-which(names(dement) == \"f.eid\")], match, ICDcode)))\n\n# if 1 or more entries, consider a case\n# but if there is no entry, it probably more often means this person has not died yet, rather than that they had no dementia\ndement$dementDeath= as.factor(ifelse(dement$countCodes >= 1, 1, NA))\n\n# append final column to file\nfile$dementDeath = dement$dementDeath\n\n######################################\n## 5. Illnesses of father and mother (Alzheimer's disease/dementia; code = 10): field ID 20107 & 20110\n#####################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = c(grep(\"f.20107\", names(file)), grep(\"f.20110\", names(file)))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# remove all minus (-) values\ndement[dement <= 0] = NA\n\n# determine codes of interest to find in field ID \ncode=c(10)\n\n# match ICDcodes with entries in all f.41270 variables filtered in 'dement'\ndement$countCodes = rowSums(!is.na(sapply(dement[,-which(names(dement) == \"f.eid\")], match, code)))\n\n# if 1 or more entries, consider a case\n# this is a report on parents, so absence of entry does not mean they don't have dementia\ndement$dementParents = as.factor(ifelse(dement$countCodes >= 1, 1, NA))\n\n# append final column to file\nfile$dementParents = dement$dementParents\n\n\n################################################\n## 5. Date F00 first reported: field ID 130836\n###############################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = grep(\"f.130836\", names(file))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# store IDs for coded dates that suggest mistakes - however, doesnt contain any of the coded fields\n# which(dement$f.130836_0.0 == \"1901-01-01\")\n\n# record who has date \ndement$dement130836 = NA\ndement$dement130836[which(!is.na(dement$f.130836_0.0))] = 1\n\n# store in file\nfile$dement130836 = as.factor(dement$dement130836)\n\n################################################\n## Combine all data sources to one phenotype\n################################################\n# keep columns of interest\nfile = file[,c(\"f.eid\", \"dement20002\", \"dement42019\", \"dementDeath\",\"dementParents\",\"dement130836\")]\n\n# read in info from item 41270 (that was saved on a txt file - see (1.)) \nvar41270 = fread(paste0(out, \"/UKB_dement_ICD41270.txt\"), header = T)\nvar41270$dementICD41270 = as.factor(var41270$dementICD41270)\n\n# merge with info above\nfile = as.data.frame(merge(var41270, file, by = \"f.eid\"))\n\n# identify those with all missing\nallMissing = which(rowSums(!is.na(file[,-which(names(file) == \"f.eid\")])) == 0) \n\n# identify cases\nfile$dement = 0 \n\n# identify cases\nfile$dement[which(file$dement20002 == 1 |\n                file$dement42019 == 1 |\n                file$dementDeath == 1 |\n                file$dementParents == 1 |\n                file$dementICD41270 == 1 |\n                file$dement130836 == 1)] = 1\n\n\n# delete allMissing from controls\nfile$dement[allMissing] = NA\n\n# make factor\nfile$dement = as.factor(file$dement)\n\n#dementICD41270 dement20002   dement42019   dementDeath   dementParents  dement      \n#0   :442840    0   :389146   1   :  7896   1   :   860   1   : 66764   1   : 73835  \n#1   :  4156    1   :   156   NA's:494461   NA's:501497   NA's:435593   0   : 409281\n#NA's: 55361    NA's:113055                                             NA's: 19241\n\n# save final variable to a seperate file\nfwrite(file[,c(\"f.eid\", \"dement\")], file = paste0(out, \"/UKB_DementiaStatus.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#apoe-status",
    "href": "UKB_pheno.html#apoe-status",
    "title": "UKB: Phenotypic data preparation",
    "section": "APOE status",
    "text": "APOE status\nFirst extract the two SNPs of interest using PLINK2.\n\n\nCode\n# filter for the two APOE SNPs\nplink2 --bfile $loc/UKB500K_autosomes_X_XY_MAF_01_CLEAN_07052018_GD --snps rs7412,rs429358 --make-bed --freq --export A --out $temp/UKB_APOE\n\n\nThen, model APOE status according to the approach used here.\n\n\nCode\n# Determine whether someone has the e4 allele\n# read in data\ndat = read.table(paste0(temp, \"/UKB_APOE.raw\"), header = T)\n\n## simplify data\n# remove -IDs and remove redundant columns\ndat = dat[-which(dat$FID < 0),c(\"FID\", \"rs429358_T\", \"rs7412_C\")]\n\n# code variables to match look-up table\ndat$rs429358 = NA\ndat$rs429358[which(dat$rs429358_T == 2)] = \"TT\"\ndat$rs429358[which(dat$rs429358_T == 1)] = \"CT\"\ndat$rs429358[which(dat$rs429358_T == 0)] = \"CC\"\n\ndat$rs7412 = NA\ndat$rs7412[which(dat$rs7412_C == 2)] = \"CC\"\ndat$rs7412[which(dat$rs7412_C == 1)] = \"CT\"\ndat$rs7412[which(dat$rs7412_C == 0)] = \"TT\"\n\n# code variables according to combination of those genotypes\ndat$APOEgeno = NA\ndat$APOEgeno[which(dat$rs7412 == \"CC\" & dat$rs429358 == \"CC\")] = \"e4/e4\"\ndat$APOEgeno[which(dat$rs7412 == \"CC\" & dat$rs429358 == \"CT\")] = \"e3/e4\"\ndat$APOEgeno[which(dat$rs7412 == \"CT\" & dat$rs429358 == \"CT\")] = \"e2/e4\"\ndat$APOEgeno[which(dat$rs7412 == \"CC\" & dat$rs429358 == \"TT\")] = \"e3/e3\"\ndat$APOEgeno[which(dat$rs7412 == \"CT\" & dat$rs429358 == \"TT\")] = \"e2/e3\"\ndat$APOEgeno[which(dat$rs7412 == \"TT\" & dat$rs429358 == \"TT\")] = \"e2/e2\"\ndat$APOEgeno[which(dat$rs7412 == \"CT\" & dat$rs429358 == \"CC\")] = \"e1/e4\"\ndat$APOEgeno[which(dat$rs7412 == \"TT\" & dat$rs429358 == \"CT\")] = \"e1/e2\"\n\n\n# infer APOE status\ndat$APOEstatus = NA\ndat$APOEstatus[which(dat$APOEgeno == \"e4/e4\" | dat$APOEgeno == \"e3/e4\" | dat$APOEgeno == \"e2/e4\" | dat$APOEgeno == \"e1/e4\")] = \"e4Allele\"\ndat$APOEstatus[which(dat$APOEgeno == \"e3/e3\" | dat$APOEgeno == \"e2/e3\" | dat$APOEgeno == \"e2/e2\" | dat$APOEgeno == \"e1/e2\")] = \"NOe4Allele\"\n\ntable(dat$APOEstatus, dat$APOEgeno)\n\n#              e1/e2  e1/e4  e2/e2  e2/e3  e2/e4  e3/e3  e3/e4  e4/e4\n#  e4Allele        0     13      0      0   8818      0  82089   8266\n#  NOe4Allele      2      0   1984  42269      0 199277      0      0\n\n\n# keep only 2 columns\ndat = dat[,c(\"FID\", \"APOEgeno\", \"APOEstatus\")]\n\nwrite.table(dat, paste0(save, \"/UKB_APOE_Nov2023.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#type-2-diabetes",
    "href": "UKB_pheno.html#type-2-diabetes",
    "title": "UKB: Phenotypic data preparation",
    "section": "Type 2 diabetes",
    "text": "Type 2 diabetes\n\n\nCode\n## dementia is defined based on multiple sources of information we have in the UKB\n## 1. Self-reported illness (field ID 20002) & self-reported diagnosis by doctor (2443)\n## 2. ICD9 diagnoses (field ID 41203 & 41205)\n## 3. ICD10 diagnoses (field ID 41202 & 41204)\n## 4. Cause of death (field ID 40001)\n\n#########################################\n## 1. Self-reported illness (field IDs 20002, 2443, 2986, 2976, 21003) \n#########################################\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n\n#### field ID 20002\n####################\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.20002\", names(file))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n# 113066\n\n# determine codes of interest to find in field ID 41270\nt2d_codes <- c(\"1233\",\"1220\")\n\n# match ICDcodes with entries in all f.20002 variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, t2d_codes)))\n\n# if 1 or more entries, consider a case\nt2d$t2d20002 = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2d20002[allMissing] = NA\n\n# append final column to file\nfile$t2d20002 = t2d$t2d20002\n\n#### field 2443\n#### diabetes self-report interview touchscreen \n#################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.2443\", names(file))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# remove all minus (-) values as they stand for \"do not know\" or \"Prefer not to answer\"\nt2d[t2d < 0] = NA\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n\n# determine codes of interest to find in field \ndiab_code <- \"1\"\n\n# match codes with entries in all variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, diab_code)))\n\n# if 1 or more entries, consider a case\nt2d$t2d2443 = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2d2443[allMissing] = NA\n\n# append final column to file\nfile$t2d2443 = t2d$t2d2443\n\n##################################################\n## 4. Cause of death (field ID 40001)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.40001\", names(file))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(t2d)[grep(\"f.40001\", names(t2d))]){\n  t2d[which(t2d[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ndiab_code <- c(\"E110\",\"E111\",\"E112\",\"E113\",\"E114\",\"E115\",\"E116\",\"E117\",\"E118\",\"E119\")\n\n# match codes with entries in all variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, diab_code)))\n\n# if 1 or more entries, consider a case\nt2d$t2dDeath = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2dDeath[allMissing] = NA\n\n# append final column to file\nfile$t2dDeath = t2d$t2dDeath\n\n# intermediate save data \nsave = file[,c(\"f.eid\", \"t2d20002\", \"t2d2443\", \"t2dDeath\")]\n\n############################################\n## 2. ICD10 diagnoses (field ID 41202 & 41204)\n############################################\n### read in new data; basket 675090 has newer data for these variables \n\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41202\", names(file)), grep(\"f.41204\", names(file)))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(t2d)[c(grep(\"f.41202\", names(t2d)), grep(\"f.41204\", names(t2d)))]){\n  t2d[which(t2d[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ndiab_code <- c(\"E110\",\"E111\",\"E112\",\"E113\",\"E114\",\"E115\",\"E116\",\"E117\",\"E118\",\"E119\")\n\n# match codes with entries in all variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, diab_code)))\n\n# if 1 or more entries, consider a case\nt2d$t2dICD41202_4 = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2dICD41202_4[allMissing] = NA\n\n# append final column to file\nfile$t2dICD41202_4 = t2d$t2dICD41202_4\n\n\n##################################################\n## 3. ICD9 diagnoses (field ID 41203 & 41205)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41203\", names(file)), grep(\"f.41205\", names(file)))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(t2d)[c(grep(\"f.41203\", names(t2d)), grep(\"f.41205\", names(t2d)))]){\n  t2d[which(t2d[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ndiab_code <- c(\"25000\",\"25010\",\"25020\",\"25090\")\n\n# match codes with entries in all variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, diab_code)))\n\n# if 1 or more entries, consider a case\nt2d$t2dICD41203_5 = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2dICD41203_5[allMissing] = NA\n\n# append final column to file\nfile$t2dICD41203_5 = t2d$t2dICD41203_5\n\n\n######################################\n## Combine all t2d\n######################################\n# merge with saved columns above\nfile = merge(file, save, by = \"f.eid\")\n\n# keep columns of interest only\nfile = as.data.frame(file[, c(\"f.eid\", \"t2d20002\", \"t2d2443\", \"t2dICD41202_4\", \"t2dICD41203_5\", \"t2dDeath\")])\n\n# identify those with all missing\nallMissing = which(rowSums(!is.na(file[,-which(names(file) == \"f.eid\")])) == 0) \n\n# identify cases\nfile$t2d = 0 \n\n# identify cases\nfile$t2d[which(file$t2d20002 == 1 |\n                file$t2d2443 == 1 |\n                file$t2dICD41202_4 == 1 |\n                file$t2dICD41203_5 == 1 |\n                file$t2dDeath == 1)] = 1\n\n# delete allMissing from controls\nfile$t2d[allMissing] = NA\n\n# make factor\nfile$t2d = as.factor(file$t2d)\n\n#     0      1 \n# 451786  50436\n\n# save final variable to a seperate file\nfwrite(file[,c(\"f.eid\", \"t2d\")], file = paste0(out, \"/UKB_t2dStatus.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#hypertension",
    "href": "UKB_pheno.html#hypertension",
    "title": "UKB: Phenotypic data preparation",
    "section": "Hypertension",
    "text": "Hypertension\n\n\nCode\n# Hypertension is defined based on multiple field IDs\n## 1. Self-reported illness (field ID 20002) & self-reported diagnosis by doctor (2443)\n## 2. ICD9 diagnoses (field ID 41203 & 41205)\n## 3. ICD10 diagnoses (field ID 41202 & 41204)\n## 4. Cause of death (field ID 40001)\n\n#########################################\n## 1. Self-reported illness (field IDs 20002, 2443, 2986, 2976, 21003) \n#########################################\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n#### field ID 20002\n####################\n# list columns of interest\nid = which(names(file) == \"f.eid\")\nCols = grep(\"f.20002\", names(file))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(hyp[,-which(names(hyp) == \"f.eid\")])) == 0) \n# 113066\n\n# determine codes of interest to find in field ID 41270\nhyp_codes <- c(\"1065\",\"1072\")\n\n# match ICDcodes with entries in all f.20002 variables filtered in 't2d'\nhyp$countCodes = rowSums(!is.na(sapply(hyp[,-which(names(hyp) == \"f.eid\")], match, hyp_codes)))\n\n# if 1 or more entries, consider a case\nhyp$hyp20002 = as.factor(ifelse(hyp$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nhyp$hyp20002[allMissing] = NA\n\n# append final column to file\nfile$hyp20002 = hyp$hyp20002\n\n#### field 131287\n#### Source of report of I10 \n#################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.131287\", names(file))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(is.na(hyp[,which(names(hyp) != \"f.eid\")])) \n\n# determine participants with non missing \nallPresent = which(!is.na(hyp[,which(names(hyp) != \"f.eid\")]))\n\n# consider non-missing a case\nhyp$hyp131287 = NA\nhyp$hyp131287[allPresent] = 1\n\n# append final column to file\nfile$hyp131287 = hyp$hyp131287\n\n##################################################\n## 4. Cause of death (field ID 40001)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.40001\", names(file))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(hyp)[grep(\"f.40001\", names(hyp))]){\n  hyp[which(hyp[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(hyp[,-which(names(hyp) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \nhyp_code <- c(\"I10\")\n\n# match codes with entries in all variables filtered in 'hyp'\nhyp$countCodes = rowSums(!is.na(sapply(hyp[,-which(names(hyp) == \"f.eid\")], match, hyp_code)))\n\n# if 1 or more entries, consider a case\nhyp$hypDeath = as.factor(ifelse(hyp$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nhyp$hypDeath[allMissing] = NA\n\n# append final column to file\nfile$hypDeath = hyp$hypDeath\n\n# intermediate save data \nsave = file[,c(\"f.eid\", \"hyp20002\", \"hyp131287\", \"hypDeath\")]\n\n############################################\n## 2. ICD10 diagnoses (field ID 41202 & 41204)\n############################################\n### read in new data; basket 675090 has newer data for these variables \n\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41202\", names(file)), grep(\"f.41204\", names(file)))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(hyp)[c(grep(\"f.41202\", names(hyp)), grep(\"f.41204\", names(hyp)))]){\n  hyp[which(hyp[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(hyp[,-which(names(hyp) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \nhyp_code <- c(\"I10\")\n\n# match codes with entries in all variables filtered in 't2d'\nhyp$countCodes = rowSums(!is.na(sapply(hyp[,-which(names(hyp) == \"f.eid\")], match, hyp_code)))\n\n# if 1 or more entries, consider a case\nhyp$hypICD41202_4 = as.factor(ifelse(hyp$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nhyp$hypICD41202_4[allMissing] = NA\n\n# append final column to file\nfile$hypICD41202_4 = hyp$hypICD41202_4\n\n\n##################################################\n## 3. ICD9 diagnoses (field ID 41203 & 41205)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41203\", names(file)), grep(\"f.41205\", names(file)))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(hyp)[c(grep(\"f.41203\", names(hyp)), grep(\"f.41205\", names(hyp)))]){\n  hyp[which(hyp[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(hyp[,-which(names(hyp) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \nhyp_code <- c(\"401\",\"4010\",\"4011\",\"4019\")\n\n# match codes with entries in all variables filtered in 't2d'\nhyp$countCodes = rowSums(!is.na(sapply(hyp[,-which(names(hyp) == \"f.eid\")], match, hyp_code)))\n\n# if 1 or more entries, consider a case\nhyp$hypICD41203_5 = as.factor(ifelse(hyp$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nhyp$hypICD41203_5[allMissing] = NA\n\n# append final column to file\nfile$hypICD41203_5 = hyp$hypICD41203_5\n\n\n######################################\n## Combine all hypertension variables \n######################################\n# merge with saved columns above\nfile = merge(file, save, by = \"f.eid\")\n\n# keep columns of interest only\nfile = as.data.frame(file[, c(\"f.eid\", \"hyp20002\", \"hyp131287\", \"hypDeath\", \"hypICD41202_4\", \"hypICD41203_5\")])\n\n# identify those with all missing\nallMissing = which(rowSums(!is.na(file[,-which(names(file) == \"f.eid\")])) == 0) \n\n# identify cases\nfile$hyp = 0 \n\n# identify cases\nfile$hyp[which(file$hyp20002 == 1 |\n                file$hyp131287 == 1 |\n                file$hypDeath == 1 |\n                file$hypICD41202_4 == 1 |\n                file$hypICD41203_5 == 1)] = 1\n\n\n# delete allMissing from controls\nfile$hyp[allMissing] = NA\n\n# make factor\nfile$hyp = as.factor(file$hyp)\n\n#     0      1 \n# 277454 204678 \n\n# save final variable to a seperate file\nfwrite(file[,c(\"f.eid\", \"hyp\")], file = paste0(out, \"/UKB_HypertensionStatus.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#smoking-packyears",
    "href": "UKB_pheno.html#smoking-packyears",
    "title": "UKB: Phenotypic data preparation",
    "section": "Smoking (packyears)",
    "text": "Smoking (packyears)\n\n\nCode\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\n# using instance at the time of initial neuroimaging visit\n# also keep \"ever smoked\" variable to determine zeros in data set\n# no need to consider variable f.20160_2.0 because when it's 0, it's also 0 in packyears \nCols = grep(\"f.20161_2.0\", names(file))\n\n# select columns of interest\nsmok = as.data.frame(file[, c(..id, ..Cols)])\n\n# rename var\nnames(smok)[which(names(smok) == \"f.20161_2.0\")] = \"packyears\"\n\n# save variable\nfwrite(smok[,c(\"f.eid\", \"packyears\")], file = paste0(out, \"/UKB_packyears.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#body-mass-index",
    "href": "UKB_pheno.html#body-mass-index",
    "title": "UKB: Phenotypic data preparation",
    "section": "Body mass index",
    "text": "Body mass index\n\n\nCode\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid = which(names(file) == \"f.eid\")\n# normal BMI item f.21001.0.0\n# impedance BMI item f.97777.0.0\nCols = c(grep(\"f.21001_2.0\", names(file)), grep(\"f.23104_2.0\", names(file)))\n\n# select columns of interest\nbmi = as.data.frame(file[, c(..id, ..Cols)])\n\n# Yaghootkar, 2016: “We excluded individuals with differences\n#>.4.56 SDs between impedance and normal BMI measures where both \n# variables were available\"\n\n## Calculate difference between f.21001.0.0 and f.23104.0.0 for each individual\nbmi$BMI_diff <- bmi$f.21001.0.0 - bmi$f.23104.0.0\n\n# Calculate mean and SD of the difference between f.21001.0.0 and f.23104.0.0\nmean_diff <- mean(bmi$BMI_diff,na.rm=T)\nSD_diff <- sd(bmi$BMI_diff,na.rm=T)\n\n# Calculate the mean of BMI measures for each individual\nbmi$bmi <- rowMeans(bmi[, c(\"f.21001_2.0\", \"f.23104_2.0\")], na.rm = T)\n\n# Set BMI_mean to NA for individuals with BMI_diff ±4.56 SD from the mean\nbmi$bmi[which(bmi$BMI_diff < mean_diff - (SD_diff*4.56) | bmi$BMI_diff > mean_diff + (SD_diff*4.56))]<-NA\n\n# save final variable to a seperate file\nfwrite(bmi[,c(\"f.eid\", \"bmi\")], file = paste0(out, \"/UKB_bmi.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#stroke",
    "href": "UKB_pheno.html#stroke",
    "title": "UKB: Phenotypic data preparation",
    "section": "Stroke",
    "text": "Stroke\n\n\nCode\n# multiple field IDs used to identify stroke\n# 1. Self-reported illness: field ID 200002\n# 2. Cause of death: field ID 40001\n# 3. Source of stroke report: field ID 42007\n# 4. ICd9 diagnoses: field IDs 41203 & 41205\n# 5. ICD10 diagnoses: field IDs 41202 & 41206\n\n#########################################\n## 1. Self-reported illness (field IDs 20002, 2443, 2986, 2976, 21003) \n#########################################\npath=\"/UK_Biobank_New/Data/Raw_Data/1027_Refresh_Dec_2022/670476\"\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n\n#### field ID 20002 (code 1081 & 1583)\n####################\n\n# list columns of interest\nid = which(names(file) == \"f.eid\")\nCols = grep(\"f.20002\", names(file))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n# 113066\n\n# determine codes of interest to find in field ID 41270\ncodes <- c(1081,1086,1491,1583)\n\n# match ICDcodes with entries in all f.20002 variables filtered in 't2d'\nstro$countCodes = rowSums(!is.na(sapply(stro[,-which(names(stro) == \"f.eid\")], match, codes)))\n\n# if 1 or more entries, consider a case\nstro$stro20002 = as.factor(ifelse(stro$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nstro$stro20002[allMissing] = NA\n\n# append final column to file\nfile$stro20002 = stro$stro20002\n\n#### field 42007\n#### Source of report of stroke\n#################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.42007\", names(file))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n\n# determine participants with non missing - consider them cases\nallPresent = which(!is.na(stro[,which(names(stro) != \"f.eid\")]))\n\n# consider non-missing a case (but missing doesn't mean they don\nstro$stro42007 = 0\nstro$stro42007[allPresent] = 1\nstro$stro42007[allMissing] = NA\n\nstro$stro42007 = as.factor(stro$stro42007)\n\n# append final column to file\nfile$stro42007 = stro$stro42007\n\n############################################\n# 2. Cause of Death (field ID 40001)\n############################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.40001\", names(file))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(stro)[grep(\"f.40001\", names(stro))]){\n  stro[which(stro[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ncodes <- c(\"I600\",\"I601\",\"I602\",\"I603\",\"I604\",\"I605\",\"I606\",\"I607\",\"I608\",\"I609\",\"I61\",\"I610\",\"I611\",\"I612\",\"I613\",\"I614\",\"I615\",\"I616\",\"I618\",\"I619\",\"I63\",\"I630\",\"I631\",\"I632\",\"I633\",\"I634\",\"I635\",\"I636\",\"I638\",\"I639\",\"I64\")\n\n\n# match codes with entries in all variables filtered in 'hyp'\nstro$countCodes = rowSums(!is.na(sapply(stro[,-which(names(stro) == \"f.eid\")], match, codes)))\n\n# if 1 or more entries, consider a case\nstro$stroDeath = as.factor(ifelse(stro$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nstro$stroDeath[allMissing] = NA\n\n# append final column to file\nfile$stroDeath = stro$stroDeath\n\n# intermediate save\nsave = file[,c(\"f.eid\", \"stro20002\", \"stro42007\", \"stroDeath\")]\n\n############################################\n## 2. ICD10 diagnoses (field ID 41202 & 41204)\n############################################\n### read in new data; basket 675090 has newer data for these variables \n\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41202\", names(file)), grep(\"f.41204\", names(file)))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(stro)[c(grep(\"f.41202\", names(stro)), grep(\"f.41204\", names(stro)))]){\n  stro[which(stro[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \nICDcode <- c(\"I600\",\"I601\",\"I602\",\"I603\",\"I604\",\"I605\",\"I606\",\"I607\",\"I608\",\"I609\",\"I61\",\"I610\",\"I611\",\"I612\",\"I613\",\"I614\",\"I615\",\"I616\",\"I618\",\"I619\",\"I63\",\"I630\",\"I631\",\"I632\",\"I633\",\"I634\",\"I635\",\"I636\",\"I638\",\"I639\",\"I64\")\n\n# match codes with entries in all variables filtered in 't2d'\nstro$countCodes = rowSums(!is.na(sapply(stro[,-which(names(stro) == \"f.eid\")], match, ICDcode)))\n\n# if 1 or more entries, consider a case\nstro$stroICD41202_4 = as.factor(ifelse(stro$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nstro$stroICD41202_4[allMissing] = NA\n\n# append final column to file\nfile$stroICD41202_4 = stro$stroICD41202_4\n\n\n##################################################\n## 3. ICD9 diagnoses (field ID 41203 & 41205)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41203\", names(file)), grep(\"f.41205\", names(file)))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(stro)[c(grep(\"f.41203\", names(stro)), grep(\"f.41205\", names(stro)))]){\n  stro[which(stro[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ncodes <- c(\"4309\",\"4319\",\"4340\",\"4341\",\"4349\",\"4369\")\n\n# match codes with entries in all variables filtered in 't2d'\nstro$countCodes = rowSums(!is.na(sapply(stro[,-which(names(stro) == \"f.eid\")], match, codes)))\n\n# if 1 or more entries, consider a case\nstro$stroICD41203_5 = as.factor(ifelse(stro$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nstro$stroICD41203_5[allMissing] = NA\n\n# append final column to file\nfile$stroICD41203_5 = stro$stroICD41203_5\n\n\n######################################\n## Combine all hypertension variables \n######################################\n\n# merge with saved columns above\nfile = merge(file, save, by = \"f.eid\")\n\n# keep columns of interest only\nfile = as.data.frame(file[, c(\"f.eid\", \"stro20002\", \"stro42007\", \"stroDeath\", \"stroICD41202_4\", \"stroICD41203_5\")])\n\n# identify those with all missing\nallMissing = which(rowSums(!is.na(file[,-which(names(file) == \"f.eid\")])) == 0) \n\n# identify cases\nfile$stroke = 0 \n\n# identify cases\nfile$stroke[which(file$stro20002 == 1 |\n                file$stro42007 == 1 |\n                file$stroDeath == 1 |\n                file$stroICD41202_4 == 1 |\n                file$stroICD41203_5 == 1)] = 1\n\n\n# delete allMissing from controls\nfile$stroke[allMissing] = NA\n\n# make factor\nfile$stroke = as.factor(file$stroke)\n\n#     0      1 \n# 461708  20197 \n\n# save final variable to a seperate file\nfwrite(file[,c(\"f.eid\", \"stroke\")], file = paste0(out, \"/UKB_StrokeStatus.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")\n\n\n\n\nCode\n#### test overlap with neuroimaging sample\n# read in file that holds neuroimaging IDs\nIDfile= list.files(path = path, pattern = \"AllAvailLong\")\n\nallNeuro = read.table(paste0(path, IDfile))\nnames(allNeuro)[1] = \"f.eid\"\n\n# read in data\nstroke = fread(paste0(out, \"/UKB_StrokeStatus.txt\"))\n\n# merge with data\nstroke = merge(stroke, allNeuro, by = \"f.eid\", all.y=T)\n\ntable(stroke$stroke)"
  },
  {
    "objectID": "UKB_pheno.html#fried-frailty-definition",
    "href": "UKB_pheno.html#fried-frailty-definition",
    "title": "UKB: Phenotypic data preparation",
    "section": "Fried frailty definition",
    "text": "Fried frailty definition\n\n\nCode\n## Fried fraitly definition\n# Weight loss: 2306\n# Exhaustion: 2080\n# Weakness: 46,47\n# Physical activity: 6164, 1011\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# R doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# Weight loss: 2306 (neuroimaging visit)\n# Coding\n# -3 = Prefer not to say (NA)\n# -1 = Do not know (0)\n# 0 = no weight loss (0)\n# 2 = gained weight (0)\n# 3 = lost weight (1)\nid = which(names(file) == \"f.eid\")\nCols = grep(\"2306_2.\", names(file))\n\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# assign categories as in Jiang et al \nsub$f.2306_2.0[which(sub$f.2306_2.0 == -3)] = NA\nsub$f.2306_2.0[which(sub$f.2306_2.0 == -1)] = 0\nsub$f.2306_2.0[which(sub$f.2306_2.0 == 0)] = 0\nsub$f.2306_2.0[which(sub$f.2306_2.0 == 2)] = 0\nsub$f.2306_2.0[which(sub$f.2306_2.0 == 3)] = 1\n\n# intermediate save\nfile$weightLoss = sub$f.2306_2.0\n\n# Exhaustion: 2080\n# Coding in Jiang et al\n# -3 = Prefer not to answer (NA)\n# -1 = Don't know (0)\n# 1 = Not at all (0)\n# 2 = Several days (0)\n# 3 = More than half the days (1)\n# 4 = Nearly every day (1)\nid = which(names(file) == \"f.eid\")\nCols = grep(\"f.2080_2.\", names(file))\n\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# assign categories as in Jiang et al \nsub$f.2080_2.0[which(sub$f.2080_2.0 == -3)] = NA\nsub$f.2080_2.0[which(sub$f.2080_2.0 == -1)] = 0\nsub$f.2080_2.0[which(sub$f.2080_2.0 == 1)] = 0\nsub$f.2080_2.0[which(sub$f.2080_2.0 == 2)] = 0\nsub$f.2080_2.0[which(sub$f.2080_2.0 == 3)] = 1\nsub$f.2080_2.0[which(sub$f.2080_2.0 == 4)] = 1\n\n# intermediate save\nfile$exhaustion = sub$f.2080_2.0\n\n# Walking speed (924)\n# -7 = None of the above (NA)\n# -3 = Prefer not to answer (NA)\n# 1 = Slow pace (1)\n# 2 = Steady average pace (0)\n# 3 = Brisk pace (0)\nid = which(names(file) == \"f.eid\")\nCols = grep(\"f.924_2.\", names(file))\n\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# assign categories as in Jiang et al \nsub$f.924_2.0[which(sub$f.924_2.0 == -7)] = NA\nsub$f.924_2.0[which(sub$f.924_2.0 == -3)] = NA\nsub$f.924_2.0[which(sub$f.924_2.0 == 1)] = 1\nsub$f.924_2.0[which(sub$f.924_2.0 == 2)] = 0\nsub$f.924_2.0[which(sub$f.924_2.0 == 3)] = 0\n\n# intermediate save\nfile$walkingSpeed = sub$f.924_2.0\n\n# Weakness (grip strength) field ID average of 46,47\n# Cut offs\n# Males: \n# BMI <= 24 & grip strength <=29\n# 24.1<=BMI<=28 & grip strength <= 30\n# BMI > 28 & grip strength <= 32\n# Females:\n# BMI <= 23 & grip <=17\n# 23.1<=BMI<=26 & grip <=17.3\n# 26.1<=BMI & grip <=18\n# BMI>29 & grip<=21\n\n# select columns\nid = which(names(file) == \"f.eid\")\nCols = c(grep(\"f.46_2.\", names(file)), grep(\"f.47_2.\", names(file)), grep(\"f.21001_2.0\", names(file)), grep(\"f.23104_2.0\", names(file)), grep(\"f.31_\", names(file)))\n# subset data\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# get average bmi between two indepdendent measures\nsub$bmi <- rowMeans(sub[, c(\"f.21001_2.0\", \"f.23104_2.0\")], na.rm = T)\n\n# get average grip strength between left and right hand\nsub$grip <- rowMeans(sub[, c(\"f.46_2.0\", \"f.47_2.0\")], na.rm = T)\n\n# create new variable to hold weakness categories\nsub$weakness = 0\n\n# identify participants who have missing bmi and missing grip as we can't determine measures for them\nallMissing = which(rowSums(!is.na(sub[,c(\"bmi\", \"grip\")])) == 0) \n\n# assign categories\n## Males: field ID f.31 is 1 for male\nmales = subset(sub, f.31_0.0 == 1)\nmales$weakness[which(males$bmi <= 24 & males$grip <=29)] = 1\nmales$weakness[which(males$bmi >= 24.1 & males$bmi <= 28 & males$grip <= 30)] = 1\nmales$weakness[which(males$bmi > 28 & males$grip <=32)] = 1\n\n## Females: f.31 is 0 for female\nfemales = subset(sub, f.31_0.0 == 0)\nfemales$weakness[which(females$bmi <= 23  & females$grip <= 17)] = 1\nfemales$weakness[which(females$bmi >= 23.1 & females$bmi <= 26 & females$grip <= 17.3)] = 1\nfemales$weakness[which(females$bmi >= 26.1 & females$bmi <= 29 & females$grip <= 18)] = 1\nfemales$weakness[which(females$bmi > 29 & females$grip <= 21)] = 1\n\n# combine males and females into 1 dataset\nboth = rbind(males, females)\n\n# merge weakness variable back with other variables\nfile = merge(file, both[,c(\"f.eid\", \"weakness\")], by = \"f.eid\")\n\n# remove participants with all missing info\nfile$weakness[allMissing] = NA\n\n# Physical activity field ID average of 6164\n## 1 = Walking for pleasure (0)\n## 2 = Other exercises (0)\n## 3 = Strenuous sports (0)\n## 4 = Light DIY (0 = more than once per week, 1 = once per weeks or less)\n## 5 = Heavy DIY (0)\n## -7 = None of the above (1)\n## -3 = Prefer not to answer (NA) \n\n# Frequency of light DIY in the 4 weeks, 1011\n## 1 = Once in the last 4 weeks\n## 2 = 2-3 times in the last4 weeks\n## 3 = Once a week\n## 4 = 2-3 times a week\n## 5 = 4-5 times a week\n## 6 = every day\n## -1 = do not know \n## -3 = prefer not to answer \n\n# select columns\nid = which(names(file) == \"f.eid\")\nCols = c(grep(\"f.6164_2.\", names(file)), grep(\"f.1011_2\", names(file)))\n\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify participants who have indicated 'none of the above' (assigned value: 1)\nNoPhysAct = which(sub$f.6164_2.0 == -7)\n\n# we also assign value 1 to people who indicate to do light DIY once per week or less\nNoPhysAct = c(NoPhysAct, which(sub$f.6164_2.0 == 4 & sub$f.1011_2.0 == 1 | sub$f.6164_2.0 == 4 & sub$f.1011_2.0 == 2 | sub$f.6164_2.0 == 4 & sub$f.1011_2.0 == 3))\n\n# identify participants with all missing data (assigned value: NAs)\nallMissing = which(rowSums(!is.na(sub[,grep(\"6164\", names(sub))])) == 0) \n\n# create variable where all participants have assigned value 0\nsub$physicalActiv = 0 \n\n# identify those with No Physical Activity\nsub$physicalActiv[NoPhysAct] = 1\n\n# identify those with all missing\nsub$physicalActiv[allMissing] = NA\n\n# temporary save\nfile$physicalActiv = sub$physicalActiv\n\n### get overall sumscore\nfrail = file[,c(\"f.eid\",\"weightLoss\", \"exhaustion\", \"walkingSpeed\", \"weakness\", \"physicalActiv\")]\nfrail$FriedFrailty = rowSums(frail[,c(\"weightLoss\", \"exhaustion\", \"walkingSpeed\", \"weakness\", \"physicalActiv\")], na.rm = F)\n# hist(frail$FriedFrailty)\n\n# save file \nfwrite(frail[,c(\"f.eid\", \"FriedFrailty\")], file = paste0(out, \"/UKB_FriedFrailty.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#brain-age",
    "href": "UKB_pheno.html#brain-age",
    "title": "UKB: Phenotypic data preparation",
    "section": "Brain Age",
    "text": "Brain Age\nBrain age was calculated with brainageR v2 using the Docker file available on GitHub, which is performed on each individual NIFTI file and then saved in separate text files.\n\n\nCode\nsudo docker run --rm -it -v ${WD}:/data -w /data docker.io/library/brainimage:latest /bin/bash\n\nDAT=\"/data/temp\"\nIDinfo=\"/data/scripts/UKBlong\"\n\n# read in participant IDs\nall=$(cat ${IDinfo}/AllAvailIDs_UKB15112023.txt)\n\n# run brainageR\nfor i in $all\ndo\n\necho $i\n\n    if [ ! -f /data/data/UKB_BrainAge/${i}_brain_predicted.age.csv ]\n    then\n\n        brainageR -f ${DAT}/BrainAge/${i}_T1_orig_defaced.nii -o /data/data/UKB_BrainAge/${i}_brain_predicted.age.csv\n\n    fi\n\ndone\n\n\n\nFormat individual-level data\nWe found that the average brain age predictions were ~15 years younger than chronological age, and this seems to be a consensus across other research groups who attempted to extract brain age from UKB neuroimaging data. To center brain age around zero, we deducted the sample brain age mean value from each individual value before calculating the brain age gap, which should not affect the interindividual variability we’re modeling.\n\n\nCode\n# identify csv files saved in Brain Age processing directory\nfiles = list.files(path = target, pattern = \"_T1_brain_predicted.age.csv\")\nfiles = paste0(target, \"/\",files)\n\n# object to hold brain age info\nsave = data.frame()\n\n# cycle through all files and save info\nfor(i in files){\n  # read in file\n  file = read.csv(i, header = T)\n  \n  # store info in object \"save\"\n  save = rbind(save, file)\n  \n}\n\n# re-name File column to subject ID\nsave$File = stringr::str_remove(save$File, pattern = \"_T1_orig_defaced\")\n\n# re-name ID column\nnames(save)[which(names(save) == \"File\")] = \"f.eid\"\n\n# rename\nsave$f.eid = stringr::str_remove(save$f.eid, pattern = \"_orig_defaced\")\n\nwrite.table(save, file = \"/CCACE_Shared/Anna_F/BrainAtrophy/data/UKB_BrainAge_T1_orig_defaced.txt\", col.names = T, row.names = F, quote = F, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#merge-all-ukb-phenotypes-into-one-file",
    "href": "UKB_pheno.html#merge-all-ukb-phenotypes-into-one-file",
    "title": "UKB: Phenotypic data preparation",
    "section": "Merge all UKB phenotypes into one file",
    "text": "Merge all UKB phenotypes into one file\n\n\nCode\n#list.files(path = wd, pattern = \"UKB_\")\n# it makes it more straightforward to conduct the following analyses if I merge all phenotypes into one file\n# Step 1: Read all phenotypes in\n# Step 2: Merge them\n# Step 3: Save\n\n# cognitive ability\ncog = fread(paste0(wd, \"/UKB_gFactor.txt\"))\n# dementia\ndement = fread(paste0(wd, \"/UKB_DementiaStatus.txt\"))\ndement$dement = as.factor(dement$dement)\n# APOE\nAPOE = fread(paste0(wd, \"/UKB_APOE_Nov2023.txt\"))\nnames(APOE)[which(names(APOE) == \"FID\")] = \"f.eid\"\nAPOE = APOE[,c(\"f.eid\", \"APOEstatus\")]\nAPOE$APOEstatus[which(APOE$APOEstatus == \"e4Allele\")] = 1\nAPOE$APOEstatus[which(APOE$APOEstatus == \"NOe4Allele\")] = 0\nAPOE$APOEstatus = as.factor(APOE$APOEstatus)\n\n# Frailty\nfrail = fread(paste0(wd, \"/UKB_FriedFrailty.txt\"))\n# diabetes\ndiab = fread(paste0(wd, \"/UKB_t2dStatus.txt\"))\ndiab$t2d = as.factor(diab$t2d)\n# hyp\nhyp = fread(paste0(wd, \"/UKB_HypertensionStatus.txt\"))\nhyp$hyp = as.factor(hyp$hyp)\n# packyears\nsmok = fread(paste0(wd, \"/UKB_packyears.txt\"))\n# bmi\nbmi = fread(paste0(wd, \"/UKB_bmi.txt\"))\n# brain age\nBrainAge = fread(paste0(wd, \"/UKB_BrainAge_T1_orig_defaced.txt\"))\n## get brain age gap: chronological age minus brain predicted age \nage = fread(paste0(wd, \"/UKB_covarGWAS.txt\"))\nnames(age)[grepl(\"FID\", names(age))] <- \"f.eid\"\nBrainAge = merge(BrainAge, age[,c(\"f.eid\", \"age\")], by = \"f.eid\")\nBrainAge$chronAge <- BrainAge$age / 12\nBrainAge$BrainAgeGap <- BrainAge$chronAge - BrainAge$brain.predicted_age\n# deducting mean age to be centered around zero\nBrainAge$brainAge <- BrainAge$BrainAgeGap - mean(BrainAge$BrainAgeGap)\nBrainAge = BrainAge[,c(\"f.eid\", \"brainAge\")]\n\n# stroke\nstroke = fread(paste0(wd, \"/UKB_StrokeStatus.txt\"))\nstroke$stroke = as.factor(stroke$stroke)\n\n# merge data\nDatList = list(cog, dement, APOE, frail, diab, hyp, smok, bmi, stroke, BrainAge)\nUKB_merged = Reduce(function(x,y) merge(x, y, by = \"f.eid\", all = T), DatList)\n\n# restrict data set to neuroimaging participants \nIDfile= list.files(path = path, pattern = \"AllAvailLong\")\n\nallNeuro = read.table(paste0(path, IDfile))\nnames(allNeuro)[1] = \"f.eid\"\n\nUKB_merged = UKB_merged[UKB_merged$f.eid %in% allNeuro$f.eid,]\n\n# choose prettier names\nnames(UKB_merged) = c(\"f.eid\",\"cog\",\"dementia\",\"APOEe4\",\"frailty\",\"diabetes\",\"hypertension\",\"packyears\",\"BMI\",\"stroke\", \"brainAge\")\n\n# write\nfwrite(UKB_merged, file = paste0(out, \"/UKB_allPheno.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  }
]