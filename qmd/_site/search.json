[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lifetime brain atrophy GWAS",
    "section": "",
    "text": "Here I display the analysis code to accompany our lifetime brain atrophy (LBA) genetics project. For consistency, the analysis steps outlined below match the order of presentation in the manuscript but does not necessarily reflect the order in which analyses were executed.\nPre-registration: https://osf.io/gydmw/\n\nAnalysis steps\nData preparation\n\nUK Biobank\n1.1. Phenotypic data\n1.2. Neuroimaging data\n1.3. Genetic data\nLBC1936\n2.1. Phenotypic data\n2.2. Neuroimaging data\nHuman Connectome Project (HCP)\nSTRADL\nMRi-Share\n\nDescription and characterisation of the LBA phenotype\n\nMeasures of LBA predict brain atrophy rated by neuroradiological experts, as well as other ageing-related health traits such as frailty and cognitive ability\nMeasures of LBA indicate age-associated brain shrinkage\n2.1. Single time-point MRI measures age correlation\n2.2. Repeated MRI measures age correlations in LBC1936\nLBA moderately captures within-person atrophic changes that were longitudinally observed between two MRI scans a few years apart\n\nGenome-wide association study of LBA\n\nSNP-heritability\nGWAS associations\nGenetic correlations\nSNP-by-age interaction models\n\n\n\n\n\nGWAS Manhattan plot for LBA (residual score)"
  },
  {
    "objectID": "pheno_assocs.html",
    "href": "pheno_assocs.html",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "",
    "text": "Results produced by the code below are described in the manuscript under section:\nMeasures of LBA predict brain atrophy rated by neuroradiological experts, as well as other ageing-related health traits such as frailty and cognitive ability.\nThe outcome traits were derived using code displayed in ‘Data preparation’: LBC and UKB."
  },
  {
    "objectID": "pheno_assocs.html#lothian-birth-cohort-lbc-1936",
    "href": "pheno_assocs.html#lothian-birth-cohort-lbc-1936",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "Lothian Birth Cohort (LBC) 1936",
    "text": "Lothian Birth Cohort (LBC) 1936\n\nWave 2: Lifetime brain atrophy (LBA; cross-sectional)\n\n\nCode\n# read in LBC data \npheno = fread(paste0(wd, \"/LBC1936_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/LBC1936_crossNeuroWave1.txt\"), data.table = F)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"Stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"iCog\", \"sCog\", \"iFrailty\", \"sFrailty\", \"iBMI\", \"sBMI\", \"BrainAge\",\"ageMRI_w2\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$resid_stand = neuro$resid_stand*(-1)\nneuro$ratio_stand = neuro$ratio_stand*(-1)\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"VisualAtrophyDeep\", \"VisualAtrophySuperficial\"), modelType = c(\"linear\", \"linear\"))\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"diff_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"ratio_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"resid_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nraw2 = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nWave 5: LBA (cross-sectional)\n\n\nCode\npheno = fread(paste0(wd, \"/LBC1936_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/LBC1936_crossNeuroWave4.txt\"), data.table = F)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"Stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"iCog\", \"sCog\", \"iFrailty\", \"sFrailty\", \"iBMI\", \"sBMI\", \"BrainAge\",\"ageMRI_w2\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$resid_stand = neuro$resid_stand*(-1)\nneuro$ratio_stand = neuro$ratio_stand*(-1)\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"VisualAtrophyDeep\", \"VisualAtrophySuperficial\"), modelType = c(\"linear\", \"linear\"))\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"diff_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"ratio_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"resid_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nraw5 = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nLongitudinal data\n\n\nCode\n# read in LBC data \npheno = fread(paste0(wd, \"/LBC1936_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), data.table = F)\n\n# make sure binary variables are coded as factors\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"Stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"iCog\", \"sCog\", \"iFrailty\", \"sFrailty\", \"iBMI\", \"sBMI\", \"BrainAge\",\"ageMRI_w2\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$TBVresid_2to5_stand = neuro$TBVresid_2to5_stand*(-1)\nneuro$TBVratio_5to2_stand = neuro$TBVratio_5to2_stand*(-1)\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"VisualAtrophyDeep\", \"VisualAtrophySuperficial\"), modelType = c(\"linear\", \"linear\"))\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVdiff_2to5_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVratio_5to2_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVresid_2to5_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nrawObs = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nCode\n# save cross data\nraw2$wave = 2\nraw5$wave = 5\nraw = rbind(raw2, raw5)\nraw$purpose = \"Estimated Atrophy (cross-sectional)\"\nraw$purpose = \"Estimated Atrophy (cross-sectional)\"\nrawObs$purpose = \"Observed Atrophy (longitudinal)\"\nrawObs$wave = NA\nsave = rbind(raw, rawObs)\nfwrite(save, file = paste0(wd, \"/LBC1936_assocs_observed_vs_estimated_atrophy.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "pheno_assocs.html#uk-biobank-ukb",
    "href": "pheno_assocs.html#uk-biobank-ukb",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "UK Biobank (UKB)",
    "text": "UK Biobank (UKB)\n\nWave 2: LBA (cross-sectional)\n\n\nCode\n# read in LBC data \npheno = fread(paste0(wd, \"/UKB_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/UKB_neuroNoLongProcess.txt\"), data.table = F)\n# get wave of interest\nneuro = neuro[which(neuro$wave == 2),]\n\n# edit:30/09/2024 (realised later that I hadn't cleaned the longitudinal data the same as the cross-sectional data, 10SD cutoff which some participants violate with the longitudinal data) - removed 6 participants\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand < 10),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand > (-10)),]\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$resid_stand = neuro$resid_stand*(-1)\nneuro$ratio_stand = neuro$ratio_stand*(-1)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"cog\", \"BMI\", \"brainAge\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"packyears\", \"frailty\"), modelType = c(\"hurdle\", \"hurdle\")) # can overwrite with logistic of linear\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"diff_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"ratio_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"resid_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nraw2 = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nWave 3: LBA (cross-sectional)\n\n\nCode\n# read in data \npheno = fread(paste0(wd, \"/UKB_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/UKB_neuroNoLongProcess.txt\"), data.table = F)\n# get wave of interest\nneuro = neuro[which(neuro$wave == 3),]\n# edit:30/09/2024 (realised later that I hadn't cleaned the longitudinal data the same as the cross-sectional data, 10SD cutoff which some participants violate with the longitudinal data) - removed 6 participants\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand < 10),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand > (-10)),]\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$resid_stand = neuro$resid_stand*(-1)\nneuro$ratio_stand = neuro$ratio_stand*(-1)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"cog\", \"BMI\", \"brainAge\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"packyears\", \"frailty\"), modelType = c(\"hurdle\", \"hurdle\")) # can overwrite with logistic of linear\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"diff_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"ratio_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"resid_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nraw5 = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nLongitudinal atrophy scores\n\n\nCode\n# read in data \npheno = fread(paste0(wd, \"/UKB_allPheno.txt\"), data.table = F)\nneuro = fread(paste0(wd, \"/UKB_neuroNoLongProcess.txt\"), data.table = F)\n# shouldn't make a difference which wave we keep because long is saved double\nneuro = neuro[which(neuro$wave == 2),]\n# edit:30/09/2024 (realised later that I hadn't cleaned the longitudinal data the same as the cross-sectional data, 10SD cutoff which some participants violate with the longitudinal data) - removed 6 participants\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVdiff_2to3_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand < 10),]\nneuro <- neuro[which(neuro$TBVratio_3to2_stand > (-10)),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand < 10),]\nneuro <- neuro[which(neuro$TBVresid_2to3_stand > (-10)),]\n\n# to ensure unified interpretation, I reverse code the ratio and residual score\nneuro$TBVresid_2to3_stand = neuro$TBVresid_2to3_stand*(-1)\nneuro$TBVratio_3to2_stand = neuro$TBVratio_3to2_stand*(-1)\n\n# make sure binary variables are coded as factors because the function will otherwise not recognise it as factor\nColNames=c(\"dementia\",\"APOEe4\",\"diabetes\",\"hypertension\",\"stroke\")\npheno[ColNames] = lapply(pheno[ColNames], as.factor)\n\n# make sure continuous variables are standardised (rawAssocAim3 deals with that)\n#ColNames=c(\"cog\", \"BMI\", \"brainAge\")\n#pheno[ColNames] = as.data.frame(scale(pheno[ColNames]))\n\n# calculate raw associations for all atrophy scores\noverwrite = data.frame(var = c(\"packyears\", \"frailty\"), modelType = c(\"hurdle\", \"hurdle\")) # can overwrite with logistic of linear\nrawDiff = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVdiff_2to3_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawRatio = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVratio_3to2_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\nrawResid = rawAssocAim3(pheno = pheno, neuro = neuro, neuroVar = \"TBVresid_2to3_stand\", numOutcomeStand = TRUE, overwrite = overwrite)\n\nrawObs = rbind(rawDiff, rawRatio, rawResid)\n\n\n\n\nCode\n# save cross data\nraw2$wave = 2\nraw5$wave = 3\nraw = rbind(raw2, raw5)\nraw$purpose = \"Estimated Atrophy (cross-sectional)\"\nraw$purpose = \"Estimated Atrophy (cross-sectional)\"\nrawObs$purpose = \"Observed Atrophy (longitudinal)\"\nrawObs$wave = NA\nsave = rbind(raw, rawObs)\nfwrite(save, file = paste0(wd, \"/UKB_assocs_observed_vs_estimated_atrophy.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "pheno_assocs.html#lbc1936",
    "href": "pheno_assocs.html#lbc1936",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "LBC1936",
    "text": "LBC1936\n\n\nCode\n# betas between atrophy scores and traits\nsetwd(wd)\nfile = list.files(pattern=\"LBC1936_assocs_observed_vs_estimated_atrophy\")\nassoc = fread(file)\n\n# brain age phenotype is not intuitively interpretable:\n# positive value should mean the participant has a healthier looking brain than expected given their age \n# negative value should mean the participant has an unhealthier looking brain than expected\n# Hence, here we just flip the assocs so that more LBA is associated with older brain age\nassoc$beta[which(assoc$Outcome == \"BrainAge\")] <- assoc$beta[which(assoc$Outcome == \"BrainAge\")]*(-1)\n\n# calculate lower and upper bounds of assoc\nassoc$ci_l = assoc$beta - (1.96*assoc$std.error)\nassoc$ci_u = assoc$beta + (1.96*assoc$std.error)\n\n# because beta shouldn't exceed 1, and it makes the plots look ugly, I will artificially reduce those here\nassoc$ci_l = ifelse(assoc$ci_l < -1.21, -1.21, assoc$ci_l)\nassoc$ci_u = ifelse(assoc$ci_u > 1.21, 1.21, assoc$ci_u)\n\n# make data frame for geom_label\nassoc$label = paste0(round(assoc$R2_percent, digits = 1), \"%\")\n# only print the label is correlation is significant\nsigBonferroni = 0.05/length(unique(assoc$Outcome))\nassoc$label[which(assoc$p.value > sigBonferroni)] = NA\n\n\n######################################################################################\n###### Continuous traits\n######################################################################################\n############## ESTIMATED ATROPHY\n### first, only estimated atrophy\n# only keep 'diff_stand', 'ratio_stand' or 'resid_stand'\nestimated = assoc[grepl(\"diff_stand|ratio_stand|resid_stand\",assoc$Predictor),]\n# restrict to wave 5 (more atrophy)\nestimated = estimated[grepl(\"5\", estimated$wave),]\n# restrict to continuous variables only\nestimated = estimated[grepl(\"Linear\", estimated$Stat.model)]\n\n# c(\"iCog\",\"sCog\",\"Dementia\",\"APOEe4\",\"iFrailty\",\"sFrailty\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"iBMI\",\"sBMI\",\"BrainAge\",\"Packyears\")\n\n# order phenotypes\nestimated$Outcomes <- factor(estimated$Outcome, \n                             levels=c(\"VisualAtrophySuperficial\",\"VisualAtrophyDeep\",\"iCog\",\"sCog\",\"iFrailty\",\"sFrailty\",\"iBMI\",\"sBMI\",\"BrainAge\"),\n                             labels = c(\"Visual atrophy\\nrating\\n (superficial)\",\"Visual atrophy\\nrating (deep)\",\"iCog\",\"sCog\",\"iFrailty\",\"sFrailty\",\"iBMI\",\"sBMI\",\"BrainAge\"),\n                             ordered=T)\n\n\n\nestLinear=ggplot(data=estimated, aes(y=Outcomes,x=beta),shape=3)+\n  xlim(-1.25, 1.25)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ci_l, xmax=ci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 0, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=-0.7, label = label, col=Predictor), size = 3.5, position=position_dodge(width=0.8))+\n  scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(-0.9, 0.9), breaks = seq(-1,1,by=0.4))+\n    theme_bw()+\n  ylab(\"Continuous outcome traits\")+\n  xlab(\"Linear regression:\\nbeta (95%CI)\")+\n  ggtitle(\"Lifetime brain atrophy\\nat age 82 years\\n(wave 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        axis.title.y = element_text(face=\"bold\", colour='black', size=12),\n        axis.title.x = element_text(colour='black', size=12),\n        panel.border = element_blank(),\n        plot.title = element_text(face = \"bold\", colour='black', size=12))+\n  theme(legend.position=\"none\")\n\n### why is diff score always the other way? Sensible because larger diff score is worse, but smaller ratio score is worse\n\n############## OBSERVED ATROPHY\n### only keep longitudinal atrophy\nobserved = assoc[grepl(\"TBV\",assoc$Predictor),]\n# restrict to continuous variables only\nobserved = observed[grepl(\"Linear\", observed$Stat.model)]\n\n# order phenotypes\nobserved$Outcomes <- factor(observed$Outcome, \n                            levels=c(\"VisualAtrophySuperficial\",\"VisualAtrophyDeep\",\"iCog\",\"sCog\",\"iFrailty\",\"sFrailty\",\"iBMI\",\"sBMI\",\"BrainAge\"),\n                            labels = c(\"Visual atrophy\\nrating\\n (superficial)\",\"Visual atrophy\\nrating (deep)\",\"iCog\",\"sCog\",\"iFrailty\",\"sFrailty\",\"iBMI\",\"sBMI\",\"BrainAge\"),\n                            ordered=T)\n\nobsLinear=ggplot(data=observed, aes(y=Outcomes,x=beta),shape=3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ci_l, xmax=ci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 0, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=-0.7, label = label, col=Predictor), size = 3.5,position=position_dodge(width=0.8))+\n    scale_y_discrete(limits = rev(levels(as.factor(observed$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(-0.9, 0.9), breaks = seq(-1,1,by=0.4))+\n  theme_bw()+\n  ylab(\"ontinuous outcome traits\")+\n  xlab(\"Linear regression:\\nbeta (95%CI)\")+\n  ggtitle(\"Observed atrophic changes\\nbetween age 73 and 82\\n(waves 2 to 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        axis.title.y = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=12))+\n  theme(legend.position=\"none\")\n\nplot1 = estLinear + \n        obsLinear + \n        plot_layout(ncol = 2)\n        #plot_annotation(title = \"Correlations with health-related outcomes in LBC\",\n                        #theme = theme(plot.title = element_text(face = \"bold\", colour = \"black\", size = 14, hjust = 0.5))) & theme(legend.position = 'bottom')\n\n#ggpubr::annotate_figure(plot, top = text_grob(\"Correlations with health-related outcomes in LBC\", \n#                                      color = \"black\", face = \"bold\", size = 14))\n\n######################################################################################\n###### Binary traits\n######################################################################################\n############## ESTIMATED ATROPHY\n### first, only estimated atrophy\n# only keep 'diff_stand', 'ratio_stand' or 'resid_stand'\nestimated = assoc[grepl(\"diff_stand|ratio_stand|resid_stand\",assoc$Predictor),]\n# restrict to wave 5 (more atrophy)\nestimated = estimated[grepl(\"5\", estimated$wave),]\n# restrict to continuous variables only\nestimated = estimated[grepl(\"Logistic|Hurdle\", estimated$Stat.model)]\n\n# transform beta to OR\nestimated$OR = exp(estimated$beta)\nestimated$ORci_l = exp(estimated$beta - (1.96 * estimated$std.error))\nestimated$ORci_u = exp(estimated$beta + (1.96 * estimated$std.error))\n\n# display as log odds\nestimated$logOdds = log(estimated$OR)\nestimated$logOdds_ci_l = log(estimated$ORci_l)\nestimated$logOdds_ci_u = log(estimated$ORci_u)\n\n# order phenotypes\nestimated$Outcomes <- factor(estimated$Outcome, \n                             levels=c(\"dementia\",\"APOEe4\",\"diabetes\",\"Stroke\",\"hypertension\",\"packyears\"),\n                             labels = c(\"Dementia\",\"APOEe4\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"Packyears\"),\n                             ordered=T)\n\nestLog=ggplot(data=estimated, aes(y=Outcomes,x=OR),shape=3)+\n  xlim(0, 2.3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ORci_l, xmax=ORci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 1, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=0.4, label = label, col=Predictor), size = 3.5,position=position_dodge(width=0.8))+\n    scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(0.3,3), breaks = seq(0,3,by=0.4))+\n  theme_bw()+\n  ylab(\"Binary outcome traits\")+\n  xlab(\"Logistic regression:\\nOdds ratio (95%CI)\")+\n  #ggtitle(\"'Estimated' brain atrophy\\n(at wave 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        panel.border = element_blank(),\n        axis.title.y = element_text(face=\"bold\", colour='black', size=12),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=13))+\n  theme(legend.position=\"none\")\n\n\n############## OBSERVED ATROPHY\n### only keep longitudinal atrophy\nobserved = assoc[grepl(\"TBV\",assoc$Predictor),]\n# restrict to continuous variables only\nobserved = observed[grepl(\"Logistic|Hurdle\", observed$Stat.model)]\n\n# transform beta to OR\nobserved$OR = exp(observed$beta)\nobserved$ORci_l = exp(observed$beta - (1.96 * observed$std.error))\nobserved$ORci_u = exp(observed$beta + (1.96 * observed$std.error))\n\n# order phenotypes\nobserved$Outcomes <- factor(observed$Outcome, \n                            levels=c(\"dementia\",\"APOEe4\",\"diabetes\",\"Stroke\",\"hypertension\",\"packyears\"),\n                            labels = c(\"Dementia\",\"APOEe4\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"Packyears\"),\n                            ordered=T)\n\nobsLog=ggplot(data=observed, aes(y=Outcomes,x=OR),shape=3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ORci_l, xmax=ORci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 1, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=0.4, label = label, col=Predictor), size = 3.5,position=position_dodge(width=1))+\n  scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(0,6.7), breaks = seq(0,6,by=1))+\n  theme_bw()+\n  ylab(\"Binary outcome traits\")+\n  xlab(\"Logistic regression:\\nOdds ratio (95%CI)\")+\n  #ggtitle(\"'Observed' brain atrophy\\n(wave 2 to 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        panel.border = element_blank(),\n        axis.title.y = element_blank(),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=13),\n        legend.position = \"bottom\")\n\nplot2 = estLog + \n  obsLog + \n  plot_layout(guides = \"collect\")\n\nlayout <- \"\nAB\nAB\nCD\n\"\n\nplot <- estLinear + obsLinear + estLog + obsLog + \n                    plot_layout(design = layout, guides = \"collect\")+\n                    plot_annotation(title = \"Brain atrophy associations with health-related outcomes in LBC\",\n                  caption = \"Note: R2 estimates are only printed if the corresponding association passed correction for multiple testing (p=0.05/15).\\n'Packyears' was analysed with Hurdle regression, but the coefficient printed here is from the negative binomial part of the equation,\\nhence on log-odds scale like the other binary traits. Pseudo-R2 estimates are for the full hurdle model.\",\n                    tag_levels = \"A\",\n                    theme = theme(legend.position = \"bottom\",plot.title = element_text(face = \"bold\", colour = \"black\", size = 14, hjust = 0.5))) \n\n\n#ggsave(paste0(out,\"phenotypic/LBC_assocs_cross_vs_long.jpg\"), bg = \"white\",plot = plot, width = 20, height = 28, units = \"cm\", dpi = 300)"
  },
  {
    "objectID": "pheno_assocs.html#ukb",
    "href": "pheno_assocs.html#ukb",
    "title": "Phenotypic associations with lifetime brain atrophy",
    "section": "UKB",
    "text": "UKB\n\n\nCode\n# read in data \nsetwd(wd)\nfile = list.files(pattern=\"UKB_assocs_observed_vs_estimated_atrophy\")\nassoc = fread(file)\n\n# brain age phenotype is not intuitively interpretable:\n# positive value should mean the particpant has a healthier looking brain tthan expected given their age \n# negative value should mean the participant has an unhealthier looking brain than expected\n# Hence, here we just flip the assocs so that more LBA is associated with older brian age\nassoc$beta[which(assoc$Outcome == \"brainAge\")] <- assoc$beta[which(assoc$Outcome == \"brainAge\")]*(-1)\n\n# calculate lower and upper bounds of assoc\nassoc$ci_l = assoc$beta - (1.96*assoc$std.error)\nassoc$ci_u = assoc$beta + (1.96*assoc$std.error)\n\n# because beta shouldn't exceed 1, and it makes the plots look ugly, I will artifically reduce those here\nassoc$ci_l = ifelse(assoc$ci_l < -1.21, -1.21, assoc$ci_l)\nassoc$ci_u = ifelse(assoc$ci_u > 1.21, 1.21, assoc$ci_u)\n\n# make data frame for geom_label\nassoc$label = paste0(round(assoc$R2_percent, digits = 1), \"%\")\n# only print the label is correlation is significant\nsigBonferroni = 0.05/length(unique(assoc$Outcome))\nassoc$label[which(assoc$p.value > sigBonferroni)] = NA\n\n######################################################################################\n###### Continuous traits\n######################################################################################\n############## ESTIMATED ATROPHY\n### first, only estimated atrophy\n# only keep 'diff_stand', 'ratio_stand' or 'resid_stand'\nestimated = assoc[grepl(\"diff_stand|ratio_stand|resid_stand\",assoc$Predictor),]\n# restrict to wave 3 (more atrophy)\nestimated = estimated[grepl(\"3\", estimated$wave),]\n# restrict to continuous variables only\nestimated = estimated[grepl(\"Linear\", estimated$Stat.model)]\n\n# order phenotypes\nestimated$Outcomes <- factor(estimated$Outcome, \n                             levels=c(\"cog\",\"BMI\",\"brainAge\"),\n                             labels = c(\"Cog\",\"BMI\",\"BrainAge\"),\n                             ordered=T)\n\nestLinear=ggplot(data=estimated, aes(y=Outcomes,x=beta),shape=3)+\n  xlim(-1.25, 1.25)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ci_l, xmax=ci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 0, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=-0.7, label = label, col=Predictor), size = 3.5, position=position_dodge(width=0.8))+\n  scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(-0.9, 0.9), breaks = seq(-1,1,by=0.4))+\n    theme_bw()+\n  ylab(\"Continuous outcome traits\")+\n  xlab(\"Linear regression:\\nbeta (95%CI)\")+\n  ggtitle(\"Lifetime brain atrophy\\n(second neuroimaging visit)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        axis.title.y = element_text(face=\"bold\", colour='black', size=12),\n        axis.title.x = element_text(colour='black', size=12),\n        panel.border = element_blank(),\n        plot.title = element_text(face = \"bold\", colour='black', size=13))+\n  theme(legend.position=\"none\")\n\n############## OBSERVED ATROPHY\n### only keep longitudinal atrophy\nobserved = assoc[grepl(\"TBV\",assoc$Predictor),]\n# restrict to continuous variables only\nobserved = observed[grepl(\"Linear\", observed$Stat.model)]\n\n# order phenotypes\nobserved$Outcomes <- factor(observed$Outcome, \n                             levels=c(\"cog\",\"BMI\",\"brainAge\"),\n                             labels = c(\"Cog\",\"BMI\",\"BrainAge\"),\n                             ordered=T)\n\nobsLinear=ggplot(data=observed, aes(y=Outcomes,x=beta),shape=3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ci_l, xmax=ci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 0, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=-0.7, label = label, col=Predictor), size = 3.5,position=position_dodge(width=0.8))+\n    scale_y_discrete(limits = rev(levels(as.factor(observed$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(-0.9, 0.9), breaks = seq(-1,1,by=0.4))+\n  theme_bw()+\n  ylab(\"ontinuous outcome traits\")+\n  xlab(\"Linear regression:\\nbeta (95%CI)\")+\n  ggtitle(\"Observed atrophic changes\\n(first to second neuroimaging visit)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        axis.title.y = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=12))+\n  theme(legend.position=\"none\")\n\n######################################################################################\n###### Binary traits\n######################################################################################\n############## ESTIMATED ATROPHY\n### first, only estimated atrophy\n# only keep 'diff_stand', 'ratio_stand' or 'resid_stand'\nestimated = assoc[grepl(\"diff_stand|ratio_stand|resid_stand\",assoc$Predictor),]\n# restrict to visit 3 (more atrophy)\nestimated = estimated[grepl(\"3\", estimated$wave),]\n# restrict to continuous variables only\nestimated = estimated[grepl(\"Logistic|Hurdle\", estimated$Stat.model)]\n\n# transform beta to OR\nestimated$OR = exp(estimated$beta)\nestimated$ORci_l = exp(estimated$beta - (1.96 * estimated$std.error))\nestimated$ORci_u = exp(estimated$beta + (1.96 * estimated$std.error))\n\n# display as log odds\nestimated$logOdds = log(estimated$OR)\nestimated$logOdds_ci_l = log(estimated$ORci_l)\nestimated$logOdds_ci_u = log(estimated$ORci_u)\n\n# order phenotypes\nestimated$Outcomes <- factor(estimated$Outcome, \n                             levels=c(\"dementia\",\"APOEe4\",\"diabetes\",\"stroke\",\"hypertension\",\"packyears\",\"frailty\"),\n                             labels = c(\"Dementia\",\"APOEe4\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"Packyears\",\"Frailty\"),\n                             ordered=T)\n\nestLog=ggplot(data=estimated, aes(y=Outcomes,x=OR),shape=3)+\n  xlim(0, 2.3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ORci_l, xmax=ORci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 1, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=0.4, label = label, col=Predictor), size = 3.5,position=position_dodge(width=0.8))+\n    scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(0.3,3.1), breaks = seq(0,3,by=0.4))+\n  theme_bw()+\n  ylab(\"Binary outcome traits\")+\n  xlab(\"Logistic regression:\\nOdds ratio (95%CI)\")+\n  #ggtitle(\"'Estimated' brain atrophy\\n(at wave 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        panel.border = element_blank(),\n        axis.title.y = element_text(face=\"bold\", colour='black', size=12),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=13))+\n  theme(legend.position=\"none\")\n\n############## OBSERVED ATROPHY\n### only keep longitudinal atrophy\nobserved = assoc[grepl(\"TBV\",assoc$Predictor),]\n# restrict to continuous variables only\nobserved = observed[grepl(\"Logistic|Hurdle\", observed$Stat.model)]\n\n# transform beta to OR\nobserved$OR = exp(observed$beta)\nobserved$ORci_l = exp(observed$beta - (1.96 * observed$std.error))\nobserved$ORci_u = exp(observed$beta + (1.96 * observed$std.error))\n\n# order phenotypes\nobserved$Outcomes <- factor(observed$Outcome, \n                             levels=c(\"dementia\",\"APOEe4\",\"diabetes\",\"stroke\",\"hypertension\",\"packyears\",\"frailty\"),\n                             labels = c(\"Dementia\",\"APOEe4\",\"Diabetes\",\"Stroke\",\"Hypertension\",\"Packyears\",\"Frailty\"),\n                             ordered=T)\n\nobsLog=ggplot(data=observed, aes(y=Outcomes,x=OR),shape=3)+\n  geom_point(aes(col=Predictor),size=2,position=position_dodge(width=0.5))+\n  geom_errorbar(aes(xmin=ORci_l, xmax=ORci_u,col=Predictor), linewidth=1, width=.2,position=position_dodge(width=0.5))+\n  geom_vline(xintercept = 1, colour=\"grey10\", alpha=0.5,linetype=2)+\n  geom_text(aes(y=Outcomes,x=0.4, label = label, col=Predictor), size = 3.5,position=position_dodge(width=1))+\n  scale_y_discrete(limits = rev(levels(as.factor(estimated$Outcomes))))+ # reverse order of y-axis\n  scale_x_continuous(limits = c(0.3,3.1), breaks = seq(0,3,by=0.4))+\n  theme_bw()+\n  ylab(\"Binary outcome traits\")+\n  xlab(\"Logistic regression:\\nOdds ratio (95%CI)\")+\n  #ggtitle(\"'Observed' brain atrophy\\n(wave 2 to 5)\")+\n  scale_color_manual(labels = c(\"Difference score\", \"Ratio score\", \"Residual score\"), values = c(\"#D81B60\", \"#FFC107\", \"#004D40\")) +\n  theme(plot.title = element_text(face = \"plain\", size=17, hjust = 0.5))+ # add centred title\n  theme(text = element_text(size=12),\n        axis.text.x = element_text(size=12),#angle=45\n        axis.text.y = element_text(size=12),\n        panel.border = element_blank(),\n        axis.title.y = element_blank(),\n        axis.title.x = element_text(colour='black', size=12),\n        plot.title = element_text(face = \"bold\", colour='black', size=13),\n        legend.position = \"bottom\")\n\n# plot all together\nlayout <- \"\nAB\nCD\nCD\n\"\n\nplot <- estLinear + obsLinear + estLog + obsLog + \n                    plot_layout(design = layout,guides = \"collect\")+\n                    plot_annotation(title = \"Brain atrophy correlations with health-related outcomes in UKB\",\n                  caption = \"Note: R2 estimates are only printed if the corresponding association passed correction for multiple testing (p=0.05/7).\\n'Packyears' & 'frailty' were analysed with Hurdle regression, but the coefficient printed here is from the negative binomial part of the equation,\\nhence on log-odds scale like the other binary traits. Pseudo-R2 estimates are for the full hurdle model.\",\n                    tag_levels = \"A\",\n                    theme = theme(legend.position = \"bottom\",plot.title = element_text(face = \"bold\", colour = \"black\", size = 14, hjust = 0.5))) \n\nggsave(paste0(out,\"phenotypic/UKB_assocs_cross_vs_long.jpg\"), bg = \"white\",plot = plot, width = 20, height = 28, units = \"cm\", dpi = 150)"
  },
  {
    "objectID": "age_corrs.html",
    "href": "age_corrs.html",
    "title": "Age-associated brain shrinkage",
    "section": "",
    "text": "Results produced by the code below are described in the manuscript under section:\nMeasures of LBA indicated age-associated brain shrinkage.\nThe traits used for correlation analysis were derived using code displayed in ‘Data preparation’: UKB."
  },
  {
    "objectID": "age_corrs.html#read-in-all-samples",
    "href": "age_corrs.html#read-in-all-samples",
    "title": "Age-associated brain shrinkage",
    "section": "Read in all samples",
    "text": "Read in all samples\n\n\nCode\n# UKB - use cross-sectional data we're also using for GWAS\nUKB = fread(paste0(wd, \"/UKB_CrossNeuroIDP_noOutliers.txt\"))\nage = fread(paste0(wd, \"/UKB_covarGWAS.txt\"))\nUKB = merge(UKB, age[,c(\"FID\", \"age\",\"sex\")], by = \"FID\")\nUKB$Sample = \"UKB\"\nnames(UKB)[which(names(UKB) == \"IID\")] = \"ID\"\nnames(UKB)[which(names(UKB) == \"age\")] = \"Age\"\n\n# restrict to fam file\nfam = fread(paste0(wd, \"/ukb_neuroimaging_brainAtrophy_GWASinput.fam\"))\nUKB = UKB[UKB$FID %in% fam$V1,]\n\n# for more intuitive interpretation, we will flip the associations for resid and ratio sscore\n# larger score = more atrophy\nUKB$resid = UKB$resid*(-1)\nUKB$resid_stand = UKB$resid_stand*(-1)\nUKB$ratio = UKB$ratio*(-1)\nUKB$ratio_stand = UKB$ratio_stand*(-1)\n\n#####################\n## Human Connectome Project\n#####################\n# read in HCP data\nHCP = fread(paste0(wd,\"/unrestricted_hcp_freesurfer.csv\"))\nHCP = HCP[,c(\"Subject\", \"Gender\", \"FS_InterCranial_Vol\", \"FS_BrainSeg_Vol_No_Vent\")]\nnames(HCP) = c(\"ID\", \"Sex\", \"ICV\", \"TBV\")\n\n# add age information\nHCPage = fread(paste0(wd, \"/RESTRICTED_annafurtjes_12_14_2023_4_18_2.csv\"))\nnames(HCPage)[which(names(HCPage) == \"Subject\")] = \"ID\"\nnames(HCPage)[which(names(HCPage) == \"Age_in_Yrs\")] = \"Age\"\nHCP = merge(HCP, HCPage[,c(\"ID\",\"Age\")], by = \"ID\")\n\n# as outlined elsewhere, empirical investigations warrant to use an age cut-off of 31 years in this sample\nHCP = HCP[which(HCP$Age <= 31),]\n\n# convert mm3 estimates to more intuitive cm3 estimates\nHCP$ICV = HCP$ICV/1000\nHCP$TBV = HCP$TBV/1000\n\n# estimate brain atrophy from single MRI scan\nHCP$diff = HCP$ICV - HCP$TBV\nHCP$ratio = HCP$TBV / HCP$ICV\n\n# Quality control: \n#print(paste0(\"Some participants have negative difference scores and ratio scores > 1, which means that their ICV estimate is smaller than their TBV estimate. This must be an error as the skull always surrounds the brain. Those \", sum((HCP$diff < 0)),\" HCP participants were excluded from the data set.\"))\n\ndeletedHCP = sum(HCP$diff < 0)\n# delete those from data \nif(sum(HCP$diff < 0) != 0){HCP=HCP[-which(HCP$diff < 0),]}\n\n# estimate residual model\nmodel <- lm(TBV ~ ICV, data = HCP)\nHCP$resid = as.vector(resid(model, na.rm=T))\n\n# for more intuitive interpretation, we will flip the associations for resid and ratio sscore\n# larger score = more atrophy\nHCP$resid = HCP$resid*(-1)\nHCP$ratio = HCP$ratio*(-1)\n\n# standardise variables\nHCP$diff_stand = as.vector(scale(HCP$diff))\nHCP$ratio_stand = as.vector(scale(HCP$ratio))\nHCP$resid_stand = as.vector(scale(HCP$resid))\n\n\n#####################\n## MRi-Share\n#####################\n# read in MRi-Share\nShare = fread(paste0(wd, \"/MRiShare_global_IDPs_BSAF2021.csv\"))\nShare$TBV = Share$SPM_GM_Volume + Share$SPM_WM_Volume\nShare = Share[,c(\"ID\", \"Age\", \"Sex\", \"eTIV\", \"TBV\")]\nnames(Share) = c(\"ID\", \"Age\", \"Sex\", \"ICV\", \"TBV\")\n\n# convert mm3 estimates to more intuitive cm3 estimates\nShare$ICV = Share$ICV/1000\nShare$TBV = Share$TBV/1000\n\n# estimate brain atrophy from single MRI scan\nShare$diff = Share$ICV - Share$TBV\nShare$ratio = Share$TBV / Share$ICV\n\nmodel <- lm(TBV ~ ICV, data = Share)\nShare$resid = resid(model)\n\n# save intercept value from the regression\nShareintercept = summary(model)$coefficients[1,1]\n\n# for more intuitive interpretation, we will flip the associations for resid and ratio sscore\n# larger score = more atrophy\nShare$resid = Share$resid*(-1)\nShare$ratio = Share$ratio*(-1)\n\n# standardise variables\nShare$diff_stand = as.vector(scale(Share$diff))\nShare$ratio_stand = as.vector(scale(Share$ratio))\nShare$resid_stand = as.vector(scale(Share$resid))"
  },
  {
    "objectID": "age_corrs.html#calculate-age-correlations-and-plot",
    "href": "age_corrs.html#calculate-age-correlations-and-plot",
    "title": "Age-associated brain shrinkage",
    "section": "Calculate age correlations and plot",
    "text": "Calculate age correlations and plot\n\n\nCode\n# determine age cut-offs to iterate through\nageCut = seq(from = 22, to = max(Share$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotShare = successivelyReduceAge(data = Share, ageCut = ageCut)\n\npShare = ggplot(data = agePlotShare)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(21, 35), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.1, 0.2), breaks = seq(from = -0.5, to = 0.35, by = 0.1))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"MRi-Share (N = \", nrow(Share),\")\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 23, to = max(HCP$Age), by = 1)\n \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nsuccessivelyYoungertHCP = successivelyReduceAge(data = HCP, ageCut = ageCut)\n\npHCP = ggplot(data = successivelyYoungertHCP)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 29, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(22, 31.5), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.2, 0.3), breaks = seq(from = -0.5, to = 0.35, by = 0.1))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"HCP (N = \", nrow(HCP),\")\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n\n#### UKB\nif(mean(UKB$Age > 100)){UKB$Age = UKB$Age /12}\n# determine age cut-offs to iterate through\nageCut = seq(from = 47, to = max(UKB$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotUKB= successivelyReduceAge(data = UKB, ageCut = ageCut)\n\npUKB = ggplot(data = agePlotUKB)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(46, 84.5), breaks = seq(from = 44, to = 84, by = 2))+\n  scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(from = -0.5, to = 0.5, by = 0.2))+\n  ggtitle(paste0(\"UKB (N = \", nrow(UKB),\")\"))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 35, to = max(STRADL$Age), by = 1)\n  \n\n########\n# change direction STRADL\nSTRADL$resid <- STRADL$resid*(-1)\nSTRADL$ratio <- STRADL$ratio*(-1)\n\n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotSTRADL = successivelyReduceAge(data = STRADL, ageCut = ageCut)\n\npSTRADL = ggplot(data = agePlotSTRADL)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(34.5, 84.5), breaks = seq(from = 34, to = 84, by = 4))+\n  scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(from = -0.5, to = 0.5, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"STRADL (N = \", nrow(STRADL),\")\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\nggsave(\"Fig3_indiv.jpg\", bg = \"white\",plot = newFig2bottoM, width = 30, height = 12, units = \"cm\", dpi = 150)"
  },
  {
    "objectID": "age_corrs.html#repeat-for-males-and-females-separately",
    "href": "age_corrs.html#repeat-for-males-and-females-separately",
    "title": "Age-associated brain shrinkage",
    "section": "Repeat for males and females separately",
    "text": "Repeat for males and females separately\n\nFEMALES\n\n\nCode\n###### FEMALES\n# determine age cut-offs to iterate through\nageCut = seq(from = 22, to = max(Share$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotShare = successivelyReduceAge(data = Share[which(Share$Sex == \"F\"),], ageCut = ageCut)\n\npShareF = ggplot(data = agePlotShare)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(21, 35), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.1, 0.2), breaks = seq(from = -0.5, to = 0.35, by = 0.1))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"MRi-Share\\n(N = \", nrow(Share[which(Share$Sex == \"F\"),]),\" females)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 23, to = max(HCP$Age), by = 1)\n \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nsuccessivelyYoungertHCP = successivelyReduceAge(data = HCP[which(HCP$Sex == \"F\")], ageCut = ageCut)\n\npHCPF = ggplot(data = successivelyYoungertHCP)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 29, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(22, 31.5), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.3, 0.6), breaks = seq(from = -0.3, to = 0.6, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"HCP\\n(N = \", nrow(HCP[which(HCP$Sex == \"F\")]),\" females)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n\n#### UKB\nif(mean(UKB$Age > 100)){UKB$Age = UKB$Age /12}\n# determine age cut-offs to iterate through\nageCut = seq(from = 47, to = max(UKB$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotUKB= successivelyReduceAge(data = UKB[UKB$sex == 1,], ageCut = ageCut)\n\npUKBF = ggplot(data = agePlotUKB)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(46, 84.5), breaks = seq(from = 44, to = 84, by = 2))+\n  scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(from = -0.5, to = 0.5, by = 0.2))+\n  ggtitle(paste0(\"UKB\\n(N = \", nrow(UKB[UKB$sex == 1,]),\" females)\"))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 35, to = max(STRADL$Age), by = 1)\n  \n\n########\n# change direction STRADL\n#STRADL$resid <- STRADL$resid*(-1)\n#STRADL$ratio <- STRADL$ratio*(-1)\n\n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotSTRADL = successivelyReduceAge(data = STRADL[STRADL$Sex == 0,], ageCut = ageCut)\n\npSTRADLF = ggplot(data = agePlotSTRADL)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(34.5, 84.5), breaks = seq(from = 34, to = 84, by = 4))+\n  scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(from = -0.5, to = 0.5, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"STRADL\\n(N = \", nrow(STRADL[STRADL$Sex == 0,]),\" females)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n\nAgecorrFEMALES = ggarrange(pShareF, pHCPF, pUKBF, pSTRADLF, labels = c(\"A\",\"B\",\"C\",\"D\"), common.legend = T, legend = \"bottom\", nrow=1)\nggsave(\"AgeCorr_FEMALES.jpg\", bg = \"white\",plot = AgecorrFEMALES, width = 30, height = 12, units = \"cm\", dpi = 150)\n\n\n\n\n\n\n\n\n\nMALES\n\n\nCode\n###### MALES\n# determine age cut-offs to iterate through\nageCut = seq(from = 22, to = max(Share$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotShare = successivelyReduceAge(data = Share[which(Share$Sex == \"M\"),], ageCut = ageCut)\n\npShareM = ggplot(data = agePlotShare)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(21, 35), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.2, 0.2), breaks = seq(from = -0.5, to = 0.35, by = 0.1))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"MRi-Share\\n(N = \", nrow(Share[which(Share$Sex == \"M\"),]),\" males)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 23, to = max(HCP$Age), by = 1)\n \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nsuccessivelyYoungertHCP = successivelyReduceAge(data = HCP[which(HCP$Sex == \"M\")], ageCut = ageCut)\n\npHCPM = ggplot(data = successivelyYoungertHCP)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 29, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(22, 31.5), breaks = seq(from = 20, to = 36, by = 2))+\n  scale_x_continuous(limits = c(-0.3, 0.6), breaks = seq(from = -0.3, to = 0.6, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"HCP\\n(N = \", nrow(HCP[which(HCP$Sex == \"M\")]),\" males)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n\n#### UKB\nif(mean(UKB$Age > 100)){UKB$Age = UKB$Age /12}\n# determine age cut-offs to iterate through\nageCut = seq(from = 48, to = max(UKB$Age), by = 1)\n  \n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotUKB= successivelyReduceAge(data = UKB[UKB$sex == 0,], ageCut = ageCut)\n\npUKBM = ggplot(data = agePlotUKB)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(48, 84.5), breaks = seq(from = 44, to = 84, by = 2))+\n  scale_x_continuous(limits = c(-0.55, 0.6), breaks = seq(from = -1, to = 0.9, by = 0.2))+\n  ggtitle(paste0(\"UKB\\n(N = \", nrow(UKB[UKB$sex == 0,]),\" males)\"))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\n# determine age cut-offs to iterate through\nageCut = seq(from = 35, to = max(STRADL$Age), by = 1)\n  \n\n########\n# change direction STRADL\n#STRADL$resid <- STRADL$resid*(-1)\n#STRADL$ratio <- STRADL$ratio*(-1)\n\n# use function to successively reduce age and determine the correlation between age and atrophy measures\nagePlotSTRADL = successivelyReduceAge(data = STRADL[STRADL$Sex == 1,], ageCut = ageCut)\n\npSTRADLM = ggplot(data = agePlotSTRADL)+\n  geom_point(aes(x = Cor, y = `Age cut-off value`, colour = Measure), alpha = 0.5)+\n  geom_errorbar(aes(y = `Age cut-off value`, xmin = ci_l, xmax = ci_u, colour = Measure), alpha = 0.3)+\n  geom_vline(xintercept = 0, color = \"grey\")+\n  #geom_hline(yintercept = 27, color = \"grey\")+\n  xlab(\"Lifetime brain atrophy\\ncorrelation with age\")+\n  ylab(\"Maximum age in subset\\n(cut-off in age in years)\")+\n  scale_y_continuous(limits = c(34.5, 84.5), breaks = seq(from = 34, to = 84, by = 4))+\n  scale_x_continuous(limits = c(-0.7, 0.55), breaks = seq(from = -1, to = 0.55, by = 0.2))+\n  scale_color_manual(values = c(\"#D81B60\",\"#FFC107\",\"#004D40\"))+\n  ggtitle(paste0(\"STRADL\\n(N = \", nrow(STRADL[STRADL$Sex == 1,]),\" males)\"))+\n  theme_bw()+\n  theme(panel.border = element_blank())\n\nAgecorrMALES = ggarrange(pShareM, pHCPM, pUKBM, pSTRADLM, labels = c(\"A\",\"B\",\"C\",\"D\"), common.legend = T, legend = \"bottom\", nrow=1)\nggsave(\"AgeCorr_MALES.jpg\", bg = \"white\",plot = AgecorrMALES, width = 30, height = 12, units = \"cm\", dpi = 150)"
  },
  {
    "objectID": "age_corrsLBC.html",
    "href": "age_corrsLBC.html",
    "title": "Lifetime brain atrophy increases with age within individuals in LBC1936",
    "section": "",
    "text": "This analysis was moved to the Supplement during revisions of the paper. It shows that measures of lifetime brain atrophy worsen over a 9-year period in LBC1936, even when we cross-sectionally process our neuroimaging data (as opposed to using FS longitudinal processing)."
  },
  {
    "objectID": "age_corrsLBC.html#read-in-data",
    "href": "age_corrsLBC.html#read-in-data",
    "title": "Lifetime brain atrophy increases with age within individuals in LBC1936",
    "section": "Read in data",
    "text": "Read in data\n\n\nCode\n# get cross-sectionally processed data from \n# wave 1\nwave1 = fread(paste0(wd, \"/LBC1936_crossNeuroWave1.txt\"))\nwave1$wave = \"wave 2\"\nwave1$age = 73\n# wave 2\nwave2 = fread(paste0(wd, \"/LBC1936_crossNeuroWave2.txt\"))\nwave2$wave = \"wave 3\"\nwave2$age = 76\n# wave 3\nwave3 = fread(paste0(wd, \"/LBC1936_crossNeuroWave3.txt\"))\nwave3$wave = \"wave 4\"\nwave3$age = 79\n# wave 4\nwave4 = fread(paste0(wd, \"/LBC1936_crossNeuroWave4.txt\"))\nwave4$wave = \"wave 5\"\nwave4$age = 82\n\n# rbind wave data\nall = rbind(wave1, wave2, wave3, wave4)\n\n# only keep participants who have all measurement points\nsave = table(all$lbc36no) == 4\nIDs = dimnames(save)[[1]][as.vector(save)]\nall = all[all$lbc36no %in% IDs,]\n\n#### later edit: so far this data has a residual score for each of the visits meaning that there can never be an increase with age in the residual score\n# Hence, here we standardise across all waves to be able to compare different time points\n# estimate residual model\nmodel <- lm(TBV ~ ICV, data = all)\nall$residALL = as.vector(resid(model, na.rm=T))"
  },
  {
    "objectID": "age_corrsLBC.html#plot-trajectories",
    "href": "age_corrsLBC.html#plot-trajectories",
    "title": "Lifetime brain atrophy increases with age within individuals in LBC1936",
    "section": "Plot trajectories",
    "text": "Plot trajectories\n\n\nCode\np_diff = plotTraject(dat = all, y = \"diff\", col = \"#D81B60\") + \n  ggtitle(\"Raw difference score\")+\n  ylab(\"<- less atrophy         more atrophy ->\")\n\np_ratio = plotTraject(dat = all, y = \"ratio\", col = \"#FFC107\") + \n  ggtitle(\"Raw ratio score\") + \n  scale_y_reverse() + \n  ylab(\"<- less atrophy         more atrophy ->\")\n\np_resid = plotTraject(dat = all, y = \"residALL\", col = \"#004D40\") + \n  ggtitle(\"Raw residual score\")+ \n  scale_y_reverse()+ \n  ylab(\"<- less atrophy         more atrophy ->\")\n\n#cowplot::plot_grid(p_diff, p_ratio, p_resid, nrow = 1, labels = c(\"A\", \"B\", \"C\"), label_size = 6, rel_widths = c(1,1,1))\n## overall title: \"Estimated brain atrophy in LBC1936 (cross-sectional processing)\"\nplot = (p_diff | p_ratio | p_resid) + \n    plot_annotation(title = \"Lifetime brain atrophy\\n(estimated from cross-sectionally processed measures of TBV and ICV)\", \n                    tag_levels = \"A\",\n                            theme = theme(plot.tag = element_text(face = \"bold\"),\n                                  plot.title = element_text(face = \"bold\", size = 20, hjust = 0.5)))\n\nggsave(\"EstimatedAtrophy_LBC1936_wave2to5.jpg\", bg = \"white\",plot = plot, width = 35, height = 20, units = \"cm\", dpi = 150)"
  },
  {
    "objectID": "long_assocs.html",
    "href": "long_assocs.html",
    "title": "Lifetime brain atrophy is correlated with longitudinally-observed atrophic changes",
    "section": "",
    "text": "Results produced by the code below are described in the manuscript under section:\nLBA moderately captured observed atrophic changes that were longitudinally observed between two MRI scans nine years apart.\nThe outcome traits were derived using code displayed in ‘Data preparation’: LBC and UKB. Ratio and residual scores were not flipped in these analyses, like they were in the analyses presented in the main manuscript. This does not affect the directions of effects when we were comparing e.g., LBA residual score with longitudinal residual score."
  },
  {
    "objectID": "long_assocs.html#ukb",
    "href": "long_assocs.html#ukb",
    "title": "Lifetime brain atrophy is correlated with longitudinally-observed atrophic changes",
    "section": "UKB",
    "text": "UKB\nCross-sectional estimates considered from second neuroimaging visit\n\n\nCode\n# read in UKB neuro data\nUKB = fread(paste0(out,\"/UKB_neuroNoLongProcess.txt\"))\n\n# restrict to second neuroimaging visit (i.e., third visit altogether)\nUKB3 = UKB[UKB$wave == 3,]\n\n# later edit: exclude extreme outliers from longitudinal data \nUKB3 <- UKB3[which(UKB3$TBVdiff_2to3_stand < 10),]\nUKB3 <- UKB3[which(UKB3$TBVdiff_2to3_stand > (-10)),]\n\nUKB3 <- UKB3[which(UKB3$TBVratio_3to2_stand < 10),]\nUKB3 <- UKB3[which(UKB3$TBVratio_3to2_stand > (-10)),]\n\nUKB3 <- UKB3[which(UKB3$TBVresid_2to3_stand < 10),]\nUKB3 <- UKB3[which(UKB3$TBVresid_2to3_stand > (-10)),]\n\n# now that more participants were excluded, need to re-calculate the residual score\nmodel <- lm(TBV ~ ICV, data = UKB3)\nUKB3$resid = resid(model)\n\nUKB3$resid_stand <- as.vector(scale(UKB3$resid))\n\n# number of participants\nn = nrow(UKB3)\n\n# plot correlations between atrophy measures\np = plot_heatmap(dat = UKB3[,c(\"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"TBVdiff_2to3\", \"TBVratio_3to2\", \"TBVresid_2to3\")], \n                 axisNames = c(\"ICV\", \"TBV\", \"CSF\", \"Difference\\nscore\\n(cross)\", \"Ratio\\nscore\\n(cross)\", \"Residual\\nscore\\n(cross)\", \"Difference\\nscore\\n(long)\", \"Ratio\\nscore\\n(long)\", \"Residual\\nscore\\n(long)\"))+\n  ggtitle(paste0(\"UKB\\nsecond neuroimaging visit\\n(N = \",n,\")\"))\np\n\n\n\n\n\nCode\n#ggsave(paste0(out, \"UKB_corPlot.jpg\"), plot = p, width = 15, height = 15, units = \"cm\", dpi = 300)\n\n\nResidual score:\n\nr = 0.29; p = 4^{-94}\n\nRatio score:\n\nr = 0.24; p = 3^{-64}\n\nDifference score:\n\nr = 0.21; p = 5.1^{-47}\n\n\nRepeat heatmap for residual score derived with T1-scaling factor\n\n\nCode\n### make same plot only for cross-sectional measures to compare to resid Scaling factor\nscaling = plot_heatmap(dat = UKB3[,c(\"ICV\", \"TBV\", \"T1ScalingFactor\", \"diff\", \"ratio\", \"resid\", \"residScalingFactor_stand\")], \n                 axisNames = c(\"ICV\", \"TBV\", \"T1 Scaling Factor\", \"Difference\\nscore\\n(cross)\", \"Ratio\\nscore\\n(cross)\", \"Residual\\nscore\\n(cross)\", \"Residual score\\n(scaling factor)\"))+\n  ggtitle(paste0(\"UKB\\nsecond neuroimaging visit\\n(N = \",n,\")\"))\nscaling\n\n\n\n\n\nCode\n#ggsave(paste0(out, \"phenotypic/UKB_scalingFactor.jpg\"), plot = scaling, width = 14, height = 14, units = \"cm\", dpi = 200)"
  },
  {
    "objectID": "long_assocs.html#lbc",
    "href": "long_assocs.html#lbc",
    "title": "Lifetime brain atrophy is correlated with longitudinally-observed atrophic changes",
    "section": "LBC",
    "text": "LBC\nCross-sectional estimates considered from final neuroimaging visit (4th scan, 5th visit)\n\n\nCode\n# read in LBC data\n## longitudinal\nlong = fread(paste0(out, \"/LBC1936_longTBVWaves2and5.txt\"), select = c(\"lbc36no\",\"TBVdiff_2to5\",\"TBVratio_5to2\",\"TBVresid_2to5\"))\n## cross-sectional\ncross4 = fread(paste0(out, \"/LBC1936_crossNeuroWave4.txt\"), select = c(\"lbc36no\",\"ICV\",\"TBV\", \"diff\", \"ratio\", \"resid\"))\n\n# merge \nLBC = merge(long, cross4, by = \"lbc36no\")\n\n# number of participants\nn = nrow(LBC)\n\n# plot correlations between atrophy measures\np = plot_heatmap(dat = LBC[,c(\"ICV\",\"TBV\", \"diff\", \"ratio\", \"resid\",\"TBVdiff_2to5\",\"TBVratio_5to2\",\"TBVresid_2to5\")], \n                 axisNames = c(\"ICV\",\"TBV\",\"Difference\\nscore\\n(cross)\",\"Ratio\\nscore\\n(cross)\",\"Residual\\nscore\\n(cross)\",\"Difference\\nscore\\n(long)\",\"Ratio\\nscore\\n(long)\",\"Residual\\nscore\\n(long)\"))+\n  ggtitle(paste0(\"LBC Wave 5\\n(N = \",n,\")\"))\np\n\n\n\n\n\nCode\n#ggsave(paste0(out, \"phenotypic/LBC_corPlot.jpg\"), plot = p, width = 15, height = 15, units = \"cm\", dpi = 300)\n\n\nResidual score:\n\nr = 0.36; p = 9.9^{-10}\n\nRatio score:\n\nr = 0.29; p = 1.4^{-6}\n\nDifference score:\n\nr = 0.3; p = 5.6^{-7}"
  },
  {
    "objectID": "LBC_pheno.html",
    "href": "LBC_pheno.html",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "",
    "text": "Data prepared here was used as input into analyses presented here. The file containing all phenotypic variables was named LBC1936_allPheno.txt."
  },
  {
    "objectID": "LBC_pheno.html#plot-cognitive-scores",
    "href": "LBC_pheno.html#plot-cognitive-scores",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Plot cognitive scores",
    "text": "Plot cognitive scores\n\n\nCode\n# consistent variable names\nnames(data) <- gsub(\"vpa_tot\", \"vpatot\", colnames(data)) \n\n# plot the different cognitive tests\np_matreas <- plot_long(dat = data, id.var = \"lbc36no\", var = \"matreas\")+\n  xlab(\"Wave\")+\n  ylab(\"Matrix reasoning\\n(# correct)\\n'matreas'\")\n\np_blkdes <- plot_long(dat = data, id.var = \"lbc36no\", var = \"blkdes\")+\n  xlab(\"Wave\")+\n  ylab(\"Block design\\n(# correct)\\n'blkdes'\")\n\np_spantot <- plot_long(dat = data, id.var = \"lbc36no\", var = \"spantot\")+\n  xlab(\"Wave\")+\n  ylab(\"Spatial span\\n(# correct)\\n'spantot'\")\n\np_vpatotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"vpatotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Verbal paired associations\\n(# correct)\\n'vpatotal'\")\n\np__lmtotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"lmtotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Logical memory\\n(# details recalled)\\n'lmtotal'\")\n\np_digback <- plot_long(dat = data, id.var = \"lbc36no\", var = \"digback\")+\n  xlab(\"Wave\")+\n  ylab(\"Digit span backwards\\n(max length)\\n'digback'\")\n\np_nart <- plot_long(dat = data, id.var = \"lbc36no\", var = \"nart\")+\n  xlab(\"Wave\")+\n  ylab(\"National Adult\\nReading Test\\n(# correct) 'nart'\")\n\np_wtar <- plot_long(dat = data, id.var = \"lbc36no\", var = \"wtar\")+\n  xlab(\"Wave\")+\n  ylab(\"Wechsler Test of\\nAdult Reading\\n(# correct) 'wtar'\")\n\np_vftot <- plot_long(dat = data, id.var = \"lbc36no\", var = \"vftot\")+\n  xlab(\"Wave\")+\n  ylab(\"Verbal fluency\\n(# correct)\\n'vftot'\")\n\np_digsym<- plot_long(dat = data, id.var = \"lbc36no\", var = \"digsym\")+\n  xlab(\"Wave\")+\n  ylab(\"Digit-symbol substitution\\n(# matched pairs)\\n'digsym'\")\n\np_symsear <- plot_long(dat = data, id.var = \"lbc36no\", var = \"symsear\")+\n  xlab(\"Wave\")+\n  ylab(\"Symbol Search\\n(# correct)\\n'symsear'\")\n\np_crtmean <- plot_long(dat = data, id.var = \"lbc36no\", var = \"crtmean\")+\n  xlab(\"Wave\")+\n  ylab(\"Four-choice reaction\\ntime (ms)\\n'crtmean'\")\n\np_ittotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"ittotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Inspection time\\n(# correct)\\n'ittotal'\")\n\n# arrange all plots\nplots <- ls(pattern=\"p_\")\nplot_list <- list()\n\nfor(i in plots){\n  plot_list[[i]] <- get(i)\n}\n\n# arrange all plots\nggarrange(plotlist = plot_list, ncol = 3, nrow = 5)\n\n\nInsert image"
  },
  {
    "objectID": "LBC_pheno.html#plot-frailty",
    "href": "LBC_pheno.html#plot-frailty",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Plot frailty",
    "text": "Plot frailty\n\n\nCode\n##### visualise \nfrailty = file[,c(1,grep(\"Frailty\", names(file)))]\n# unify naming (otherwise plotting function won't work)\nnames(frailty) = str_replace(names(frailty), pattern= \"_W\", replacement = \"_w\")\n\n# find people who were tested at one wave only to exclude them from slope prediction\nonewave = which(rowSums(is.na(frailty[,grep(\"_W\", names(frailty))])) == 4)\n\n# find people who were not tested at wave 1 to exclude from intercept\nNotWave1 = which(is.na(frailty$FrailtyIndex_W1))\n\n# set slopes for participants who were only tested at one wave to NA\nfrailtyPred$s[onewave] <- NA \n# set intercepts for participants who were not tested at w1 to NA \nfrailtyPred$i[NotWave1] <- NA \n\n##### inspect trajectory\n\np_frailty <- plot_long(dat = frailty, id.var = \"lbc36no\", var = \"FrailtyIndex\")+\n  xlab(\"Wave\")+\n  ylab(\"Frailty index\")\n\np_frailty"
  },
  {
    "objectID": "LBC_pheno.html#plot-bmi",
    "href": "LBC_pheno.html#plot-bmi",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Plot BMI",
    "text": "Plot BMI\n\n\nCode\n## visualise\nbmi = file[,c(1,grep(\"bmi\", names(file)))]\n\n# find people who were tested at one wave only to exclude them from slope prediction\nonewave = which(rowSums(is.na(bmi[,grep(\"_W\", names(bmi))])) == 4)\n\n# find people who were not tested at wave 1 to exclude from intercept\nNotWave1 = which(is.na(bmi$bmiIndex_W1))\n\n# set slopes for participants who were only tested at one wave to NA\nbmiPred$s[onewave] <- NA \n# set intercepts for participants who were not tested at w1 to NA \nbmiPred$i[NotWave1] <- NA \n\nsummary(bmiPred)\n\n# write.table to dat\nwrite.table(bmiPred, file = paste0(wd, \"/LBC1936_bmiFactorScores.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n##### inspect trajectory\nplot_long(dat = bmi, id.var = \"lbc36no\", var = \"bmi\")+\n  xlab(\"Wave\")+\n  ylab(\"bmi index\")"
  },
  {
    "objectID": "LBC_pheno.html#load-packages",
    "href": "LBC_pheno.html#load-packages",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Load packages",
    "text": "Load packages\n\n\nCode\nlibrary(data.table)"
  },
  {
    "objectID": "LBC_pheno.html#cognitive-tests-factor-scores",
    "href": "LBC_pheno.html#cognitive-tests-factor-scores",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Cognitive tests (factor scores)",
    "text": "Cognitive tests (factor scores)\nThis script was kindly provided by Joanna Moodie who had modelled a general factor of cognitive ability in the LBC1936 for one of her projects.\n\n\nCode\n#######################################\n## Read in and format data\n######################################\n\n# read data\ndata = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AF_07NOV2023.sav\"), to.data.frame=T)\n\n##########################################################\n### Format data\n##########################################################\n\ndata=data[,c(\"lbc36no\",\n              \"matreas_w1\",\"matreas_w2\",\"matreas_w3\",\"matreas_w4\",\"matreas_w5\",\n              \"blkdes_w1\",\"blkdes_w2\",\"blkdes_w3\",\"blkdes_w4\",\"blkdes_w5\",\n              \"spantot_w1\",\"spantot_w2\",\"spantot_w3\",\"spantot_w4\",\"spantot_w5\",\n              \"vpatotal_w1\",\"vpatotal_w2\",\"vpatotal_w3\",\"vpatotal_w4\",\"vpa_total_w5\",\n              \"lmtotal_w1\",\"lmtotal_w2\",\"lmtotal_w3\",\"lmtotal_w4\",\"lmtotal_w5\",\n              \"digback_w1\",\"digback_w2\",\"digback_w3\",\"digback_w4\",\"digback_w5\",\n              \"nart_w1\",\"nart_w2\",\"nart_total_w3\",\"nart_total_w4\",\"nart_total_w5\",\n              \"wtar_w1\",\"wtar_w2\",\"wtar_total_w3\",\"wtar_total_w4\",\"wtar_total_w5\",\n              \"vftot_w1\",\"vftot_w2\",\"vftot_w3\" ,\"vftot_w4\",\"vftot_w5\",\n              \"digsym_w1\",\"digsym_w2\",\"digsym_w3\",\"digsym_w4\",\"digsym_w5\",\n              \"symsear_w1\",\"symsear_w2\",\"symsear_w3\",\"symsear_w4\",\"symsear_w5\",\n              \"crtmean_w1\",\"crtmean_w2\",\"crtmean_w3\",\"crtmean_w4\",\"crtmean_w5\",\n              \"ittotal_w1\",\"ittotal_w2\",\"ittotal_w3\",\"ittotal_w4\",\"ittotal_w5\")]\n\n\n# recode missing values\ndata[data == -999] <- NA\ndata[data == -777] <- NA\ndata[data == 999] <- NA\ndata[data == 888] <- NA\ndata[data == -888] <- NA\n\n# assign correct variable classes\n# some numeric columns are coded as factors when some should be integers and some should be numeric\n# the code below transforms variables first into characters, then numeric because some of the values otherwise get corrupted\nIntNames = names(data)[-grep(\"crtmean\", names(data))]\nIntNames = IntNames[-which(IntNames == \"lbc36no\")]\ndata[IntNames] = lapply(data[IntNames], as.character)\ndata[IntNames] = lapply(data[IntNames], as.integer)\n\nNumNames = names(data)[grep(\"crtmean\", names(data))]\ndata[NumNames] = lapply(data[NumNames], as.character)\ndata[NumNames] = lapply(data[NumNames], as.numeric)\n\n# the symbol search variable has some impossible values (< 0).. these should be removed\nfor (i in names(data)[grep(\"symsear\", names(data))]) {\n  data[which(data[,i] < 0),i] <- NA\n}\n\n#rescaled some of the cognitive test variables so that variances are within a similar\n#range see http://www.statmodel.com/discussion/messages/11/1615.html?1335376547\n\ndset_mod <- mutate(data,\n                   blkdes_w1 = blkdes_w1/2,\n                   blkdes_w2 = blkdes_w2/2,\n                   blkdes_w3 = blkdes_w3/2,\n                   blkdes_w4 = blkdes_w4/2,\n                   blkdes_w5 = blkdes_w5/2,\n                   vftot_w1 = vftot_w1/2,\n                   vftot_w2 = vftot_w2/2,\n                   vftot_w3 = vftot_w3/2,\n                   vftot_w4 = vftot_w4/2,\n                   vftot_w5 = vftot_w5/2,\n                   lmtotal_w1 = lmtotal_w1/3,\n                   lmtotal_w2 = lmtotal_w2/3,\n                   lmtotal_w3 = lmtotal_w3/3,\n                   lmtotal_w4 = lmtotal_w4/3,\n                   lmtotal_w5 = lmtotal_w5/3,\n                   digback_w1 = 3*digback_w1,\n                   digback_w2 = 3*digback_w2,\n                   digback_w3 = 3*digback_w3,\n                   digback_w4 = 3*digback_w4,\n                   digback_w5 = 3*digback_w5,\n                   digsym_w1 = digsym_w1/2,\n                   digsym_w2 = digsym_w2/2,\n                   digsym_w3 = digsym_w3/2,\n                   digsym_w4 = digsym_w4/2,\n                   digsym_w5 = digsym_w5/2,\n                   ittotal_w1 = ittotal_w1/2,\n                   ittotal_w2 = ittotal_w2/2,\n                   ittotal_w3 = ittotal_w3/2,\n                   ittotal_w4 = ittotal_w4/2,\n                   ittotal_w5 = ittotal_w5/2,\n                   crtmean_w1 = -50 * crtmean_w1,\n                   crtmean_w2 = -50 * crtmean_w2,\n                   crtmean_w3 = -50 * crtmean_w3,\n                   crtmean_w4 = -50 * crtmean_w4,\n                   crtmean_w5 = -50 * crtmean_w5)\n\n#==================================================================\n# LBC1936 factor of curves model\n#==================================================================\nmodel <- '\n# test growth curves\nImatreas =~ 1*matreas_w1 + 1*matreas_w2 + 1*matreas_w3 + 1*matreas_w4 + 1*matreas_w5\nSmatreas =~ 0*matreas_w1 + 2.98*matreas_w2 + 6.75*matreas_w3 + 9.82*matreas_w4 + 12.54*matreas_w5\n\nIblkdes =~ 1*blkdes_w1 + 1*blkdes_w2 + 1*blkdes_w3 + 1*blkdes_w4 + 1*blkdes_w5\nSblkdes=~ 0*blkdes_w1 + 2.98*blkdes_w2 + 6.75*blkdes_w3 + 9.82*blkdes_w4 + 12.54*blkdes_w5\n\nIspantot =~ 1*spantot_w1 + 1*spantot_w2 + 1*spantot_w3 + 1*spantot_w4 + 1*spantot_w5\nSspantot=~ 0*spantot_w1 + 2.98*spantot_w2 + 6.75*spantot_w3 + 9.82*spantot_w4 + 12.54*spantot_w5\n\nInart =~ 1*nart_w1 + 1*nart_w2 + 1*nart_total_w3 + 1*nart_total_w4 + 1*nart_total_w5\nSnart =~ 0*nart_w1 + 2.98*nart_w2 + 6.75*nart_total_w3 + 9.82*nart_total_w4 + 12.54*nart_total_w5\n\nIwtar =~ 1*wtar_w1 + 1*wtar_w2 + 1*wtar_total_w3 + 1*wtar_total_w4 + 1*wtar_total_w5\nSwtar =~ 0*wtar_w1 + 2.98*wtar_w2 + 6.75*wtar_total_w3 + 9.82*wtar_total_w4 + 12.54*wtar_total_w5\n\nIvftot =~ 1*vftot_w1 + 1*vftot_w2 + 1*vftot_w3 + 1*vftot_w4 + 1*vftot_w5\nSvftot =~ 0*vftot_w1 + 2.98*vftot_w2 + 6.75*vftot_w3 + 9.82*vftot_w4 + 12.54*vftot_w5\n\nIvpatotal =~ 1*vpatotal_w1 + 1*vpatotal_w2 + 1*vpatotal_w3 + 1*vpatotal_w4 + 1*vpa_total_w5\nSvpatotal =~ 0*vpatotal_w1 + 2.98*vpatotal_w2 + 6.75*vpatotal_w3 + 9.82*vpatotal_w4 + 12.54*vpa_total_w5\n\nIlmtotal =~ 1*lmtotal_w1 + 1*lmtotal_w2 + 1*lmtotal_w3 + 1*lmtotal_w4 + 1*lmtotal_w5\nSlmtotal =~ 0*lmtotal_w1 + 2.98*lmtotal_w2 + 6.75*lmtotal_w3 + 9.82*lmtotal_w4 + 12.54*lmtotal_w5\n\nIdigback =~ 1*digback_w1 + 1*digback_w2 + 1*digback_w3 + 1*digback_w4 + 1*digback_w5\nSdigback =~ 0*digback_w1 + 2.98*digback_w2 + 6.75*digback_w3 + 9.82*digback_w4 + 12.54*digback_w5\n\nIsymsear =~ 1*symsear_w1 + 1*symsear_w2 + 1*symsear_w3 + 1*symsear_w4 + 1*symsear_w5\nSsymsear =~ 0*symsear_w1 + 2.98*symsear_w2 + 6.75*symsear_w3 + 9.82*symsear_w4 + 12.54*symsear_w5\n\nIdigsym =~ 1*digsym_w1 + 1*digsym_w2 + 1*digsym_w3 + 1*digsym_w4 + 1*digsym_w5\nSdigsym =~ 0*digsym_w1 + 2.98*digsym_w2 + 6.75*digsym_w3 + 9.82*digsym_w4 + 12.54*digsym_w5\n\nIittotal =~ 1*ittotal_w1 + 1*ittotal_w2 + 1*ittotal_w3 + 1*ittotal_w4 + 1*ittotal_w5\nSittotal =~ 0*ittotal_w1 + 2.98*ittotal_w2 + 6.75*ittotal_w3 + 9.82*ittotal_w4 + 12.54*ittotal_w5\n\nIcrtmean =~ 1*crtmean_w1 + 1*crtmean_w2 + 1*crtmean_w3 + 1*crtmean_w4 + 1*crtmean_w5\nScrtmean =~ 0*crtmean_w1 + 2.98*crtmean_w2 + 6.75*crtmean_w3 + 9.82*crtmean_w4 + 12.54*crtmean_w5\n\n# latent g intercept and slope \nIg =~  Iblkdes + Imatreas  + Ispantot + Ivftot + Ivpatotal + Ilmtotal +\n  Idigback + Isymsear + Idigsym + Icrtmean + Iittotal + Inart + Iwtar \n# \nSg =~ Sblkdes + Smatreas + Sspantot + Svftot + Svpatotal + Slmtotal +\n  Sdigback + Ssymsear + Sdigsym + Scrtmean + Sittotal + Snart + Swtar \n\n#indicator as scaling reference: loading=1, int=0\nIblkdes ~ 0*1\nSblkdes ~ 0*1 \n\n# within-wave covariances between nart and wtar\nnart_w1 ~~ wtar_w1\nnart_w2 ~~ wtar_w2\nnart_total_w3 ~~ wtar_total_w3\nnart_total_w4 ~~ wtar_total_w4\nnart_total_w5 ~~ wtar_total_w5\n\n# within-test intercept-slope covariances\nImatreas ~~ Smatreas\nIblkdes ~~ Sblkdes\n#Ispantot ~~Sspantot\nInart ~~ Snart\nIwtar ~~ Swtar\nIvftot ~~ Svftot\nIvpatotal ~~ Svpatotal\nIlmtotal ~~ Slmtotal\nIdigback ~~ Sdigback\nIsymsear ~~ Ssymsear\nIdigsym ~~ Sdigsym\nIittotal ~~ Sittotal\nIcrtmean ~~ Scrtmean\n\n\n# within-domain intercept-intercept and slope-slope covariances\nIblkdes ~~ Imatreas # Visuospatial domain\nIblkdes ~~ Ispantot\nImatreas ~~ Ispantot\nSblkdes ~~ Smatreas \n#Sblkdes ~~ Sspantot\n#Smatreas ~~ Sspantot\n\nInart ~~ Ivftot #Crystalized domain\nIwtar ~~ Ivftot\nIwtar ~~ Inart\nSnart ~~ Svftot\nSwtar ~~ Svftot\nSwtar ~~ Snart\n\nIlmtotal ~~ Ivpatotal # Verbal memory domain\nIlmtotal ~~ Idigback\nIvpatotal ~~ Idigback\nSlmtotal ~~ Svpatotal\nSlmtotal ~~ Sdigback\nSvpatotal ~~ Sdigback\n\nIittotal ~~ Idigsym #Processing speed domain\nIittotal ~~ Isymsear\nIittotal ~~ Icrtmean\nIdigsym ~~ Isymsear\nIdigsym ~~ Icrtmean\nIsymsear ~~ Icrtmean\nSittotal ~~ Sdigsym \nSittotal ~~ Ssymsear\nSittotal ~~ Scrtmean\nSdigsym ~~ Ssymsear\nSdigsym ~~ Scrtmean\nSsymsear ~~ Scrtmean\n\n#fixed negative residual variance to 0 \nSspantot ~~ 0*Sspantot\n'\n\n# fit model in lavaan \nfit <- growth(model = model, dset_mod,  missing = \"ml.x\")\n#save=standardizedsolution(fit, output=\"data.frame\")\nsummary(fit, fit.measures = T, standardized = T)\n\n#==================================================================\n# extract LBC1936 g intercepts and slopes\n#==================================================================\n\ncogscores <- dset_mod %>% select(contains(\"matreas\") | contains(\"blkdes\") | contains(\"spantot\") | contains(\"nart\") | contains(\"vftot\") | contains(\"vpa\") | contains(\"lmtotal\") | contains(\"digback\") | contains(\"symsear\") | contains(\"digsym\") | contains(\"ittotal\") | contains(\"crtmean\") )\nwavesindex <- rep(c(1,2,3,4,5), 12) # index the waves \n\n# find people that were only tested at one wave only to exclude them from prediction\n# and find people that were not tested at wave 1\nw1 <- matrix(NA, nrow(cogscores), 1)\nw2 <- matrix(NA, nrow(cogscores), 1)\nw3 <- matrix(NA, nrow(cogscores), 1)\nw4 <- matrix(NA, nrow(cogscores), 1)\nw5 <- matrix(NA, nrow(cogscores), 1)\nfor (i in 1:nrow(cogscores)) {\n  if (!all(is.na(cogscores[i, which(wavesindex == 1)]))) {\n    w1[i,1] <- 1 }\n  if (!all(is.na(cogscores[i, which(wavesindex == 2)]))) {\n    w2[i,1] <- 1 }\n  if (!all(is.na(cogscores[i, which(wavesindex == 3)]))) {\n    w3[i,1] <- 1 }\n  if (!all(is.na(cogscores[i, which(wavesindex == 4)]))) {\n    w4[i,1] <- 1 }\n  if (!all(is.na(cogscores[i, which(wavesindex == 5)]))) {\n    w5[i,1] <- 1 }\n}\n\nwavesample <- cbind(w1, w2, w3, w4, w5)\nrowSums(wavesample, na.rm = T)\nonewave <- which(rowSums(wavesample, na.rm = T) < 2) # these people were only tested at one wave\n\n\n# delete participant with all missing data\n#dset_mod1 = dset_mod[-which(rowSums(is.na(dset_mod)) >= (ncol(dset_mod)-1)),]\n\n# extract factor scores\nfactorScores <- data.frame(lavPredict(fit, dset_mod, \n                 type =\"lv\", \n                 method = \"regression\", \n                 label = TRUE))\n\n# set slopes for participants who were only tested at one wave to NA\nfactorScores$Sg[onewave] <- NA \n# set intercepts for participants who were not tested at w1 to NA \nfactorScores$Ig[which(is.na(w1))] <- NA \n# add participant ID back to factor scores\nfactorScores <- data.frame(lbc36no = dset_mod$lbc36no, Ig = factorScores$Ig, Sg = factorScores$Sg)\n\n# report Ns\ncbind(c(\"N intercepts included\", \"N intercepts excluded\", \"N slopes included\", \"N slopes excluded\"), as.numeric(c(length(which(is.na(factorScores$Ig) == F)), length(which(is.na(factorScores$Ig) == T)), length(which(is.na(factorScores$Sg) == F)), length(which(is.na(factorScores$Sg) == T))))) \n\n# write.table to dat\nwrite.table(factorScores, file = paste0(wd, \"/LBC1936_Cog_FactorScores.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n\n\nPlot cognitive scores\n\n\nCode\n# consistent variable names\nnames(data) <- gsub(\"vpa_tot\", \"vpatot\", colnames(data)) \n\n# plot the different cognitive tests\np_matreas <- plot_long(dat = data, id.var = \"lbc36no\", var = \"matreas\")+\n  xlab(\"Wave\")+\n  ylab(\"Matrix reasoning\\n(# correct)\\n'matreas'\")\n\np_blkdes <- plot_long(dat = data, id.var = \"lbc36no\", var = \"blkdes\")+\n  xlab(\"Wave\")+\n  ylab(\"Block design\\n(# correct)\\n'blkdes'\")\n\np_spantot <- plot_long(dat = data, id.var = \"lbc36no\", var = \"spantot\")+\n  xlab(\"Wave\")+\n  ylab(\"Spatial span\\n(# correct)\\n'spantot'\")\n\np_vpatotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"vpatotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Verbal paired associations\\n(# correct)\\n'vpatotal'\")\n\np__lmtotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"lmtotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Logical memory\\n(# details recalled)\\n'lmtotal'\")\n\np_digback <- plot_long(dat = data, id.var = \"lbc36no\", var = \"digback\")+\n  xlab(\"Wave\")+\n  ylab(\"Digit span backwards\\n(max length)\\n'digback'\")\n\np_nart <- plot_long(dat = data, id.var = \"lbc36no\", var = \"nart\")+\n  xlab(\"Wave\")+\n  ylab(\"National Adult\\nReading Test\\n(# correct) 'nart'\")\n\np_wtar <- plot_long(dat = data, id.var = \"lbc36no\", var = \"wtar\")+\n  xlab(\"Wave\")+\n  ylab(\"Wechsler Test of\\nAdult Reading\\n(# correct) 'wtar'\")\n\np_vftot <- plot_long(dat = data, id.var = \"lbc36no\", var = \"vftot\")+\n  xlab(\"Wave\")+\n  ylab(\"Verbal fluency\\n(# correct)\\n'vftot'\")\n\np_digsym<- plot_long(dat = data, id.var = \"lbc36no\", var = \"digsym\")+\n  xlab(\"Wave\")+\n  ylab(\"Digit-symbol substitution\\n(# matched pairs)\\n'digsym'\")\n\np_symsear <- plot_long(dat = data, id.var = \"lbc36no\", var = \"symsear\")+\n  xlab(\"Wave\")+\n  ylab(\"Symbol Search\\n(# correct)\\n'symsear'\")\n\np_crtmean <- plot_long(dat = data, id.var = \"lbc36no\", var = \"crtmean\")+\n  xlab(\"Wave\")+\n  ylab(\"Four-choice reaction\\ntime (ms)\\n'crtmean'\")\n\np_ittotal <- plot_long(dat = data, id.var = \"lbc36no\", var = \"ittotal\")+\n  xlab(\"Wave\")+\n  ylab(\"Inspection time\\n(# correct)\\n'ittotal'\")\n\n# arrange all plots\nplots <- ls(pattern=\"p_\")\nplot_list <- list()\n\nfor(i in plots){\n  plot_list[[i]] <- get(i)\n}\n\n# arrange all plots\nggarrange(plotlist = plot_list, ncol = 3, nrow = 5)\n\n\nInsert image"
  },
  {
    "objectID": "LBC_pheno.html#clinically-ascertained-all-cause-dementia",
    "href": "LBC_pheno.html#clinically-ascertained-all-cause-dementia",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Clinically-ascertained all-cause dementia",
    "text": "Clinically-ascertained all-cause dementia\nThis variable is available through the LBC1936 and was derived as described here.\n\n\nCode\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_EarlyAccessDementiaAscertainment_AF_07DEC2023.sav\"), to.data.frame=T)\n\n# remove redundant spacing \nfor(i in names(file)){\n  file[,i] = stringr::str_remove_all(file[,i], pattern = \" \")\n}\n\n# remove NA coding\nfile[file == -999] <- NA\nfile[file == -888] <- NA\n\n# no participants with all missing data\n# which(is.na(file$dement_w1))\n\ntable(file$dementia_code)\n#  Dementia NoDementia \n#       118        747 \n\nfile$dementia_code[which(file$dementia_code == \"Dementia\")] <- 1\nfile$dementia_code[which(file$dementia_code == \"NoDementia\")] <- 0\n\n# binary variable as factor\nfile$dementia_code = as.factor(file$dementia_code)\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\", \"dementia_code\")], file = paste0(wd, \"/LBC1936_dementia.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#apoe-status",
    "href": "LBC_pheno.html#apoe-status",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "APOE status",
    "text": "APOE status\n\n\nCode\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AF_07NOV2023.sav\"), to.data.frame=T)\n\ntable(file[,grep(\"APOE\", names(file))])\n#APOEgenotype\n#APOEe4               ?     e2/e2 e2/e3 e2/e4 e3/e3 e3/e4 e4/e4\n#No e4 allele     0     0     5   120     0   597     0     0\n#e4 allele        0     0     0     0    23     0   262    21\n\n\ntable(file$APOEe4)\n#No e4 allele    e4 allele \n#722          306\n\n# recode\nfile$APOEe4 = as.numeric(file$APOEe4)-1\n\n# make factor\nfile$APOEe4 = as.factor(file$APOEe4)\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\", \"APOEe4\")], file = paste0(wd, \"/LBC1936_APOEe4.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#frailty",
    "href": "LBC_pheno.html#frailty",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Frailty",
    "text": "Frailty\n\n\nCode\n# names(file)[grep(\"Frailty\", names(file))]\nfrailty = file[,c(1,grep(\"Frailty\", names(file)))]\n\n# model slopes and intercepts\nfrailModel <- '\n          i =~ 1*FrailtyIndex_W1 + 1*FrailtyIndex_W2 + 1*FrailtyIndex_W3 + 1*FrailtyIndex_W4 + 1*FrailtyIndex_W5\n          s =~ 0*FrailtyIndex_W1 + 2.98*FrailtyIndex_W2 + 6.75*FrailtyIndex_W3 + 9.82*FrailtyIndex_W4 + 12.54*FrailtyIndex_W5\n          '\n\nfit = growth(frailModel, frailty, missing = \"ml.x\")\nsummary(fit, standardized = T)\n\n# estimate individual-level values for slope and intercept\nfrailtyPred = as.data.frame(lavPredict(fit,\n                 type =\"lv\", \n                 method = \"regression\", \n                 label = TRUE))\n\n# merge with participant ID\nfrailtyPred = cbind(frailty$lbc36no,frailtyPred)\nnames(frailtyPred) = c(\"lbc36no\", \"iFrail\",\"sFrail\")\n\n# save\nwrite.table(frailtyPred, file = paste0(wd, \"/LBC1936_Frailty.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n\n\nPlot frailty\n\n\nCode\n##### visualise \nfrailty = file[,c(1,grep(\"Frailty\", names(file)))]\n# unify naming (otherwise plotting function won't work)\nnames(frailty) = str_replace(names(frailty), pattern= \"_W\", replacement = \"_w\")\n\n# find people who were tested at one wave only to exclude them from slope prediction\nonewave = which(rowSums(is.na(frailty[,grep(\"_W\", names(frailty))])) == 4)\n\n# find people who were not tested at wave 1 to exclude from intercept\nNotWave1 = which(is.na(frailty$FrailtyIndex_W1))\n\n# set slopes for participants who were only tested at one wave to NA\nfrailtyPred$s[onewave] <- NA \n# set intercepts for participants who were not tested at w1 to NA \nfrailtyPred$i[NotWave1] <- NA \n\n##### inspect trajectory\n\np_frailty <- plot_long(dat = frailty, id.var = \"lbc36no\", var = \"FrailtyIndex\")+\n  xlab(\"Wave\")+\n  ylab(\"Frailty index\")\n\np_frailty"
  },
  {
    "objectID": "LBC_pheno.html#diabetes",
    "href": "LBC_pheno.html#diabetes",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Diabetes",
    "text": "Diabetes\n\n\nCode\n# no participants with all missing data\n# which(is.na(file$diab_w1))\n\n# first of all, all No\nfile$diab_life = 0\nfile$diab_life[which(file$diab_w1 == \"Yes\" | \n                         file$diab_w2 == \"Yes\" |\n                         file$diab_w3 == \"Yes\" | \n                         file$diab_w4 == \"Yes\" | \n                         file$diab_w5 == \"Yes\")] = 1\n\n# remove those with all missing\nallMissing = which(rowSums(!is.na(file[,grep(\"diab_w\", names(file))])) == 0) \nif(length(allMissing) != 0){file$diab_life[allMissing] = NA}\n\n\ntable(file$diab_life)\n# No Yes \n# 942 149 \n\nfile$diab_life = as.factor(file$diab_life)\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\", \"diab_life\")], file = paste0(wd, \"/LBC1936_diabetes.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#hypertension",
    "href": "LBC_pheno.html#hypertension",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Hypertension",
    "text": "Hypertension\n\n\nCode\n# no participants with all missing data\n# which(is.na(file$hibp_w1))\n\n# first of all, all No\nfile$hypertension_life = 0\nfile$hypertension_life[which(file$hibp_w1 == \"Yes\" | \n                       file$hibp_w2 == \"Yes\" |\n                       file$hibp_w3 == \"Yes\" | \n                       file$hibp_w4 == \"Yes\" | \n                       file$hibp_w5 == \"Yes\")] = 1\n\n# remove those with all missing\nallMissing = which(rowSums(!is.na(file[,grep(\"hibp_w\", names(file))])) == 0) \nif(length(allMissing) != 0){file$hypertension_life[allMissing] = NA}\n\nfile$hypertension_life = as.factor(file$hypertension_life)\n\ntable(file$hypertension_life)\n# No Yes \n# 448 643 \n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\", \"hypertension_life\")], file = paste0(wd, \"/LBC1936_hypertension.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#smoking-packyears",
    "href": "LBC_pheno.html#smoking-packyears",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Smoking (packyears)",
    "text": "Smoking (packyears)\nNumber of cigarettes per year x years of smoking / 20 (pack size)\n\n\nCode\n# keep variables of interest\nsmok = file[,c(1,grep(\"smo\", names(file)))]\n\n# wave 1 has 465 more non-missing values than wave 2 \n# all.equal(smok$smokagestop_w1, smok$smokagestop_w2)\n\n# keep only first wave\nsmok = smok[,c(1,grep(\"_w1\", names(smok)))]\n\n# identify participants who have starting year but not end year as they must still be smokers\nsmok$smokagestop = ifelse(smok$smokcat_w1 == \"current smoker\", 70, smok$smokagestop_w1)\n\n# extract years of smoking (so far, regardless if they stopped smoking or not)\nsmok$yearsSmok = smok$smokagestop - smok$smokagestart_w1\n\n# two entries are smoknumcigs == 0 which would null the equation\n# one of those participants have indicates to be a current smoker and the other one to hae never smoked (but still gave starting age)\n# as I don't know how that came about, I have deleted these two data points\nsmok[which(smok$smoknumcigs_w1 == 0),] = NA\n\n# calculate packyears\nsmok$packyears = (smok$smoknumcigs_w1 * 365 * smok$yearsSmok)/20\n\n# null the never smokers\nsmok$packyears[which(smok$smokcat_w1 == \"never smoked\")] = 0\n\nhist(smok$packyears)\n\n# write.table to dat\nwrite.table(smok[,c(\"lbc36no\", \"packyears\")], file = paste0(wd, \"/LBC1936_packYears.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#body-mass-index-bmi",
    "href": "LBC_pheno.html#body-mass-index-bmi",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Body mass index (BMI)",
    "text": "Body mass index (BMI)\n\n\nCode\n# names(file)[grep(\"bmi\", names(file))]\nbmi = file[,c(1,grep(\"bmi\", names(file)))]\n\n# recode missing\nbmi[bmi == -999] <- NA\n\n# model slopes and intercepts\nBMImodel <- '\n          i =~ 1*bmi_w1 + 1*bmi_w2 + 1*bmi_w3 + 1*bmi_w4 + 1*bmi_w5\n          s =~ 0*bmi_w1 + 2.98*bmi_w2 + 6.75*bmi_w3 + 9.82*bmi_w4 + 12.54*bmi_w5\n          '\n\nfit = growth(BMImodel, bmi, missing = \"ml.x\")\nsummary(fit, standardized = T)\n\n# estimate individual-level values for slope and intercept\nbmiPred = as.data.frame(lavPredict(fit,\n                 type =\"lv\", \n                 method = \"regression\", \n                 label = TRUE))\n\n# merge with participant ID\nbmiPred = cbind(bmi$lbc36no, bmiPred)\nnames(bmiPred) = c(\"lbc36no\", \"iBMI\", \"sBMI\")\n\n# write.table to dat\nwrite.table(bmiPred[,c(\"lbc36no\", \"iBMI\", \"sBMI\")], file = paste0(wd, \"/LBC1936_bmi.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n\n\nPlot BMI\n\n\nCode\n## visualise\nbmi = file[,c(1,grep(\"bmi\", names(file)))]\n\n# find people who were tested at one wave only to exclude them from slope prediction\nonewave = which(rowSums(is.na(bmi[,grep(\"_W\", names(bmi))])) == 4)\n\n# find people who were not tested at wave 1 to exclude from intercept\nNotWave1 = which(is.na(bmi$bmiIndex_W1))\n\n# set slopes for participants who were only tested at one wave to NA\nbmiPred$s[onewave] <- NA \n# set intercepts for participants who were not tested at w1 to NA \nbmiPred$i[NotWave1] <- NA \n\nsummary(bmiPred)\n\n# write.table to dat\nwrite.table(bmiPred, file = paste0(wd, \"/LBC1936_bmiFactorScores.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")\n\n##### inspect trajectory\nplot_long(dat = bmi, id.var = \"lbc36no\", var = \"bmi\")+\n  xlab(\"Wave\")+\n  ylab(\"bmi index\")"
  },
  {
    "objectID": "LBC_pheno.html#brain-age",
    "href": "LBC_pheno.html#brain-age",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Brain age",
    "text": "Brain age\n\n\nCode\n# read file\nfile = as.data.frame(foreign::read.spss(paste0(target, \"/BrainAgeVia2p1_AF_07DEC2023.sav\"), to.file.frame=T))\n\n# remove ID info when waves are indicated in \"JC_BrainAge_ID\"\nfile$JC_BrainAge_ID = as.numeric(gsub(\".*_w\", \"\", file$JC_BrainAge_ID))\n\n# rename columns more intuitively\nnames(file)[which(names(file) == \"JC_BrainAge_ID\")] = \"wave\"\nnames(file)[which(names(file) == \"JCBA_brain_age_W2\")] = \"brainAgeEst\"\n\n# read in age info \nageInfo = as.data.frame(foreign::read.spss(\"LBC1936_BrainAtrophy_AF_07NOV2023.sav\"))\n# strange name formatting\nageInfo$lbc36no =gsub(\" \", \"\", ageInfo$lbc36no)\n\n# merge info \nfile = merge(file, ageInfo[, c(\"lbc36no\", \"agedays_w2\")], by = \"lbc36no\")\n\n# get age in days in years to match estimated brain age\nfile$agedays_w2 = file$agedays_w2/365\n\n# get brain age gap which is the difference between brain age and chronological age\n# positive value should mean the particpant has a healthier looking brain tthan expected given their age \n# negative value should mean the participant has an unhealthier looking brain than expected\nfile$brainAge = file$agedays_w2 - file$brainAgeEst\n\n# summary stats\nsummary(file$brainAge)\nhist(file$brainAge)\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\",\"brainAge\")], file = paste0(wd, \"/LBC1936_brainAge.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#stroke",
    "href": "LBC_pheno.html#stroke",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Stroke",
    "text": "Stroke\n\n\nCode\n# set path to where data was saved \ntarget=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\n\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AF_07NOV2023.sav\"), to.data.frame=T)\n\n# names(file)[grep(\"stroke\", names(file))]\n\n# first of all, all No\nfile$strokemask_life = 0\n# identify people who had no scan\nfile$strokemask_life[which(is.na(file$stroke_mask_w2) & \n                             is.na(file$stroke_mask_w3) &\n                             is.na(file$stroke_mask_w4) &\n                             is.na(file$stroke_mask_w5))] = NA\n\nfile$strokemask_life[which(file$stroke_mask_w2 == \"Yes Stroke Mask - Had scan\" | \n                               file$stroke_mask_w3 == \"Yes Stroke Mask - Had scan\" |\n                               file$stroke_mask_w4 == \"Yes Stroke Mask - Had scan\" | \n                               file$stroke_mask_w5 == \"Yes Stroke Mask - Had scan\")] = 1\n\n\nfile$strokemask_life <- as.factor(file$strokemask_life)\n\ntable(file$strokemask_life)\n# No Yes \n# 544 156 \n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\",\"strokemask_life\")], file = paste0(wd, \"/LBC1936_strokemask.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#visual-rating-scales",
    "href": "LBC_pheno.html#visual-rating-scales",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Visual rating scales",
    "text": "Visual rating scales\nNote we did not have access to rating scales at wave 5, only wave 3.\n\n\nCode\n# set path to where data was saved \ntarget=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\n\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AtrophyRating_AF_03MAY2024.sav\"), to.data.frame=T)\n\n# strange name formatting\nfile$lbc36no = gsub(\" \", \"\", file$lbc36no)\n\n# atrophy deep and superficial are in great agreement (only 80 participants where they are not identical)\nsum(file$atrophy_deep_w3 == file$atrophy_superficial_w3, na.rm=T)\n\n# recode so the scales are numeric\nfile$atrophy_deep_recoded = NA\nfile$atrophy_deep_recoded[grepl(\"(<25th)\", file$atrophy_deep_w3)] = 1\nfile$atrophy_deep_recoded[grepl(\"(25-50th)\", file$atrophy_deep_w3)] = 2\nfile$atrophy_deep_recoded[grepl(\"(50-75th)\", file$atrophy_deep_w3)] = 3\nfile$atrophy_deep_recoded[grepl(\"(75-95th)\", file$atrophy_deep_w3)] = 4\nfile$atrophy_deep_recoded[grepl(\"(>95th)\", file$atrophy_deep_w3)] = 5\nfile$atrophy_deep_recoded[grepl(\"(>>5)\", file$atrophy_deep_w3)] = 6\n\n# same for atrophy superficial\nfile$atrophy_superficial_recoded = NA\nfile$atrophy_superficial_recoded[grepl(\"(<25th)\", file$atrophy_superficial_w3)] = 1\nfile$atrophy_superficial_recoded[grepl(\"(25-50th)\", file$atrophy_superficial_w3)] = 2\nfile$atrophy_superficial_recoded[grepl(\"(50-75th)\", file$atrophy_superficial_w3)] = 3\nfile$atrophy_superficial_recoded[grepl(\"(75-95th)\", file$atrophy_superficial_w3)] = 4\nfile$atrophy_superficial_recoded[grepl(\"(>95th)\", file$atrophy_superficial_w3)] = 5\nfile$atrophy_superficial_recoded[grepl(\"(>>5)\", file$atrophy_superficial_w3)] = 6\n\n\n# write.table to dat\nwrite.table(file[,c(\"lbc36no\",\"atrophy_deep_recoded\",\"atrophy_superficial_recoded\")], file = paste0(target, \"/LBC1936_atrophyScales.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#age",
    "href": "LBC_pheno.html#age",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Age",
    "text": "Age\n\n\nCode\n# read data\nfile = foreign::read.spss(paste0(target, \"/LBC1936_BrainAtrophy_AF_07NOV2023.sav\"), to.data.frame=T)\n\n# strange name formatting\nfile$lbc36no = gsub(\" \", \"\", file$lbc36no)\n\n# keep age variable for first neuroimaging visit\nfile = file[,c(\"lbc36no\", \"ageMRI_w2\")]\n\n# save file\nwrite.table(file, file = paste0(target, \"/LBC1936_age_w2.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#merge-all-lbc-variables-into-one-file",
    "href": "LBC_pheno.html#merge-all-lbc-variables-into-one-file",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Merge all LBC variables into one file",
    "text": "Merge all LBC variables into one file\n\n\nCode\n# it makes it more straightforward to conduct the following analyses if I merge all phenotypes into one file\n# Step 1: Read all phenotypes in\n# Step 2: Merge them\n# Step 3: Save\n\n# cognitive ability\ncog = fread(paste0(wd, \"/LBC1936_Cog_FactorScores.txt\"))\n# dementia\ndement = fread(paste0(wd, \"/LBC1936_dementia.txt\"))\n# APOE\nAPOE = fread(paste0(wd, \"/LBC1936_APOEe4.txt\"))\n# Frailty\nfrail = fread(paste0(wd, \"/LBC1936_Frailty.txt\"))\n# diabetes\ndiab = fread(paste0(wd, \"/LBC1936_diabetes.txt\"))\n# hyp\nhyp = fread(paste0(wd, \"/LBC1936_hypertension.txt\"))\n# packyears\nsmok = fread(paste0(wd, \"/LBC1936_packYears.txt\"))\n# bmi\nbmi = fread(paste0(wd, \"/LBC1936_bmi.txt\"))\n# brain age\nBrainAge = fread(paste0(wd, \"/LBC1936_brainAge.txt\"))\n# stroke\nstroke = fread(paste0(wd, \"/LBC1936_strokemask.txt\"))\n# atrophy scales\natrophy = fread(paste0(wd, \"/LBC1936_atrophyScales.txt\"))\n# age\nage = fread(paste0(wd, \"/LBC1936_age_w2.txt\"))\n\n# merge data\nDatList = list(cog, dement, APOE, frail, diab, hyp, smok, bmi, BrainAge, stroke, atrophy, age)\nLBC_merged = Reduce(function(x,y) merge(x, y, by = \"lbc36no\", all = T), DatList)\n\n# remove empty rows\nLBC_merged = LBC_merged[-which(rowSums(!is.na(LBC_merged)) == 0),]\n\n# choose prettier names\nnames(LBC_merged) = c(\"lbc36no\",\"iCog\",\"sCog\",\"dementia\",\"APOEe4\",\"iFrailty\",\"sFrailty\",\"diabetes\",\"hypertension\",\"packyears\",\"iBMI\",\"sBMI\",\"BrainAge\",\"Stroke\",\"VisualAtrophyDeep\",\"VisualAtrophySuperficial\",\"ageMRI_w2\")\n\n# remove empty rows\nLBC_merged = LBC_merged[rowSums(is.na(LBC_merged)) != ncol(LBC_merged),]\n\n# write\nfwrite(LBC_merged, file = paste0(wd, \"/LBC1936_allPheno.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "LBC_pheno.html#function-for-longitudinal-plots",
    "href": "LBC_pheno.html#function-for-longitudinal-plots",
    "title": "LBC1936: Phenotypic data preparation",
    "section": "Function for longitudinal plots",
    "text": "Function for longitudinal plots\n\n\nCode\n# write function to plot longitudinal  data\nplot_long = function(dat = data, id.var = \"lbc36no\", var = \"matreas\"){\n\n    # make sure dat is data.frame\n    dat = as.data.frame(dat)\n  # select data for the chosen cognitive test\n  dat = dat[,c(which(names(dat) == id.var), grep(var, names(dat)))]\n\n  # transform wide to long format\n  long <- reshape2::melt(dat, id.vars = id.var, value.name = var)\n  names(long)[which(names(long) == \"variable\")] = \"Wave\"\n  names(long)[which(names(long) == var)] = \"var\"\n  names(long)[which(names(long) == id.var)] = \"id.var\"\n  # remove redundant naming from waves\n  long$Wave = as.numeric(sub(\".*_w\", \"\", long$Wave))\n\n  plot = ggplot(data = long, aes(x = Wave, y = var, group = id.var))+\n  geom_point(color = \"#82A0D8\", size = .5)+\n  geom_line(aes(group=as.factor(id.var)),method=\"lm\", se=F, color = \"#8DDFCB\", size = 0.2, alpha = .2, stat =  \"smooth\") +\n    theme(legend.position = \"none\")+\n    theme_bw()+\n    theme(text = element_text(size=20),\n          plot.margin=unit(c(1, 1, 1, 1), \"cm\"),\n          axis.text.y = element_text(size =20),\n          axis.text.x = element_text(size =20),\n          panel.border = element_blank())\n\n  return(plot)\n}"
  },
  {
    "objectID": "LBC_neuro.html",
    "href": "LBC_neuro.html",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "",
    "text": "Code displayed here was used to obtain neuroimaging measures: TBV, ICV, LBA (difference, ratio, residual scores). These measures were obtained for all waves from cross-sectionally processed data, and from longitudinal data considering waves 2 and 5.\nThe LBC neuroimaging data was processed with FS v5.1, which does not produce BrainSegNotVent estimates that we pre-registered to use across all samples. Instead, we derive TBV as the sum of GMV (cortical and subcortical should also include cerebellum) + cerebellum WMV + cerebral WMV, as was done in a previous paper. One participant was excluded because TBV estimate was larger than ICV estimate - total of 269 participants with two assessments."
  },
  {
    "objectID": "LBC_neuro.html#extract-from-cross-sectional-fs-processing-stream-output",
    "href": "LBC_neuro.html#extract-from-cross-sectional-fs-processing-stream-output",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "Extract from cross-sectional FS processing stream output",
    "text": "Extract from cross-sectional FS processing stream output\nData from the cross-sectional processing stream is used in all cases where samples are compared, as well as in analyses where cross-sectional and longitudinal measures are directly compared.\nThe naming is kind of confusing and here I am using the naming as the folder are called. There are wave 1, wave 2, wave 3 and wave 4 (which technically are waves 2,3,4,5 and scans 1,2,3,4), but that’s what they were called during processing.\n\nTabulate FS data\n\n\nCode\n#!/bin/bash\n# Extract FreeSurfer variables (volume, area, thickness) for our LBC1936 W2 (scan 1) cross-sectional subjects\n# Colin Buchanan, 2022\n# Colin's script was adapted to extract cross-sectional estimates from LBC at wave 5\n\n\n# first, create subjects list of participants with wave 5 data\n#R\n\n#setwd(\"/Brain_Imaging/LBC1936_FS_long/LBC_long_W4\")\n# note that all subjects in this directory without a wave are the templates and the ones with a wave but without 'long' should be the longitudinally processed waves\n# list dirs\n#dirs = dir()\n# keep only W4 because it's visit 5\n#dirs = dirs[grepl(\"W4\", dirs)]\n# remove long scans\n#dirs = dirs[!grepl(\".long.\", dirs)]\n\n#write.table(dirs, \"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave3.csv\", col.names=F, row.names=F, quote=F, sep=\"\\t\")\n## use this ID list to extract measurements from FS dirs\n\n\nFREESURFER_HOME=/Cluster_Filespace/mharris4/LBC_long_W4/freesurfer510\n$FREESURFER_HOME/SetUpFreeSurfer.sh\nout=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\nref1=\"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave1.csv\"\nref2=\"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave2.csv\"\nref3=\"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave3.csv\"\nref4=\"/CCACE_Shared/Anna_F/BrainAtrophy/scripts/LBClong/subjects_wave4.csv\"\n\n\nSUBJECTS_DIR=/Brain_Imaging/LBC1936_FS_long/LBC_long_W4\n\n# Wave1 \nasegstats2table --subjectsfile $ref1 --meas volume --common-segs --delimiter comma --tablefile \"${out}/LBC1936_global_w1_cross.csv\"\n\n# Wave2 \nasegstats2table --subjectsfile $ref2 --meas volume --common-segs --delimiter comma --tablefile \"${out}/LBC1936_global_w2_cross.csv\"\n\n# Wave3\nasegstats2table --subjectsfile $ref3 --meas volume --common-segs --delimiter comma --tablefile \"${out}/LBC1936_global_w3_cross.csv\"\n\n# Wave4\nasegstats2table --subjectsfile $ref4 --meas volume --common-segs --delimiter comma --tablefile \"${out}/LBC1936_global_w4_cross.csv\"\n\n\n\n\nFormat Wave 1\n\n\nCode\n# read in cross-stats\n## wave 1\ncrossDir=\"/Brain_Imaging/LBC1936_FS_long/freesurfer_crosssect_stats\"\ncross = fread(paste0(crossDir, \"/global_w2.csv\"), data.table=F)\n# the naming here is super confusing because it switches from wave 1 to scan 1 ...\n# but the actual participant names definitively say which wave this scan is from and it's (wave 1 naming, which corresponds to the second wave but first scan - so confusing)\n# I will name it below to reflect the folder names (i.e., wave 1 in this case)\nnames(cross) = gsub(\"-\", \".\", names(cross), fixed = T)\n\n# format each of those variables to long format\n## TotalGrayVol\n## Right.Cerebellum.White.Matter\n## Left.Cerebellum.White.Matter\ncrossLBC = cross[ , c(1, grep(\"TotalGrayVol|Right.Cerebellum.White.Matter|Left.Cerebellum.White.Matter|CorticalWhiteMatterVol|CSF|IntraCranialVol\", names(cross))) ]\n# remove lh & rhCorticalWhiteMatterVol (because whole measure is also included)\ncrossLBC = crossLBC[, !grepl(\"rh|lh\", names(crossLBC))]\n\n# calculate sum of the regions\ncrossLBC$TBV = rowSums(crossLBC[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n\n# rename for easier names\nnames(crossLBC)[grep(\"Measure:volume\", names(crossLBC))] = \"lbc36no\"\nnames(crossLBC)[grep(\"IntraCranialVol\", names(crossLBC))] = \"ICV\"\n\ncrossLBC = crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\")]\n\n# two participants have a smaller ICV than TBV sum(crossLBC$ICV - crossLBC$TBV < 0)\n# must be an error (LBC360213 & LBC361303)\ncrossLBC = crossLBC[-which(crossLBC$ICV - crossLBC$TBV <0),]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\ncrossLBC$ICV = crossLBC$ICV/1000\ncrossLBC$TBV = crossLBC$TBV/1000\n\n# estimate brain atrophy from single MRI scan\ncrossLBC$diff = crossLBC$ICV - crossLBC$TBV\ncrossLBC$ratio = crossLBC$TBV / crossLBC$ICV\n\n##### derive the residuals for each time point separately \nmodel <- lm(TBV ~ ICV, data = crossLBC)\ncrossLBC$resid = resid(model)\n\n# standardise variables within one time-point\ncrossLBC$resid_stand = as.vector(scale(crossLBC$resid))\ncrossLBC$diff_stand = as.vector(scale(crossLBC$diff))\ncrossLBC$ratio_stand = as.vector(scale(crossLBC$ratio))\ncrossLBC$TBVstand = as.vector(scale(crossLBC$TBV))\ncrossLBC$ICVstand = as.vector(scale(crossLBC$ICV))\ncrossLBC$CSFstand = as.vector(scale(crossLBC$CSF))\n\n# rename participant labels to match global naming\ncrossLBC$lbc36no = stringr::str_remove(crossLBC$lbc36no, pattern = \"_W1\")\n\n# store as txt file\nfwrite(crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\")], paste0(wd, \"/LBC1936_crossNeuroWave1.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nFormat Wave 2\n\n\nCode\n# read in cross-stats\n## wave 2\ncrossDir=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\ncross = fread(paste0(crossDir, \"/LBC1936_global_w2_cross.csv\"), data.table=F)\nnames(cross) = gsub(\"-\", \".\", names(cross), fixed = T)\n\n# format each of those variables to long format\n## TotalGrayVol\n## Right.Cerebellum.White.Matter\n## Left.Cerebellum.White.Matter\ncrossLBC = cross[ , c(1, grep(\"TotalGrayVol|Right.Cerebellum.White.Matter|Left.Cerebellum.White.Matter|CorticalWhiteMatterVol|CSF|IntraCranialVol\", names(cross))) ]\n# remove lh & rhCorticalWhiteMatterVol (because whole measure is also included)\ncrossLBC = crossLBC[, !grepl(\"rh|lh\", names(crossLBC))]\n\n# calculate sum of the regions\ncrossLBC$TBV = rowSums(crossLBC[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n\n# rename for easier names\nnames(crossLBC)[grep(\"Measure:volume\", names(crossLBC))] = \"lbc36no\"\nnames(crossLBC)[grep(\"IntraCranialVol\", names(crossLBC))] = \"ICV\"\n\ncrossLBC = crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\")]\n\n# two participants have a smaller ICV than TBV sum(crossLBC$ICV - crossLBC$TBV < 0)\n# must be an error (LBC360213 & LBC361303)\ncrossLBC = crossLBC[-which(crossLBC$ICV - crossLBC$TBV <0),]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\ncrossLBC$ICV = crossLBC$ICV/1000\ncrossLBC$TBV = crossLBC$TBV/1000\n\n# estimate brain atrophy from single MRI scan\ncrossLBC$diff = crossLBC$ICV - crossLBC$TBV\ncrossLBC$ratio = crossLBC$TBV / crossLBC$ICV\n\n##### derive the residuals for each time point separately \nmodel <- lm(TBV ~ ICV, data = crossLBC)\ncrossLBC$resid = resid(model)\n\n# standardise variables within one time-point\ncrossLBC$resid_stand = as.vector(scale(crossLBC$resid))\ncrossLBC$diff_stand = as.vector(scale(crossLBC$diff))\ncrossLBC$ratio_stand = as.vector(scale(crossLBC$ratio))\ncrossLBC$TBVstand = as.vector(scale(crossLBC$TBV))\ncrossLBC$ICVstand = as.vector(scale(crossLBC$ICV))\ncrossLBC$CSFstand = as.vector(scale(crossLBC$CSF))\n\n# rename participant labels to match global naming\ncrossLBC$lbc36no = stringr::str_remove(crossLBC$lbc36no, pattern = \"_W2\")\n\n# store as txt file\nfwrite(crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\")], paste0(wd, \"/LBC1936_crossNeuroWave2.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nFormat Wave 3\n\n\nCode\n# read in cross-stats\n## wave 3\ncrossDir=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\ncross = fread(paste0(crossDir, \"/LBC1936_global_w3_cross.csv\"), data.table=F)\nnames(cross) = gsub(\"-\", \".\", names(cross), fixed = T)\n\n# format each of those variables to long format\n## TotalGrayVol\n## Right.Cerebellum.White.Matter\n## Left.Cerebellum.White.Matter\ncrossLBC = cross[ , c(1, grep(\"TotalGrayVol|Right.Cerebellum.White.Matter|Left.Cerebellum.White.Matter|CorticalWhiteMatterVol|CSF|IntraCranialVol\", names(cross))) ]\n# remove lh & rhCorticalWhiteMatterVol (because whole measure is also included)\ncrossLBC = crossLBC[, !grepl(\"rh|lh\", names(crossLBC))]\n\n# calculate sum of the regions\ncrossLBC$TBV = rowSums(crossLBC[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n\n# rename for easier names\nnames(crossLBC)[grep(\"Measure:volume\", names(crossLBC))] = \"lbc36no\"\nnames(crossLBC)[grep(\"IntraCranialVol\", names(crossLBC))] = \"ICV\"\n\ncrossLBC = crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\")]\n\n# no participants have smaller ICV than TBV\n#crossLBC = crossLBC[-which(crossLBC$ICV - crossLBC$TBV <0),]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\ncrossLBC$ICV = crossLBC$ICV/1000\ncrossLBC$TBV = crossLBC$TBV/1000\n\n# estimate brain atrophy from single MRI scan\ncrossLBC$diff = crossLBC$ICV - crossLBC$TBV\ncrossLBC$ratio = crossLBC$TBV / crossLBC$ICV\n\n##### derive the residuals for each time point separately \nmodel <- lm(TBV ~ ICV, data = crossLBC)\ncrossLBC$resid = resid(model)\n\n# standardise variables within one time-point\ncrossLBC$resid_stand = as.vector(scale(crossLBC$resid))\ncrossLBC$diff_stand = as.vector(scale(crossLBC$diff))\ncrossLBC$ratio_stand = as.vector(scale(crossLBC$ratio))\ncrossLBC$TBVstand = as.vector(scale(crossLBC$TBV))\ncrossLBC$ICVstand = as.vector(scale(crossLBC$ICV))\ncrossLBC$CSFstand = as.vector(scale(crossLBC$CSF))\n\n# rename participant labels to match global naming\ncrossLBC$lbc36no = stringr::str_remove(crossLBC$lbc36no, pattern = \"_W3\")\n\n# store as txt file\nfwrite(crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\")], paste0(wd, \"/LBC1936_crossNeuroWave3.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nFormat Wave 4\n\n\nCode\n# read in cross-stats\n## wave 4\ncrossDir=\"/CCACE_Shared/Anna_F/BrainAtrophy/data\"\ncross = fread(paste0(crossDir, \"/LBC1936_global_w4_cross.csv\"), data.table=F)\nnames(cross) = gsub(\"-\", \".\", names(cross), fixed = T)\n\n# format each of those variables to long format\n## TotalGrayVol\n## Right.Cerebellum.White.Matter\n## Left.Cerebellum.White.Matter\ncrossLBC = cross[ , c(1, grep(\"TotalGrayVol|Right.Cerebellum.White.Matter|Left.Cerebellum.White.Matter|CorticalWhiteMatterVol|CSF|IntraCranialVol\", names(cross))) ]\n# remove lh & rhCorticalWhiteMatterVol (because whole measure is also included)\ncrossLBC = crossLBC[, !grepl(\"rh|lh\", names(crossLBC))]\n\n# calculate sum of the regions\ncrossLBC$TBV = rowSums(crossLBC[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n\n# rename for easier names\nnames(crossLBC)[grep(\"Measure:volume\", names(crossLBC))] = \"lbc36no\"\nnames(crossLBC)[grep(\"IntraCranialVol\", names(crossLBC))] = \"ICV\"\n\ncrossLBC = crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\")]\n\n# no participants have smaller ICV than TBV\n#crossLBC = crossLBC[-which(crossLBC$ICV - crossLBC$TBV <0),]\n\n### calculate atrophy measures\n# convert mm3 estimates to more intuitive cm3 estimates\ncrossLBC$ICV = crossLBC$ICV/1000\ncrossLBC$TBV = crossLBC$TBV/1000\n\n# estimate brain atrophy from single MRI scan\ncrossLBC$diff = crossLBC$ICV - crossLBC$TBV\ncrossLBC$ratio = crossLBC$TBV / crossLBC$ICV\n\n##### derive the residuals for each time point separately \nmodel <- lm(TBV ~ ICV, data = crossLBC)\ncrossLBC$resid = resid(model)\n\n# standardise variables within one time-point\ncrossLBC$resid_stand = as.vector(scale(crossLBC$resid))\ncrossLBC$diff_stand = as.vector(scale(crossLBC$diff))\ncrossLBC$ratio_stand = as.vector(scale(crossLBC$ratio))\ncrossLBC$TBVstand = as.vector(scale(crossLBC$TBV))\ncrossLBC$ICVstand = as.vector(scale(crossLBC$ICV))\ncrossLBC$CSFstand = as.vector(scale(crossLBC$CSF))\n\n# rename participant labels to match global naming\ncrossLBC$lbc36no = stringr::str_remove(crossLBC$lbc36no, pattern = \"_W4\")\n\n# store as txt file\nfwrite(crossLBC[,c(\"lbc36no\", \"ICV\", \"TBV\", \"CSF\", \"diff\", \"ratio\", \"resid\", \"ICVstand\", \"TBVstand\", \"CSFstand\", \"resid_stand\", \"diff_stand\", \"ratio_stand\")], paste0(wd, \"/LBC1936_crossNeuroWave4.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nVisualise cross-secional scores (waves 2 and 5)\n\n\nCode\n# read in LBC neuroimaging data \nLBC2 = fread(paste0(wd, \"/LBC1936_crossNeuroWave1.txt\"))\nLBC2$wave = 2\nLBC5 = fread(paste0(wd, \"/LBC1936_crossNeuroWave4.txt\"))\nLBC5$wave = 5\nLBC = rbind(LBC2, LBC5)\n\np1 = plot_hist(dat = LBC, var = \"diff\", split_sample_by = \"wave\")+\n          xlab(\"ICV - TBV\\n(raw difference score)\")+\n          theme(legend.position=\"none\")+\n          make_pretty()\n# delete SD stats\np1$layers[[2]]=NULL\n\np2 = plot_hist(dat = LBC, var = \"ratio\", split_sample_by = \"wave\")+\n          xlab(\"TBV / ICV\\n(raw ratio score)\")+\n          theme(legend.position=\"none\")+\n          make_pretty()\n# delete SD stats\np2$layers[[2]]=NULL\n\np3 = plot_hist(dat = LBC, var = \"resid\", split_sample_by = \"wave\")+\n          xlab(\"TBV ~ ICV\\n(raw residual score)\")+\n          make_pretty()\n# delete SD stats\np3$layers[[2]]=NULL\n\n# combine plots\nplot = plot_grid(p1, p2, p3, nrow = 1, labels = c(\"A\", \"B\", \"C\"), label_size = 6, rel_widths = c(1,1,1.5))\n# save plot\nggsave(paste0(wd, \"LBCDistributionsCross.jpg\"), plot = plot, width = 12, height = 5, units = \"cm\", dpi = 300)\n\n\n\n\n\nDistribution of atrophy scores derived from cross-sectional data\n\n\n\n\nDescriptive statistics (cross-sectional measures)\n\nCode\noptions(knitr.kable.NA = \"\")\n\n# calculate descriptives\ndes = descriptives(samples = c(\"LBC2\", \"LBC5\"))\n# cut-offs not needed\ndes = des[-which(des$Statistic == \"Cut off\"),]\n\nknitr::kable(des, col.names = c(\"Stats\",\"LBC (wave 1)\", \"LBC (wave 4)\"))"
  },
  {
    "objectID": "LBC_neuro.html#extract-from-longitudinal-fs-processing-stream-output",
    "href": "LBC_neuro.html#extract-from-longitudinal-fs-processing-stream-output",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "Extract from longitudinal FS processing stream output",
    "text": "Extract from longitudinal FS processing stream output\nLBC1936 provides data that was processed with the longitudinal processing stream.\nData from the longitudinal processing stream should be used when we are interested in inter-individual changes across time (i.e., analyses not involving ICV).\n\n\nCode\n# https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.25572\n# this paper defines total brain volume as:\n# GMV (cortical and subcortical shoudl also include cerebellum) + cerebellum WMV + cerebral WMV\n# TotalGrayVol + Cerebellum.White.Matter + CorticalWhiteMatterVol\nint=\"/Brain_Imaging/LBC1936_FS_long/freesurfer_stats\"\nLBC = read.table(paste0(int,\"/freesurfer_stats_long_w2to5.csv\"), header=T, sep=\",\")\n\n# format each of those variables to long format\n## TotalGrayVol\nTotalGray = LBC[,c(1,grep(\"TotalGrayVol\", names(LBC)))]\nTotalGray = reshape2::melt(TotalGray, id = \"lbc36no\", variable = \"wave\")\nnames(TotalGray)[grep(\"value\", names(TotalGray))] = \"TotalGrayVol\"\nTotalGray$wave = as.numeric(str_remove(TotalGray$wave, pattern = \"TotalGrayVol_w\"))\n\n## Right.Cerebellum.White.Matter\nRCerebellumWM = LBC[,c(1,grep(\"Right.Cerebellum.White.Matter\", names(LBC)))]\nRCerebellumWM = reshape2::melt(RCerebellumWM, id = \"lbc36no\", variable = \"wave\")\nnames(RCerebellumWM)[grep(\"value\", names(RCerebellumWM))] = \"Right.Cerebellum.White.Matter\"\nRCerebellumWM$wave = as.numeric(str_remove(RCerebellumWM$wave, pattern = \"Right.Cerebellum.White.Matter_w\"))\n\n## Left.Cerebellum.White.Matter\nLCerebellumWM = LBC[,c(1,grep(\"Left.Cerebellum.White.Matter\", names(LBC)))]\nLCerebellumWM = reshape2::melt(LCerebellumWM, id = \"lbc36no\", variable = \"wave\")\nnames(LCerebellumWM)[grep(\"value\", names(LCerebellumWM))] = \"Left.Cerebellum.White.Matter\"\nLCerebellumWM$wave = as.numeric(str_remove(LCerebellumWM$wave, pattern = \"Left.Cerebellum.White.Matter_w\"))\n\n## CorticalWhiteMatterVol\n# find column names first\ncols = names(LBC)[c(1,grep(\"CorticalWhiteMatterVol\", names(LBC)))]\ncols = cols[-grep(\"rh\", cols)]\ncols = cols[-grep(\"lh\", cols)]\n# subset data\nCorticalWhite = LBC[,cols]\nCorticalWhite = reshape2::melt(CorticalWhite, id = \"lbc36no\", variable = \"wave\")\nnames(CorticalWhite)[grep(\"value\", names(CorticalWhite))] = \"CorticalWhiteMatterVol\"\nCorticalWhite$wave = as.numeric(str_remove(CorticalWhite$wave, pattern = \"CorticalWhiteMatterVol_w\"))\n\n## IntraCranialVol\nIntraCran = LBC[,c(1,grep(\"IntraCranialVol\", names(LBC)))]\nIntraCran = reshape2::melt(IntraCran, id = \"lbc36no\", variable = \"wave\")\nnames(IntraCran)[grep(\"value\", names(IntraCran))] = \"ICV\"\nIntraCran$wave = as.numeric(str_remove(IntraCran$wave, pattern = \"IntraCranialVol_w\"))\n#### only looking at IntraCranVol to see if ICV is stable across time - which it is\n# going forward, I will not use ICV from the longitudinal scans  because it is not suitable for cross-person comparisons - here we can only look at within-person changes of TBV\n# for that reason I am not including IntraCran in the merge list below\n\n#### merge the different variables\nDatList = list(TotalGray, RCerebellumWM, LCerebellumWM, CorticalWhite) \nLBC_merged = Reduce(function(x,y) merge(x, y, by = c(\"lbc36no\", \"wave\"), all = T), DatList)\n# no need for time points 3 and 4 for our study\nLBC_merged = LBC_merged[-which(LBC_merged$wave == 3),]\nLBC_merged = LBC_merged[-which(LBC_merged$wave == 4),]\n# get rid of all missing values because we can't confidently compute TBV if some parts of the equation re missing\nLBC_merged = na.omit(LBC_merged)\n\n# calculate sum of the regions\nLBC_merged$TBV = rowSums(LBC_merged[,c(\"TotalGrayVol\", \"Right.Cerebellum.White.Matter\", \"Left.Cerebellum.White.Matter\", \"CorticalWhiteMatterVol\")], na.rm = F)\n#length(unique(LBC_merged$lbc36no))\n# 460\n\n###### include CSF\n# for the analyses in aim 3.1 we only need CSF at wave 5\nCSF = LBC[,c(1,grep(\"CSF_w5\", names(LBC)), grep(\"CSF_w2\", names(LBC)))]\nCSF = reshape2::melt(CSF, id = \"lbc36no\", variable = \"wave\")\nnames(CSF)[grep(\"value\", names(CSF))] = \"CSF\"\nCSF$wave = as.numeric(str_remove(CSF$wave, pattern = \"CSF_w\"))\n### also realised later that I would not want to include CSF measures from longitudinal processing as it's not comparable between participants - will only use CSF estimates from cross-setional processing\n\n# convert mm3 estimates to more intuitive cm3 estimates\nLBC_merged$TBV = LBC_merged$TBV/1000\n\n# store as txt file\nfwrite(LBC_merged[,c(\"lbc36no\", \"wave\", \"TBV\")], paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\nFormat longitudinal change measures\n\n\nCode\n# read in LBC data \nneuro = fread(paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), data.table = F)\n# extract longitudinal atrophy from LBC data\n#### Difference score \n# Step 1: change to wide format\ntemp = reshape(neuro, idvar = \"lbc36no\", timevar = \"wave\", direction = \"wide\")\ntemp = temp[,c(\"lbc36no\", \"TBV.2\", \"TBV.5\")]\n\n# Step 2: calculate difference in TBV between wave 2 and wave 5\ntemp$TBVdiff_2to5 = temp$TBV.2 - temp$TBV.5\n\n###### Ratio score\n# Step 2: calculate difference in TBV between wave 2 and wave 5\ntemp$TBVratio_5to2 = temp$TBV.5 / temp$TBV.2\n\n###### Resid score\n# remove missing because results with missing produces weird dimensions\ntemp1 = temp[!is.na(temp$TBV.2),]\ntemp1 = temp1[!is.na(temp1$TBV.5),] \n# Step 2: calculate difference in TBV between wave 2 and wave 5\nmodel = lm(TBV.5 ~ TBV.2, data = temp1)\ntemp1$TBVresid_2to5 = resid(model)\n\n# merge back in with complete temp\ntemp = merge(temp, temp1[,c(\"lbc36no\", \"TBVresid_2to5\")], by = \"lbc36no\", all = T)\n\n# Step 3: merge data back with long data\n# changed my mind about that - keeping long and cross data separate will make it easier to treat them distinctly\n# neuro = merge(neuro, temp[,c(\"lbc36no\", \"TBVdiff_2to5\", \"TBVratio_5to2\", \"TBVresid_2to5\")], by = \"lbc36no\", all = T)\n\n# standardise variables\nneuro = temp\nneuro$TBVdiff_2to5_stand = as.numeric(scale(neuro$TBVdiff_2to5))\nneuro$TBVratio_5to2_stand = as.numeric(scale(neuro$TBVratio_5to2))\nneuro$TBVresid_2to5_stand = as.numeric(scale(neuro$TBVresid_2to5))\n\n# store as txt file\nfwrite(neuro, paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), quote = F, col.names = T, sep = \"\\t\")\n\n\n\n\nVisualise longitudinal atrophy scores\n\n\nCode\n# read in LBC neuroimaging data \nLBC = fread(paste0(wd, \"/LBC1936_longTBVWaves2and5.txt\"), data.table=F)\n\n# get N\nN = sum(!is.na(LBC$TBVresid_2to5))\n\n# for the code to run, I will re-name columns, but this is just to not having to recode the plot_hist function - naming remains separate between cross-sectional and longitudinal variables in the saved data \nLBC$diff = LBC$TBVdiff_2to5\np1 = plot_hist(dat = LBC, var = \"diff\", split_sample_by = NULL)+\n          xlab(\"TBV1 - TBV2\\n(difference score)\")+\n            ylab(paste0(\"Count (N = \", N, \")\"))+\n          theme(legend.position=\"none\")+\n          make_pretty()\n# delete SD stats\np1$layers[[2]]=NULL\n\nLBC$ratio = LBC$TBVratio_5to2\np2 = plot_hist(dat = LBC, var = \"ratio\", split_sample_by = NULL)+\n          xlab(\"TBV2 / TBV1\\n(ratio score)\")+\n            ylab(paste0(\"Count (N = \", N, \")\"))+\n          theme(legend.position=\"none\")+\n          make_pretty()\n# delete SD stats\np2$layers[[2]]=NULL\n\nLBC$resid = LBC$TBVresid_2to5\np3 = plot_hist(dat = LBC, var = \"resid\", split_sample_by = NULL)+\n          xlab(\"TBV2 ~ TBV1\\n(residual score)\")+\n            ylab(paste0(\"Count (N = \", N, \")\"))+\n          make_pretty()\n# delete SD stats\np3$layers[[2]]=NULL\n\n# combine plots\nplot = plot_grid(p1, p2, p3, nrow = 1, labels = c(\"A\", \"B\", \"C\"), label_size = 6, rel_widths = c(1,1,1))\n# save plot\nggsave(paste0(wd, \"LBCDistributionsLong.jpg\"), plot = plot, width = 12, height = 5, units = \"cm\", dpi = 300)\n\n\n\n\n\nAtrophy measures in the LBC at two time points"
  },
  {
    "objectID": "LBC_neuro.html#load-packages",
    "href": "LBC_neuro.html#load-packages",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "Load packages",
    "text": "Load packages\n\n\nCode\nlibrary(cowplot)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(data.table)\nlibrary(stringr)\nlibrary(dplyr)"
  },
  {
    "objectID": "LBC_neuro.html#define-functions",
    "href": "LBC_neuro.html#define-functions",
    "title": "LBC1936: Neuroimaging data preparation",
    "section": "Define functions",
    "text": "Define functions\nFunctions plot_hist and descriptives expect input data set to contain variables called diff, ratio, resid. plot_hist can also handle diff_stand, ratio_stand, resid_stand and will add an extra x-axis if input are standardised variables.\n\n\nCode\nplot_hist <- function(dat = dat, var = \"diff_stand\", split_sample_by = NULL){\n  # install packages if they don't already exits\n  packages = c(\"ggplot2\",\"stringr\", \"tidyr\", \"dplyr\")\n  install.packages(setdiff(packages, rownames(installed.packages())))\n  # load packages\n  library(ggplot2)\n  library(stringr)\n  library(tidyr)\n  library(dplyr)\n\n  # make sure input data is data.frame\n  dat = as.data.frame(dat)\n  # rename for simplicity\n  dat$var = dat[,var]\n\n  # calculate summary stats\n    df_stats <-\n        dat %>%\n        summarize(\n          mean = mean(var, na.rm=T),\n          median = median(var, na.rm=T)\n        ) %>%\n        gather(key = Statistic, value = value, mean:median)\n\n    # calculate SD cutoffs\n    insert = c(\"+2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) + 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    insert = c(\"-2 SDs\", as.numeric(df_stats[which(df_stats$Statistic == \"mean\"), \"value\"]) - 2*sd(dat$var, na.rm=T))\n    df_stats <- rbind(df_stats, insert)\n\n    # format\n    df_stats$value <- as.numeric(df_stats$value)\n\n    # consider one-sided nature of cut-off\n    # if difference score, we use the upper 2 SD limit\n    # if ratio or residual score, we use the lower 2 SD limit\n    if(var == \"diff\" | var == \"diff_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"-2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }else if(var == \"ratio\" | var == \"resid\" | var == \"ratio_stand\" | var == \"resid_stand\"){\n      df_stats$value[which(df_stats$Statistic == \"+2 SDs\")]<-NA\n      # changed my mind, no need for median\n      df_stats <- df_stats[-which(df_stats$Statistic == \"median\"),]\n      # changed my mind, no need for mean either, it's just distracting\n      df_stats <- df_stats[-which(df_stats$Statistic == \"mean\"),]\n    }\n\n\n  # PLOT\n  # different output when there is a \"sample\" column\n  if(is.null(split_sample_by)){\n      plot = ggplot(dat, aes(x = var))+\n          geom_histogram(bins = 100, alpha = 0.5, fill = \"#56B4E9\")+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n\n\n  }else if(!is.null(split_sample_by)){\n\n    if(length(which(names(dat) == split_sample_by)) == 0){\n      message(paste0(\"You have indicated that you wanted to group plotted values by \", split_sample_by,\", but the data contains no such column.\")); break\n    }\n\n    # incorporate grouping variable\n    names(dat)[which(names(dat) == split_sample_by)] = \"split_sample_by\"\n    # make sure its a factor\n    dat$split_sample_by = as.factor(dat$split_sample_by)\n\n    colors = c(\"#56B4E9\",\"#009E73\", \"#E69F00\") # \"#79AC78\" #grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]\n    colors = colors[1:length(unique(dat$split_sample_by))]\n\n      plot = ggplot(dat)+\n          geom_histogram(aes(x = var, fill = split_sample_by), bins = 100, alpha = 0.5)+\n          scale_fill_manual(values = colors, name = split_sample_by)+\n          geom_vline(data = df_stats, aes(xintercept = value, color = Statistic), size = 0.5)+\n          xlab(var)+\n          ylab(\"Count\")+\n          theme_bw()\n  }\n\n    # make second x-axis if we're working with standardised variables\n    if(length(grep(\"_stand\", var)) != 0){\n\n      # calculate mean from original variable\n      varOr = str_remove(var, \"_stand\")\n      mean = mean(dat[,varOr], na.rm=T)\n      sd = sd(dat[,varOr], na.rm=T)\n\n      # add secondary x axis\n      plot = plot+\n         scale_x_continuous(sec.axis = sec_axis(name = \"Raw values\", trans=~.*sd+mean))\n\n    }\n\n  plot = plot+theme(panel.border = element_blank())\n\n  return(plot)\n}\n\n# this onyl works for the correct naming of the variable names to diff, ratio and resid\ndescriptives = function(samples = c(\"HCP\", \"Share\", \"both\")){\n  # define statistics to include\n  stats = c(\"N\", \"TBV: Mean (SD)\", \"ICV: Mean (SD)\", \"cor(ICV,TBV)\",\n            \"*Difference score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\",\n            \"*Ratio score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\",\n            \"*Residual score*\", \"Mean (SD)\", \"Median\", \"Range\", \"Variance\", \"Cut off\")\n\n  # object to hold results\n  res = as.data.frame(matrix(ncol = length(samples)+1, nrow = length(stats)))\n  names(res) = c(\"Statistic\", samples)\n  res$Statistic = stats\n\n  for(i in samples){\n    # pull sample\n    dat = as.data.frame(get(i))\n\n    # N\n    N = sum(!is.na(dat$diff))\n    res[which(res$Statistic == \"N\"), which(names(res) == i)] = N\n\n    # TBV: Mean (SD)\n    mean = round(mean(dat$TBV, na.rm = T), digits = 2)\n    SD = signif(sd(dat$TBV, na.rm = T), digits = 2)\n    res[which(res$Statistic == \"TBV: Mean (SD)\"), which(names(res) == i)] = paste0(mean, \" (\", SD,\")\")\n\n    # ICV: Mean (SD)\n    mean = round(mean(dat$ICV, na.rm = T), digits = 2)\n    SD = signif(sd(dat$ICV, na.rm = T), digits = 2)\n    res[which(res$Statistic == \"ICV: Mean (SD)\"), which(names(res) == i)] = paste0(mean, \" (\", SD,\")\")\n\n    # ICV TBV correlation\n    cor = round(cor.test(dat$ICV, dat$TBV)$estimate, digits = 2)\n    res[which(res$Statistic == \"cor(ICV,TBV)\"), which(names(res) == i)] = cor\n\n    # Cycle through different scores\n    for(j in c(\"Difference\", \"Ratio\", \"Resid\")){\n        # determine variable that matches the right score\n        if(j == \"Difference\"){\n          VarName = \"diff\"\n        }else if(j == \"Ratio\"){\n          VarName = \"ratio\"\n        }else if(j == \"Resid\"){\n          VarName = \"resid\"\n        }\n\n        dat$var = dat[,VarName]\n\n        ### Calculate mean and SD\n        mean = round(mean(dat$var, na.rm=T), digits = 2)\n        sd = round(sd(dat$var, na.rm=T), digits = 2)\n        # find correct position in res to store result\n        index = grep(j, res$Statistic)\n        Cand = grep(\"Mean\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        # store mean result\n        res[pos, which(names(res) == i)] = paste0(mean, \" (\", sd, \")\")\n\n        ### Calculate median\n        median = round(median(dat$var, na.rm=T), digits = 2)\n        #store median result\n        Cand = grep(\"Median\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = median\n\n        ### Calculate range\n        min = round(min(dat$var, na.rm = T), digits = 2)\n        max = round(max(dat$var, na.rm = T), digits = 2)\n        # store results\n        Cand = grep(\"Range\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = paste0(min, \" to \", max)\n\n        ## Calculate variance\n        variance = signif(var(dat$var, na.rm = T), digit = 2)\n        # store variance result\n        Cand = grep(\"Variance\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = variance\n\n        ### calculate cut-off\n        if(j == \"Difference\"){\n          cutOff = mean(dat$var, na.rm = T)+(2*sd(dat$var, na.rm = T))\n        }else{\n            cutOff = mean(dat$var, na.rm = T)-(2*sd(dat$var, na.rm = T))\n        }\n        # store results\n        Cand = grep(\"Cut\", res$Statistic)\n        pos = Cand[which(Cand > index)][1]\n        res[pos, which(names(res) == i)] = round(cutOff, digit = 1)\n    }\n  }\n\n  return(res)\n}"
  },
  {
    "objectID": "UKB_pheno.html",
    "href": "UKB_pheno.html",
    "title": "UKB: Phenotypic data preparation",
    "section": "",
    "text": "Data prepared here was used as input into analyses presented here. The file containing all phenotypic variables was named UKB_allPheno.txt."
  },
  {
    "objectID": "UKB_pheno.html#load-packages",
    "href": "UKB_pheno.html#load-packages",
    "title": "UKB: Phenotypic data preparation",
    "section": "Load packages",
    "text": "Load packages\n\n\nCode\nlibrary(data.table)\nlibrary(lavaan)"
  },
  {
    "objectID": "UKB_pheno.html#define-function",
    "href": "UKB_pheno.html#define-function",
    "title": "UKB: Phenotypic data preparation",
    "section": "Define function",
    "text": "Define function\nThe function getFieldLoc identifies the directory with the most recent UKB download. We have multiple files on the server from different downloads, and I want to be sure I use the most recent one (i.e., the one saved in a directory with the highest number).\n\n\nCode\n# script that searches in our available data if we have a certain data field\ngetFieldLoc = function(path = path, fileName = fileName, fieldID = fieldID){\n  \n  # read all the field.ukb files\n  files_to_read = list.files(\n    path = path,\n    pattern = fileName,\n    recursive = T,\n    full.names = T\n  )\n  # read all files\n  dat = lapply(files_to_read, fread)\n  names(dat) = files_to_read\n  \n  # search for field ID of interest \n  candidates = names(dat)[grep(fieldID, dat)]\n  # figure out which one is from the most recent file (i.e., highest number)\n  candidates = str_remove(candidates, paste0(path, \"/\"))\n  candidates = str_remove(candidates, paste0(\"/\", fileName))\n  candidates = unique(as.numeric(sapply(str_extract_all(candidates, \"\\\\d+\"), tail , 1)))\n  most_recent = max(candidates, na.rm =T)\n  \n  return(paste0(path, most_recent))\n}"
  },
  {
    "objectID": "UKB_pheno.html#cognitive-test-factor-scores",
    "href": "UKB_pheno.html#cognitive-test-factor-scores",
    "title": "UKB: Phenotypic data preparation",
    "section": "Cognitive test (factor scores)",
    "text": "Cognitive test (factor scores)\nInput data used here was kindly provided by Joanna Moodie who had modeled the g-factor for one of her previous projects. The factor was modeled on the whole UKB sample, and then restricted to the participants with available neuroimaging data. This was deemed appropriate because the variables were still normally distributed in the restricted sample. Modelling the factor in the full sample will hopefully mean that the factor was based on a more representative sample than relying on participants who returned for their second neuroimaigng visit (healthy selection bias).\n\n\nCode\n### This script was adapted from Joanna Moodies script\nlibrary(lavaan)\n\n# read in the data\nUKBcog <- read.csv(\"/CCACE_Shared/Joanna/ForShare/UKB_g/UKB_cogtests.csv\", sep = \" \")\n# name ID column\nnames(UKBcog)[which(names(UKBcog) == \"ID\")] = \"f.eid\"\n\n# re-formatting data\nUKBcog$cog_trailB_log[which(UKBcog$cog_trailB_log == 0)] <- NA\nUKBcog$cog_prosmem[which(UKBcog$cog_prosmem == 2)] <- 0\nUKBcog$cog_pairedAss <- exp(UKBcog$cog_pairedAss_log)\n\n#UKBcog <- merge(UKBcog, ' ', by = \"ID\") # select the participants you want in the sample\n\n# keep varibales of interest\nUKBcog = UKBcog[,c(\"f.eid\",\n                            \"cog_RT_log\",\n                            \"cog_numeric_memory\",\n                            \"cog_fluid_intelligence\",\n                            \"cog_trailB_log\",\n                            \"cog_matrix_pattern_correct\",\n                            \"cog_tower\",\n                            \"cog_digsym\",\n                            \"cog_pairedAss\",\n                            \"cog_prosmem\",\n                            \"cog_pairsmatch_incorrect_log\")]\n\n# define general factor model\ncogmodel <- 'g =~  cog_RT_log +\n                    cog_numeric_memory + \n                    cog_fluid_intelligence + \n                    cog_trailB_log + \n                    cog_matrix_pattern_correct + \n                    cog_tower + \n                    cog_digsym + \n                    cog_pairsmatch_incorrect_log +\n                    cog_prosmem +\n                    cog_pairedAss\n                    '\n\n# fit general factor model\nfit = cfa(cogmodel, data = UKBcog, missing=\"ML\")\nsummary(fit, fit.measures=TRUE, standardized=T)\n\n# save fit for later\nsave(fit, file = paste0(out,\"/fit.RData\"))\n\n# predict individual-level factor scores \ngPheno <- lavPredict(fit, UKBcog)\ngPheno <- as.data.frame(cbind(UKBcog$f.eid, gPheno))\n\n# re-name columns\nnames(gPheno) = c(\"f.eid\",\"g\")\n\n# hist(gPheno$g)\n\n# standardise\ngPheno$gStand = scale(gPheno$g)\n\n# save in separate file \nwrite.table(gPheno[,c(\"f.eid\", \"gStand\")], file = paste0(out, \"/UKB_gFactor.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#dementia-status",
    "href": "UKB_pheno.html#dementia-status",
    "title": "UKB: Phenotypic data preparation",
    "section": "Dementia status",
    "text": "Dementia status\nNote that according to this definition only 5 participants a diagnosis, and the remaining ones are proxy cases.\n\n\nCode\n## find items and where they are stored on the server\n## dementia is defined based on multiple sources of information we have in the UKV\n## 1. ICD-10 diagnoses: field ID 41270\n## 2. Self-reported illnesses (dementia, alzheimers, cognitive impairment): field ID 20002\n## 3. Source of all cause dementia report: field ID 42019\n## 4. Contributing causes of death: field ID 40001 & 40002\n## 5. Date F00 first reported: field ID 130836\n\n#############################################\n## 1. ICD-10 diagnoses: field ID 41270\n#############################################\n# determine file paths\npath1 = getFieldLoc(path = path, \n            fileName = \"fields.ukb\", \n            fieldID = 41270)\n\nfileID = list.files(pat=path1,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path1, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\n#names(file) = str_replace(names(file), pattern = \"-\", replacement = \"_\")\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = grep(\"f.41270\", names(file))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# format \"\" to be NA\nfor(i in names(dement)[grep(\"f.41270\", names(dement))]){\n  dement[which(dement[,i] == \"\"),i] = NA\n}\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(dement[,-which(names(dement) == \"f.eid\")])) == 0) \n# 55361\n\n# determine ICD codes of interest to find in field ID 41270\nICDcode=c(\"F00\",\"F000\",\"F001\",\"F002\",\"F009\",\"G30\",\"G300\",\"G301\",\"G308\",\"G309\",\"F01\",\"F010\",\"F011\",\"F012\",\"F013\",\"F018\",\"F019\",\"I673\",\"F03\",\"G311\",\"G318\")\n\n# match ICDcodes with entries in all f.41270 variables filtered in 'dement'\ndement$countCodes = rowSums(!is.na(sapply(dement[,-which(names(dement) == \"f.eid\")], match, ICDcode)))\n\n# if 1 or more entries, consisder a case\ndement$dementICD41270 = as.factor(ifelse(dement$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\ndement$dementICD41270[allMissing] = NA\n\n# save this variable to a seperate file\nfwrite(dement[,c(\"f.eid\", \"dementICD41270\")], file = paste0(out, \"/UKB_dement_ICD41270.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")\n\n#############################################\n## 2. Self-reported illnesses (dementia, alzheimers, cognitive impairment, code = 1263): field ID 20002\n#############################################\npath2=\"/UK_Biobank_New/Data/Raw_Data/1027_Refresh_Dec_2022/670476\" \nfileID = list.files(pat=path2,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path2, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\n#names(file) = str_replace(names(file), pattern = \"-\", replacement = \"_\")\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = grep(\"f.20002\", names(file))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# identify those who have fully missing data (except their participant ID)\nallMissing = which(rowSums(!is.na(dement[,-which(names(dement) == \"f.eid\")])) == 0) \n# 113066\n\n# determine code of interest to find in this field ID\ncode = \"1263\"\n\n# match ICDcodes with entries in all f.41270 variables filtered in 'dement'\ndement$countCodes = rowSums(!is.na(sapply(dement[,-which(names(dement) == \"f.eid\")], match,code)))\n\n# if 1 or more entries, consisder a case\ndement$dement20002 = as.factor(ifelse(dement$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\ndement$dement20002[allMissing] = NA\n\n# append final column to file\nfile$dement20002 = dement$dement20002\n\n#################################################\n## 3. Source of all cause dementia report: field ID 42019\n################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = grep(\"f.42019\", names(file))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# only record whether participants have an entry, which is indicative of dementia\n# however the absence of a code does not mean they don't have dementia\ndement$dement42019 = as.factor(ifelse(!is.na(dement$f.42019_0.0), 1, NA))\n\n# append final column to file\nfile$dement42019 = dement$dement42019\n\n###################################################\n## 4. Contributing causes of death: field ID 40001 & 40002\n###################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = c(grep(\"f.40001\", names(file)), grep(\"f.40002\", names(file)))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# format \"\" to be NA\nfor(i in names(dement)[c(grep(\"f.40001\", names(dement)), grep(\"f.40002\", names(dement)))]){\n  dement[which(dement[,i] == \"\"),i] = NA\n}\n\n# determine ICD codes of interest to find in field ID \nICDcode=c(\"F00\",\"F000\",\"F001\",\"F002\",\"F009\",\"G30\",\"G300\",\"G301\",\"G308\",\"G309\",\"F01\",\"F010\",\"F011\",\"F012\",\"F013\",\"F018\",\"F019\",\"I673\",\"F03\",\"G311\",\"G318\")\n\n# match ICDcodes with entries in all f.41270 variables filtered in 'dement'\ndement$countCodes = rowSums(!is.na(sapply(dement[,-which(names(dement) == \"f.eid\")], match, ICDcode)))\n\n# if 1 or more entries, consider a case\n# but if there is no entry, it probably more often means this person has not died yet, rather than that they had no dementia\ndement$dementDeath= as.factor(ifelse(dement$countCodes >= 1, 1, NA))\n\n# append final column to file\nfile$dementDeath = dement$dementDeath\n\n######################################\n## 5. Illnesses of father and mother (Alzheimer's disease/dementia; code = 10): field ID 20107 & 20110\n#####################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = c(grep(\"f.20107\", names(file)), grep(\"f.20110\", names(file)))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# remove all minus (-) values\ndement[dement <= 0] = NA\n\n# determine codes of interest to find in field ID \ncode=c(10)\n\n# match ICDcodes with entries in all f.41270 variables filtered in 'dement'\ndement$countCodes = rowSums(!is.na(sapply(dement[,-which(names(dement) == \"f.eid\")], match, code)))\n\n# if 1 or more entries, consider a case\n# this is a report on parents, so absence of entry does not mean they don't have dementia\ndement$dementParents = as.factor(ifelse(dement$countCodes >= 1, 1, NA))\n\n# append final column to file\nfile$dementParents = dement$dementParents\n\n\n################################################\n## 5. Date F00 first reported: field ID 130836\n###############################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\ndementCols = grep(\"f.130836\", names(file))\n\n# select columns of interest\ndement = as.data.frame(file[, c(..id, ..dementCols)])\n\n# store IDs for coded dates that suggest mistakes - however, doesnt contain any of the coded fields\n# which(dement$f.130836_0.0 == \"1901-01-01\")\n\n# record who has date \ndement$dement130836 = NA\ndement$dement130836[which(!is.na(dement$f.130836_0.0))] = 1\n\n# store in file\nfile$dement130836 = as.factor(dement$dement130836)\n\n################################################\n## Combine all data sources to one phenotype\n################################################\n# keep columns of interest\nfile = file[,c(\"f.eid\", \"dement20002\", \"dement42019\", \"dementDeath\",\"dementParents\",\"dement130836\")]\n\n# read in info from item 41270 (that was saved on a txt file - see (1.)) \nvar41270 = fread(paste0(out, \"/UKB_dement_ICD41270.txt\"), header = T)\nvar41270$dementICD41270 = as.factor(var41270$dementICD41270)\n\n# merge with info above\nfile = as.data.frame(merge(var41270, file, by = \"f.eid\"))\n\n# identify those with all missing\nallMissing = which(rowSums(!is.na(file[,-which(names(file) == \"f.eid\")])) == 0) \n\n# identify cases\nfile$dement = 0 \n\n# identify cases\nfile$dement[which(file$dement20002 == 1 |\n                file$dement42019 == 1 |\n                file$dementDeath == 1 |\n                file$dementParents == 1 |\n                file$dementICD41270 == 1 |\n                file$dement130836 == 1)] = 1\n\n\n# delete allMissing from controls\nfile$dement[allMissing] = NA\n\n# make factor\nfile$dement = as.factor(file$dement)\n\n#dementICD41270 dement20002   dement42019   dementDeath   dementParents  dement      \n#0   :442840    0   :389146   1   :  7896   1   :   860   1   : 66764   1   : 73835  \n#1   :  4156    1   :   156   NA's:494461   NA's:501497   NA's:435593   0   : 409281\n#NA's: 55361    NA's:113055                                             NA's: 19241\n\n# save final variable to a seperate file\nfwrite(file[,c(\"f.eid\", \"dement\")], file = paste0(out, \"/UKB_DementiaStatus.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#apoe-status",
    "href": "UKB_pheno.html#apoe-status",
    "title": "UKB: Phenotypic data preparation",
    "section": "APOE status",
    "text": "APOE status\nFirst extract the two SNPs of interest using PLINK2.\n\n\nCode\n# filter for the two APOE SNPs\nplink2 --bfile $loc/UKB500K_autosomes_X_XY_MAF_01_CLEAN_07052018_GD --snps rs7412,rs429358 --make-bed --freq --export A --out $temp/UKB_APOE\n\n\nThen, model APOE status according to the approach used here.\n\n\nCode\n# Determine whether someone has the e4 allele\n# read in data\ndat = read.table(paste0(temp, \"/UKB_APOE.raw\"), header = T)\n\n## simplify data\n# remove -IDs and remove redundant columns\ndat = dat[-which(dat$FID < 0),c(\"FID\", \"rs429358_T\", \"rs7412_C\")]\n\n# code variables to match look-up table\ndat$rs429358 = NA\ndat$rs429358[which(dat$rs429358_T == 2)] = \"TT\"\ndat$rs429358[which(dat$rs429358_T == 1)] = \"CT\"\ndat$rs429358[which(dat$rs429358_T == 0)] = \"CC\"\n\ndat$rs7412 = NA\ndat$rs7412[which(dat$rs7412_C == 2)] = \"CC\"\ndat$rs7412[which(dat$rs7412_C == 1)] = \"CT\"\ndat$rs7412[which(dat$rs7412_C == 0)] = \"TT\"\n\n# code variables according to combination of those genotypes\ndat$APOEgeno = NA\ndat$APOEgeno[which(dat$rs7412 == \"CC\" & dat$rs429358 == \"CC\")] = \"e4/e4\"\ndat$APOEgeno[which(dat$rs7412 == \"CC\" & dat$rs429358 == \"CT\")] = \"e3/e4\"\ndat$APOEgeno[which(dat$rs7412 == \"CT\" & dat$rs429358 == \"CT\")] = \"e2/e4\"\ndat$APOEgeno[which(dat$rs7412 == \"CC\" & dat$rs429358 == \"TT\")] = \"e3/e3\"\ndat$APOEgeno[which(dat$rs7412 == \"CT\" & dat$rs429358 == \"TT\")] = \"e2/e3\"\ndat$APOEgeno[which(dat$rs7412 == \"TT\" & dat$rs429358 == \"TT\")] = \"e2/e2\"\ndat$APOEgeno[which(dat$rs7412 == \"CT\" & dat$rs429358 == \"CC\")] = \"e1/e4\"\ndat$APOEgeno[which(dat$rs7412 == \"TT\" & dat$rs429358 == \"CT\")] = \"e1/e2\"\n\n\n# infer APOE status\ndat$APOEstatus = NA\ndat$APOEstatus[which(dat$APOEgeno == \"e4/e4\" | dat$APOEgeno == \"e3/e4\" | dat$APOEgeno == \"e2/e4\" | dat$APOEgeno == \"e1/e4\")] = \"e4Allele\"\ndat$APOEstatus[which(dat$APOEgeno == \"e3/e3\" | dat$APOEgeno == \"e2/e3\" | dat$APOEgeno == \"e2/e2\" | dat$APOEgeno == \"e1/e2\")] = \"NOe4Allele\"\n\ntable(dat$APOEstatus, dat$APOEgeno)\n\n#              e1/e2  e1/e4  e2/e2  e2/e3  e2/e4  e3/e3  e3/e4  e4/e4\n#  e4Allele        0     13      0      0   8818      0  82089   8266\n#  NOe4Allele      2      0   1984  42269      0 199277      0      0\n\n\n# keep only 2 columns\ndat = dat[,c(\"FID\", \"APOEgeno\", \"APOEstatus\")]\n\nwrite.table(dat, paste0(save, \"/UKB_APOE_Nov2023.txt\"), quote = F, col.names = T, row.names = F, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#type-2-diabetes",
    "href": "UKB_pheno.html#type-2-diabetes",
    "title": "UKB: Phenotypic data preparation",
    "section": "Type 2 diabetes",
    "text": "Type 2 diabetes\n\n\nCode\n## dementia is defined based on multiple sources of information we have in the UKB\n## 1. Self-reported illness (field ID 20002) & self-reported diagnosis by doctor (2443)\n## 2. ICD9 diagnoses (field ID 41203 & 41205)\n## 3. ICD10 diagnoses (field ID 41202 & 41204)\n## 4. Cause of death (field ID 40001)\n\n#########################################\n## 1. Self-reported illness (field IDs 20002, 2443, 2986, 2976, 21003) \n#########################################\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n\n#### field ID 20002\n####################\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.20002\", names(file))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n# 113066\n\n# determine codes of interest to find in field ID 41270\nt2d_codes <- c(\"1233\",\"1220\")\n\n# match ICDcodes with entries in all f.20002 variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, t2d_codes)))\n\n# if 1 or more entries, consider a case\nt2d$t2d20002 = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2d20002[allMissing] = NA\n\n# append final column to file\nfile$t2d20002 = t2d$t2d20002\n\n#### field 2443\n#### diabetes self-report interview touchscreen \n#################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.2443\", names(file))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# remove all minus (-) values as they stand for \"do not know\" or \"Prefer not to answer\"\nt2d[t2d < 0] = NA\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n\n# determine codes of interest to find in field \ndiab_code <- \"1\"\n\n# match codes with entries in all variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, diab_code)))\n\n# if 1 or more entries, consider a case\nt2d$t2d2443 = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2d2443[allMissing] = NA\n\n# append final column to file\nfile$t2d2443 = t2d$t2d2443\n\n##################################################\n## 4. Cause of death (field ID 40001)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.40001\", names(file))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(t2d)[grep(\"f.40001\", names(t2d))]){\n  t2d[which(t2d[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ndiab_code <- c(\"E110\",\"E111\",\"E112\",\"E113\",\"E114\",\"E115\",\"E116\",\"E117\",\"E118\",\"E119\")\n\n# match codes with entries in all variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, diab_code)))\n\n# if 1 or more entries, consider a case\nt2d$t2dDeath = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2dDeath[allMissing] = NA\n\n# append final column to file\nfile$t2dDeath = t2d$t2dDeath\n\n# intermediate save data \nsave = file[,c(\"f.eid\", \"t2d20002\", \"t2d2443\", \"t2dDeath\")]\n\n############################################\n## 2. ICD10 diagnoses (field ID 41202 & 41204)\n############################################\n### read in new data; basket 675090 has newer data for these variables \n\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41202\", names(file)), grep(\"f.41204\", names(file)))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(t2d)[c(grep(\"f.41202\", names(t2d)), grep(\"f.41204\", names(t2d)))]){\n  t2d[which(t2d[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ndiab_code <- c(\"E110\",\"E111\",\"E112\",\"E113\",\"E114\",\"E115\",\"E116\",\"E117\",\"E118\",\"E119\")\n\n# match codes with entries in all variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, diab_code)))\n\n# if 1 or more entries, consider a case\nt2d$t2dICD41202_4 = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2dICD41202_4[allMissing] = NA\n\n# append final column to file\nfile$t2dICD41202_4 = t2d$t2dICD41202_4\n\n\n##################################################\n## 3. ICD9 diagnoses (field ID 41203 & 41205)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41203\", names(file)), grep(\"f.41205\", names(file)))\n\n# select columns of interest\nt2d = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(t2d)[c(grep(\"f.41203\", names(t2d)), grep(\"f.41205\", names(t2d)))]){\n  t2d[which(t2d[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(t2d[,-which(names(t2d) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ndiab_code <- c(\"25000\",\"25010\",\"25020\",\"25090\")\n\n# match codes with entries in all variables filtered in 't2d'\nt2d$countCodes = rowSums(!is.na(sapply(t2d[,-which(names(t2d) == \"f.eid\")], match, diab_code)))\n\n# if 1 or more entries, consider a case\nt2d$t2dICD41203_5 = as.factor(ifelse(t2d$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nt2d$t2dICD41203_5[allMissing] = NA\n\n# append final column to file\nfile$t2dICD41203_5 = t2d$t2dICD41203_5\n\n\n######################################\n## Combine all t2d\n######################################\n# merge with saved columns above\nfile = merge(file, save, by = \"f.eid\")\n\n# keep columns of interest only\nfile = as.data.frame(file[, c(\"f.eid\", \"t2d20002\", \"t2d2443\", \"t2dICD41202_4\", \"t2dICD41203_5\", \"t2dDeath\")])\n\n# identify those with all missing\nallMissing = which(rowSums(!is.na(file[,-which(names(file) == \"f.eid\")])) == 0) \n\n# identify cases\nfile$t2d = 0 \n\n# identify cases\nfile$t2d[which(file$t2d20002 == 1 |\n                file$t2d2443 == 1 |\n                file$t2dICD41202_4 == 1 |\n                file$t2dICD41203_5 == 1 |\n                file$t2dDeath == 1)] = 1\n\n# delete allMissing from controls\nfile$t2d[allMissing] = NA\n\n# make factor\nfile$t2d = as.factor(file$t2d)\n\n#     0      1 \n# 451786  50436\n\n# save final variable to a seperate file\nfwrite(file[,c(\"f.eid\", \"t2d\")], file = paste0(out, \"/UKB_t2dStatus.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#hypertension",
    "href": "UKB_pheno.html#hypertension",
    "title": "UKB: Phenotypic data preparation",
    "section": "Hypertension",
    "text": "Hypertension\n\n\nCode\n# Hypertension is defined based on multiple field IDs\n## 1. Self-reported illness (field ID 20002) & self-reported diagnosis by doctor (2443)\n## 2. ICD9 diagnoses (field ID 41203 & 41205)\n## 3. ICD10 diagnoses (field ID 41202 & 41204)\n## 4. Cause of death (field ID 40001)\n\n#########################################\n## 1. Self-reported illness (field IDs 20002, 2443, 2986, 2976, 21003) \n#########################################\npath=\"/UK_Biobank_New/Data/Raw_Data/1027_Refresh_Dec_2022/670476\"\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n#### field ID 20002\n####################\n# list columns of interest\nid = which(names(file) == \"f.eid\")\nCols = grep(\"f.20002\", names(file))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(hyp[,-which(names(hyp) == \"f.eid\")])) == 0) \n# 113066\n\n# determine codes of interest to find in field ID 41270\nhyp_codes <- c(\"1065\",\"1072\")\n\n# match ICDcodes with entries in all f.20002 variables filtered in 't2d'\nhyp$countCodes = rowSums(!is.na(sapply(hyp[,-which(names(hyp) == \"f.eid\")], match, hyp_codes)))\n\n# if 1 or more entries, consider a case\nhyp$hyp20002 = as.factor(ifelse(hyp$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nhyp$hyp20002[allMissing] = NA\n\n# append final column to file\nfile$hyp20002 = hyp$hyp20002\n\n#### field 131287\n#### Source of report of I10 \n#################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.131287\", names(file))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(is.na(hyp[,which(names(hyp) != \"f.eid\")])) \n\n# determine participants with non missing \nallPresent = which(!is.na(hyp[,which(names(hyp) != \"f.eid\")]))\n\n# consider non-missing a case\nhyp$hyp131287 = NA\nhyp$hyp131287[allPresent] = 1\n\n# append final column to file\nfile$hyp131287 = hyp$hyp131287\n\n##################################################\n## 4. Cause of death (field ID 40001)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.40001\", names(file))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(hyp)[grep(\"f.40001\", names(hyp))]){\n  hyp[which(hyp[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(hyp[,-which(names(hyp) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \nhyp_code <- c(\"I10\")\n\n# match codes with entries in all variables filtered in 'hyp'\nhyp$countCodes = rowSums(!is.na(sapply(hyp[,-which(names(hyp) == \"f.eid\")], match, hyp_code)))\n\n# if 1 or more entries, consider a case\nhyp$hypDeath = as.factor(ifelse(hyp$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nhyp$hypDeath[allMissing] = NA\n\n# append final column to file\nfile$hypDeath = hyp$hypDeath\n\n# intermediate save data \nsave = file[,c(\"f.eid\", \"hyp20002\", \"hyp131287\", \"hypDeath\")]\n\n############################################\n## 2. ICD10 diagnoses (field ID 41202 & 41204)\n############################################\n### read in new data; basket 675090 has newer data for these variables \n\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41202\", names(file)), grep(\"f.41204\", names(file)))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(hyp)[c(grep(\"f.41202\", names(hyp)), grep(\"f.41204\", names(hyp)))]){\n  hyp[which(hyp[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(hyp[,-which(names(hyp) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \nhyp_code <- c(\"I10\")\n\n# match codes with entries in all variables filtered in 't2d'\nhyp$countCodes = rowSums(!is.na(sapply(hyp[,-which(names(hyp) == \"f.eid\")], match, hyp_code)))\n\n# if 1 or more entries, consider a case\nhyp$hypICD41202_4 = as.factor(ifelse(hyp$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nhyp$hypICD41202_4[allMissing] = NA\n\n# append final column to file\nfile$hypICD41202_4 = hyp$hypICD41202_4\n\n\n##################################################\n## 3. ICD9 diagnoses (field ID 41203 & 41205)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41203\", names(file)), grep(\"f.41205\", names(file)))\n\n# select columns of interest\nhyp = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(hyp)[c(grep(\"f.41203\", names(hyp)), grep(\"f.41205\", names(hyp)))]){\n  hyp[which(hyp[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(hyp[,-which(names(hyp) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \nhyp_code <- c(\"401\",\"4010\",\"4011\",\"4019\")\n\n# match codes with entries in all variables filtered in 't2d'\nhyp$countCodes = rowSums(!is.na(sapply(hyp[,-which(names(hyp) == \"f.eid\")], match, hyp_code)))\n\n# if 1 or more entries, consider a case\nhyp$hypICD41203_5 = as.factor(ifelse(hyp$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nhyp$hypICD41203_5[allMissing] = NA\n\n# append final column to file\nfile$hypICD41203_5 = hyp$hypICD41203_5\n\n\n######################################\n## Combine all hypertension variables \n######################################\n# merge with saved columns above\nfile = merge(file, save, by = \"f.eid\")\n\n# keep columns of interest only\nfile = as.data.frame(file[, c(\"f.eid\", \"hyp20002\", \"hyp131287\", \"hypDeath\", \"hypICD41202_4\", \"hypICD41203_5\")])\n\n# identify those with all missing\nallMissing = which(rowSums(!is.na(file[,-which(names(file) == \"f.eid\")])) == 0) \n\n# identify cases\nfile$hyp = 0 \n\n# identify cases\nfile$hyp[which(file$hyp20002 == 1 |\n                file$hyp131287 == 1 |\n                file$hypDeath == 1 |\n                file$hypICD41202_4 == 1 |\n                file$hypICD41203_5 == 1)] = 1\n\n\n# delete allMissing from controls\nfile$hyp[allMissing] = NA\n\n# make factor\nfile$hyp = as.factor(file$hyp)\n\n#     0      1 \n# 277454 204678 \n\n# save final variable to a seperate file\nfwrite(file[,c(\"f.eid\", \"hyp\")], file = paste0(out, \"/UKB_HypertensionStatus.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#smoking-packyears",
    "href": "UKB_pheno.html#smoking-packyears",
    "title": "UKB: Phenotypic data preparation",
    "section": "Smoking (packyears)",
    "text": "Smoking (packyears)\n\n\nCode\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\n# using instance at the time of initial neuroimaging visit\n# also keep \"ever smoked\" variable to determine zeros in data set\n# no need to consider variable f.20160_2.0 because when it's 0, it's also 0 in packyears \nCols = grep(\"f.20161_2.0\", names(file))\n\n# select columns of interest\nsmok = as.data.frame(file[, c(..id, ..Cols)])\n\n# rename var\nnames(smok)[which(names(smok) == \"f.20161_2.0\")] = \"packyears\"\n\n# save variable\nfwrite(smok[,c(\"f.eid\", \"packyears\")], file = paste0(out, \"/UKB_packyears.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#body-mass-index",
    "href": "UKB_pheno.html#body-mass-index",
    "title": "UKB: Phenotypic data preparation",
    "section": "Body mass index",
    "text": "Body mass index\n\n\nCode\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid = which(names(file) == \"f.eid\")\n# normal BMI item f.21001.0.0\n# impedance BMI item f.97777.0.0\nCols = c(grep(\"f.21001_2.0\", names(file)), grep(\"f.23104_2.0\", names(file)))\n\n# select columns of interest\nbmi = as.data.frame(file[, c(..id, ..Cols)])\n\n# Yaghootkar, 2016: “We excluded individuals with differences\n#>.4.56 SDs between impedance and normal BMI measures where both \n# variables were available\"\n\n## Calculate difference between f.21001.0.0 and f.23104.0.0 for each individual\nbmi$BMI_diff <- bmi$f.21001.0.0 - bmi$f.23104.0.0\n\n# Calculate mean and SD of the difference between f.21001.0.0 and f.23104.0.0\nmean_diff <- mean(bmi$BMI_diff,na.rm=T)\nSD_diff <- sd(bmi$BMI_diff,na.rm=T)\n\n# Calculate the mean of BMI measures for each individual\nbmi$bmi <- rowMeans(bmi[, c(\"f.21001_2.0\", \"f.23104_2.0\")], na.rm = T)\n\n# Set BMI_mean to NA for individuals with BMI_diff ±4.56 SD from the mean\nbmi$bmi[which(bmi$BMI_diff < mean_diff - (SD_diff*4.56) | bmi$BMI_diff > mean_diff + (SD_diff*4.56))]<-NA\n\n# save final variable to a seperate file\nfwrite(bmi[,c(\"f.eid\", \"bmi\")], file = paste0(out, \"/UKB_bmi.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#stroke",
    "href": "UKB_pheno.html#stroke",
    "title": "UKB: Phenotypic data preparation",
    "section": "Stroke",
    "text": "Stroke\n\n\nCode\n# multiple field IDs used to identify stroke\n# 1. Self-reported illness: field ID 200002\n# 2. Cause of death: field ID 40001\n# 3. Source of stroke report: field ID 42007\n# 4. ICd9 diagnoses: field IDs 41203 & 41205\n# 5. ICD10 diagnoses: field IDs 41202 & 41206\n\n#########################################\n## 1. Self-reported illness (field IDs 20002, 2443, 2986, 2976, 21003) \n#########################################\n# 20002.* path=\"/UK_Biobank_New/Data/Raw_Data/1027_Refresh_Dec_2022/670476\" \n# 42007.* path=\"/UK_Biobank_New/Data/Raw_Data/1027_Refresh_Dec_2022/670476\" \n\npath=\"/UK_Biobank_New/Data/Raw_Data/1027_Refresh_Dec_2022/670476\"\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n\n#### field ID 20002 (code 1081 & 1583)\n####################\n\n# list columns of interest\nid = which(names(file) == \"f.eid\")\nCols = grep(\"f.20002\", names(file))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n# 113066\n\n# determine codes of interest to find in field ID 41270\ncodes <- c(1081,1086,1491,1583)\n\n# match ICDcodes with entries in all f.20002 variables filtered in 't2d'\nstro$countCodes = rowSums(!is.na(sapply(stro[,-which(names(stro) == \"f.eid\")], match, codes)))\n\n# if 1 or more entries, consider a case\nstro$stro20002 = as.factor(ifelse(stro$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nstro$stro20002[allMissing] = NA\n\n# append final column to file\nfile$stro20002 = stro$stro20002\n\n#### field 42007\n#### Source of report of stroke\n#################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.42007\", names(file))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n\n# determine participants with non missing - consider them cases\nallPresent = which(!is.na(stro[,which(names(stro) != \"f.eid\")]))\n\n# consider non-missing a case (but missing doesn't mean they don\nstro$stro42007 = 0\nstro$stro42007[allPresent] = 1\nstro$stro42007[allMissing] = NA\n\nstro$stro42007 = as.factor(stro$stro42007)\n\n# append final column to file\nfile$stro42007 = stro$stro42007\n\n############################################\n# 2. Cause of Death (field ID 40001)\n############################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = grep(\"f.40001\", names(file))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(stro)[grep(\"f.40001\", names(stro))]){\n  stro[which(stro[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ncodes <- c(\"I600\",\"I601\",\"I602\",\"I603\",\"I604\",\"I605\",\"I606\",\"I607\",\"I608\",\"I609\",\"I61\",\"I610\",\"I611\",\"I612\",\"I613\",\"I614\",\"I615\",\"I616\",\"I618\",\"I619\",\"I63\",\"I630\",\"I631\",\"I632\",\"I633\",\"I634\",\"I635\",\"I636\",\"I638\",\"I639\",\"I64\")\n\n\n# match codes with entries in all variables filtered in 'hyp'\nstro$countCodes = rowSums(!is.na(sapply(stro[,-which(names(stro) == \"f.eid\")], match, codes)))\n\n# if 1 or more entries, consider a case\nstro$stroDeath = as.factor(ifelse(stro$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nstro$stroDeath[allMissing] = NA\n\n# append final column to file\nfile$stroDeath = stro$stroDeath\n\n# intermediate save\nsave = file[,c(\"f.eid\", \"stro20002\", \"stro42007\", \"stroDeath\")]\n\n############################################\n## 2. ICD10 diagnoses (field ID 41202 & 41204)\n############################################\n### read in new data; basket 675090 has newer data for these variables \n\npath=\"/UK_Biobank_New/Data/Raw_Data/1027_Refresh_Dec_2022/675090\"\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# file doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41202\", names(file)), grep(\"f.41204\", names(file)))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(stro)[c(grep(\"f.41202\", names(stro)), grep(\"f.41204\", names(stro)))]){\n  stro[which(stro[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \nICDcode <- c(\"I600\",\"I601\",\"I602\",\"I603\",\"I604\",\"I605\",\"I606\",\"I607\",\"I608\",\"I609\",\"I61\",\"I610\",\"I611\",\"I612\",\"I613\",\"I614\",\"I615\",\"I616\",\"I618\",\"I619\",\"I63\",\"I630\",\"I631\",\"I632\",\"I633\",\"I634\",\"I635\",\"I636\",\"I638\",\"I639\",\"I64\")\n\n# match codes with entries in all variables filtered in 't2d'\nstro$countCodes = rowSums(!is.na(sapply(stro[,-which(names(stro) == \"f.eid\")], match, ICDcode)))\n\n# if 1 or more entries, consider a case\nstro$stroICD41202_4 = as.factor(ifelse(stro$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nstro$stroICD41202_4[allMissing] = NA\n\n# append final column to file\nfile$stroICD41202_4 = stro$stroICD41202_4\n\n\n##################################################\n## 3. ICD9 diagnoses (field ID 41203 & 41205)\n##################################################\n# list columns of interest\nid=which(names(file) == \"f.eid\")\nCols = c(grep(\"f.41203\", names(file)), grep(\"f.41205\", names(file)))\n\n# select columns of interest\nstro = as.data.frame(file[, c(..id, ..Cols)])\n\n# format \"\" to be NA\nfor(i in names(stro)[c(grep(\"f.41203\", names(stro)), grep(\"f.41205\", names(stro)))]){\n  stro[which(stro[,i] == \"\"),i] = NA\n}\n\n# identify those who have fully missing data (ecxept their participant ID)\nallMissing = which(rowSums(!is.na(stro[,-which(names(stro) == \"f.eid\")])) == 0) \n\n# determine ICD10 codes of interest to find in field \ncodes <- c(\"4309\",\"4319\",\"4340\",\"4341\",\"4349\",\"4369\")\n\n# match codes with entries in all variables filtered in 't2d'\nstro$countCodes = rowSums(!is.na(sapply(stro[,-which(names(stro) == \"f.eid\")], match, codes)))\n\n# if 1 or more entries, consider a case\nstro$stroICD41203_5 = as.factor(ifelse(stro$countCodes >= 1, 1, 0))\n\n# delete those with missing IDs\nstro$stroICD41203_5[allMissing] = NA\n\n# append final column to file\nfile$stroICD41203_5 = stro$stroICD41203_5\n\n\n######################################\n## Combine all hypertension variables \n######################################\n\n# merge with saved columns above\nfile = merge(file, save, by = \"f.eid\")\n\n# keep columns of interest only\nfile = as.data.frame(file[, c(\"f.eid\", \"stro20002\", \"stro42007\", \"stroDeath\", \"stroICD41202_4\", \"stroICD41203_5\")])\n\n# identify those with all missing\nallMissing = which(rowSums(!is.na(file[,-which(names(file) == \"f.eid\")])) == 0) \n\n# identify cases\nfile$stroke = 0 \n\n# identify cases\nfile$stroke[which(file$stro20002 == 1 |\n                file$stro42007 == 1 |\n                file$stroDeath == 1 |\n                file$stroICD41202_4 == 1 |\n                file$stroICD41203_5 == 1)] = 1\n\n\n# delete allMissing from controls\nfile$stroke[allMissing] = NA\n\n# make factor\nfile$stroke = as.factor(file$stroke)\n\n#     0      1 \n# 461708  20197 \n\n# save final variable to a seperate file\nfwrite(file[,c(\"f.eid\", \"stroke\")], file = paste0(out, \"/UKB_StrokeStatus.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")\n\n\n\n\nCode\n#### test overlap with neuroimaging sample\n# read in file that holds neuroimaging IDs\npath=\"/CCACE_Shared/Anna_F/DownloadUKB/Nov2023/\"\nIDfile= list.files(path = path, pattern = \"AllAvailLong\")\n\nallNeuro = read.table(paste0(path, IDfile))\nnames(allNeuro)[1] = \"f.eid\"\n\n# read in data\nstroke = fread(paste0(out, \"/UKB_StrokeStatus.txt\"))\n\n# merge with data\nstroke = merge(stroke, allNeuro, by = \"f.eid\", all.y=T)\n\ntable(stroke$stroke)"
  },
  {
    "objectID": "UKB_pheno.html#fried-frailty-definition",
    "href": "UKB_pheno.html#fried-frailty-definition",
    "title": "UKB: Phenotypic data preparation",
    "section": "Fried frailty definition",
    "text": "Fried frailty definition\n\n\nCode\n## Fried fraitly definition\n# Weight loss: 2306\n# Exhaustion: 2080\n# Weakness: 46,47\n# Physical activity: 6164, 1011\n\npath=\"/UK_Biobank_New/Data/Raw_Data/1027_Refresh_Dec_2022/670476\"\nfileID = list.files(pat=path,pattern=\"csv\")\n# read in file\nfile = fread(paste0(path, \"/\", fileID))\n# R doesnt like column names that start with number and it doesnt like -\nnames(file) = paste0(\"f.\",names(file))\nnames(file) = gsub(\"-\", \"_\", names(file), fixed = T)\n\n# Weight loss: 2306 (neuroimaging visit)\n# Coding\n# -3 = Prefer not to say (NA)\n# -1 = Do not know (0)\n# 0 = no weight loss (0)\n# 2 = gained weight (0)\n# 3 = lost weight (1)\nid = which(names(file) == \"f.eid\")\nCols = grep(\"2306_2.\", names(file))\n\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# assign categories as in Jiang et al \nsub$f.2306_2.0[which(sub$f.2306_2.0 == -3)] = NA\nsub$f.2306_2.0[which(sub$f.2306_2.0 == -1)] = 0\nsub$f.2306_2.0[which(sub$f.2306_2.0 == 0)] = 0\nsub$f.2306_2.0[which(sub$f.2306_2.0 == 2)] = 0\nsub$f.2306_2.0[which(sub$f.2306_2.0 == 3)] = 1\n\n# intermediate save\nfile$weightLoss = sub$f.2306_2.0\n\n# Exhaustion: 2080\n# Coding in Jiang et al\n# -3 = Prefer not to answer (NA)\n# -1 = Don't know (0)\n# 1 = Not at all (0)\n# 2 = Several days (0)\n# 3 = More than half the days (1)\n# 4 = Nearly every day (1)\nid = which(names(file) == \"f.eid\")\nCols = grep(\"f.2080_2.\", names(file))\n\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# assign categories as in Jiang et al \nsub$f.2080_2.0[which(sub$f.2080_2.0 == -3)] = NA\nsub$f.2080_2.0[which(sub$f.2080_2.0 == -1)] = 0\nsub$f.2080_2.0[which(sub$f.2080_2.0 == 1)] = 0\nsub$f.2080_2.0[which(sub$f.2080_2.0 == 2)] = 0\nsub$f.2080_2.0[which(sub$f.2080_2.0 == 3)] = 1\nsub$f.2080_2.0[which(sub$f.2080_2.0 == 4)] = 1\n\n# intermediate save\nfile$exhaustion = sub$f.2080_2.0\n\n# Walking speed (924)\n# -7 = None of the above (NA)\n# -3 = Prefer not to answer (NA)\n# 1 = Slow pace (1)\n# 2 = Steady average pace (0)\n# 3 = Brisk pace (0)\nid = which(names(file) == \"f.eid\")\nCols = grep(\"f.924_2.\", names(file))\n\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# assign categories as in Jiang et al \nsub$f.924_2.0[which(sub$f.924_2.0 == -7)] = NA\nsub$f.924_2.0[which(sub$f.924_2.0 == -3)] = NA\nsub$f.924_2.0[which(sub$f.924_2.0 == 1)] = 1\nsub$f.924_2.0[which(sub$f.924_2.0 == 2)] = 0\nsub$f.924_2.0[which(sub$f.924_2.0 == 3)] = 0\n\n# intermediate save\nfile$walkingSpeed = sub$f.924_2.0\n\n# Weakness (grip strength) field ID average of 46,47\n# Cut offs\n# Males: \n# BMI <= 24 & grip strength <=29\n# 24.1<=BMI<=28 & grip strength <= 30\n# BMI > 28 & grip strength <= 32\n# Females:\n# BMI <= 23 & grip <=17\n# 23.1<=BMI<=26 & grip <=17.3\n# 26.1<=BMI & grip <=18\n# BMI>29 & grip<=21\n\n# select columns\nid = which(names(file) == \"f.eid\")\nCols = c(grep(\"f.46_2.\", names(file)), grep(\"f.47_2.\", names(file)), grep(\"f.21001_2.0\", names(file)), grep(\"f.23104_2.0\", names(file)), grep(\"f.31_\", names(file)))\n# subset data\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# get average bmi between two indepdendent measures\nsub$bmi <- rowMeans(sub[, c(\"f.21001_2.0\", \"f.23104_2.0\")], na.rm = T)\n\n# get average grip strength between left and right hand\nsub$grip <- rowMeans(sub[, c(\"f.46_2.0\", \"f.47_2.0\")], na.rm = T)\n\n# create new variable to hold weakness categories\nsub$weakness = 0\n\n# identify participants who have missing bmi and missing grip as we can't determine measures for them\nallMissing = which(rowSums(!is.na(sub[,c(\"bmi\", \"grip\")])) == 0) \n\n# assign categories\n## Males: field ID f.31 is 1 for male\nmales = subset(sub, f.31_0.0 == 1)\nmales$weakness[which(males$bmi <= 24 & males$grip <=29)] = 1\nmales$weakness[which(males$bmi >= 24.1 & males$bmi <= 28 & males$grip <= 30)] = 1\nmales$weakness[which(males$bmi > 28 & males$grip <=32)] = 1\n\n## Females: f.31 is 0 for female\nfemales = subset(sub, f.31_0.0 == 0)\nfemales$weakness[which(females$bmi <= 23  & females$grip <= 17)] = 1\nfemales$weakness[which(females$bmi >= 23.1 & females$bmi <= 26 & females$grip <= 17.3)] = 1\nfemales$weakness[which(females$bmi >= 26.1 & females$bmi <= 29 & females$grip <= 18)] = 1\nfemales$weakness[which(females$bmi > 29 & females$grip <= 21)] = 1\n\n# combine males and females into 1 dataset\nboth = rbind(males, females)\n\n# merge weakness variable back with other variables\nfile = merge(file, both[,c(\"f.eid\", \"weakness\")], by = \"f.eid\")\n\n# remove participants with all missing info\nfile$weakness[allMissing] = NA\n\n# Physical activity field ID average of 6164\n## 1 = Walking for pleasure (0)\n## 2 = Other exercises (0)\n## 3 = Strenuous sports (0)\n## 4 = Light DIY (0 = more than once per week, 1 = once per weeks or less)\n## 5 = Heavy DIY (0)\n## -7 = None of the above (1)\n## -3 = Prefer not to answer (NA) \n\n# Frequency of light DIY in the 4 weeks, 1011\n## 1 = Once in the last 4 weeks\n## 2 = 2-3 times in the last4 weeks\n## 3 = Once a week\n## 4 = 2-3 times a week\n## 5 = 4-5 times a week\n## 6 = every day\n## -1 = do not know \n## -3 = prefer not to answer \n\n# select columns\nid = which(names(file) == \"f.eid\")\nCols = c(grep(\"f.6164_2.\", names(file)), grep(\"f.1011_2\", names(file)))\n\nsub = as.data.frame(file[, c(..id, ..Cols)])\n\n# identify participants who have indicated 'none of the above' (assigned value: 1)\nNoPhysAct = which(sub$f.6164_2.0 == -7)\n\n# we also assign value 1 to people who indicate to do light DIY once per week or less\nNoPhysAct = c(NoPhysAct, which(sub$f.6164_2.0 == 4 & sub$f.1011_2.0 == 1 | sub$f.6164_2.0 == 4 & sub$f.1011_2.0 == 2 | sub$f.6164_2.0 == 4 & sub$f.1011_2.0 == 3))\n\n# identify participants with all missing data (assigned value: NAs)\nallMissing = which(rowSums(!is.na(sub[,grep(\"6164\", names(sub))])) == 0) \n\n# create variable where all participants have assigned value 0\nsub$physicalActiv = 0 \n\n# identify those with No Physical Activity\nsub$physicalActiv[NoPhysAct] = 1\n\n# identify those with all missing\nsub$physicalActiv[allMissing] = NA\n\n# temporary save\nfile$physicalActiv = sub$physicalActiv\n\n### get overall sumscore\nfrail = file[,c(\"f.eid\",\"weightLoss\", \"exhaustion\", \"walkingSpeed\", \"weakness\", \"physicalActiv\")]\nfrail$FriedFrailty = rowSums(frail[,c(\"weightLoss\", \"exhaustion\", \"walkingSpeed\", \"weakness\", \"physicalActiv\")], na.rm = F)\n# hist(frail$FriedFrailty)\n\n# save file \nfwrite(frail[,c(\"f.eid\", \"FriedFrailty\")], file = paste0(out, \"/UKB_FriedFrailty.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#brain-age",
    "href": "UKB_pheno.html#brain-age",
    "title": "UKB: Phenotypic data preparation",
    "section": "Brain Age",
    "text": "Brain Age\nBrain age was calculated with (brainageR v2)[github.com/james-cole/brainageR] using the Docker file available on (GitHub)[https://github.com/fprados/brainageR_dockerfile], which is performed on each individual NIFTI file and then saved in separate text files.\n\n\nCode\nsudo docker run --rm -it -v ${WD}:/data -w /data docker.io/library/brainimage:latest /bin/bash\n\nDAT=\"/data/temp\"\nIDinfo=\"/data/scripts/UKBlong\"\n\n# read in participant IDs\nall=$(cat ${IDinfo}/AllAvailIDs_UKB15112023.txt)\n\n# run brainageR\nfor i in $all\ndo\n\necho $i\n\n    if [ ! -f /data/data/UKB_BrainAge/${i}_brain_predicted.age.csv ]\n    then\n\n        brainageR -f ${DAT}/BrainAge/${i}_T1.nii -o /data/data/UKB_BrainAge/${i}_brain_predicted.age.csv\n\n    fi\n\ndone\n\n\n\nFormat individual-level data\nWe found that the average brain age prediction was ~15 years younger than chronological age, and this seems to be a concensus across other research groups that attempted to extract brain age from UKB neuroimaging data. To center brain age around zero, we deduct the sample brain age mean value from each individual value before calculating the brain age gap, which should not affect the interindividual variability we’re modeling.\n\n\nCode\n# identify csv files saved in Brain Age processing directory\nfiles = list.files(path = target, pattern = \"_T1_brain_predicted.age.csv\")\nfiles = paste0(target, \"/\",files)\n\n# object to hold brain age info\nsave = data.frame()\n\n# cycle through all files and save info\nfor(i in files){\n  # read in file\n  file = read.csv(i, header = T)\n  \n  # store info in object \"save\"\n  save = rbind(save, file)\n  \n}\n\n# re-name File column to subject ID\nsave$File = stringr::str_remove(save$File, pattern = \"_T1_orig_defaced\")\n\n# re-name ID column\nnames(save)[which(names(save) == \"File\")] = \"f.eid\"\n\n# rename\nsave$f.eid = stringr::str_remove(save$f.eid, pattern = \"_orig_defaced\")\n\nwrite.table(save, file = \"/CCACE_Shared/Anna_F/BrainAtrophy/data/UKB_BrainAge_T1_orig_defaced.txt\", col.names = T, row.names = F, quote = F, sep = \"\\t\")"
  },
  {
    "objectID": "UKB_pheno.html#merge-all-ukb-phenotypes-into-one-file",
    "href": "UKB_pheno.html#merge-all-ukb-phenotypes-into-one-file",
    "title": "UKB: Phenotypic data preparation",
    "section": "Merge all UKB phenotypes into one file",
    "text": "Merge all UKB phenotypes into one file\n\n\nCode\n#list.files(path = wd, pattern = \"UKB_\")\n# it makes it more straightforward to conduct the following analyses if I merge all phenotypes into one file\n# Step 1: Read all phenotypes in\n# Step 2: Merge them\n# Step 3: Save\n\n# cognitive ability\ncog = fread(paste0(wd, \"/UKB_gFactor.txt\"))\n# dementia\ndement = fread(paste0(wd, \"/UKB_DementiaStatus.txt\"))\ndement$dement = as.factor(dement$dement)\n# APOE\nAPOE = fread(paste0(wd, \"/UKB_APOE_Nov2023.txt\"))\nnames(APOE)[which(names(APOE) == \"FID\")] = \"f.eid\"\nAPOE = APOE[,c(\"f.eid\", \"APOEstatus\")]\nAPOE$APOEstatus[which(APOE$APOEstatus == \"e4Allele\")] = 1\nAPOE$APOEstatus[which(APOE$APOEstatus == \"NOe4Allele\")] = 0\nAPOE$APOEstatus = as.factor(APOE$APOEstatus)\n\n# Frailty\nfrail = fread(paste0(wd, \"/UKB_FriedFrailty.txt\"))\n# diabetes\ndiab = fread(paste0(wd, \"/UKB_t2dStatus.txt\"))\ndiab$t2d = as.factor(diab$t2d)\n# hyp\nhyp = fread(paste0(wd, \"/UKB_HypertensionStatus.txt\"))\nhyp$hyp = as.factor(hyp$hyp)\n# packyears\nsmok = fread(paste0(wd, \"/UKB_packyears.txt\"))\n# bmi\nbmi = fread(paste0(wd, \"/UKB_bmi.txt\"))\n# brain age\nBrainAge = fread(paste0(wd, \"/UKB_BrainAge_T1_orig_defaced.txt\"))\n## get brain age gap: chronological age minus brain predicted age \nage = fread(paste0(wd, \"/UKB_covarGWAS.txt\"))\nnames(age)[grepl(\"FID\", names(age))] <- \"f.eid\"\nBrainAge = merge(BrainAge, age[,c(\"f.eid\", \"age\")], by = \"f.eid\")\nBrainAge$chronAge <- BrainAge$age / 12\nBrainAge$BrainAgeGap <- BrainAge$chronAge - BrainAge$brain.predicted_age\n# deducting mean age to be centered around zero\nBrainAge$brainAge <- BrainAge$BrainAgeGap - mean(BrainAge$BrainAgeGap)\nBrainAge = BrainAge[,c(\"f.eid\", \"brainAge\")]\n\n# stroke\nstroke = fread(paste0(wd, \"/UKB_StrokeStatus.txt\"))\nstroke$stroke = as.factor(stroke$stroke)\n\n# merge data\nDatList = list(cog, dement, APOE, frail, diab, hyp, smok, bmi, stroke, BrainAge)\nUKB_merged = Reduce(function(x,y) merge(x, y, by = \"f.eid\", all = T), DatList)\n\n# restrict data set to neuroimaging participants \npath=\"/CCACE_Shared/Anna_F/DownloadUKB/Nov2023/\"\nIDfile= list.files(path = path, pattern = \"AllAvailLong\")\n\nallNeuro = read.table(paste0(path, IDfile))\nnames(allNeuro)[1] = \"f.eid\"\n\nUKB_merged = UKB_merged[UKB_merged$f.eid %in% allNeuro$f.eid,]\n\n# choose prettier names\nnames(UKB_merged) = c(\"f.eid\",\"cog\",\"dementia\",\"APOEe4\",\"frailty\",\"diabetes\",\"hypertension\",\"packyears\",\"BMI\",\"stroke\", \"brainAge\")\n\n# write\nfwrite(UKB_merged, file = paste0(out, \"/UKB_allPheno.txt\"), col.names = T, row.names = F, quote = F, na = NA, sep = \"\\t\")"
  }
]